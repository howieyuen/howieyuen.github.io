'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/golang/language-basics/unsafe/','title':"unsafe",'section':"语言基础",'content':" 本文转自： Go 里面的 unsafe 包详解\n The unsafe Package in Golang #  Golang 的 unsafe 包是一个很特殊的包。为什么这样说呢？本文将详细解释。\n来自 go 语言官方文档的警告 #  unsafe 包的文档是这么说的：\n导入 unsafe 的软件包可能不可移植，并且不受 Go 1 兼容性指南的保护。  Go 1 兼容性指南这么说：\n导入 unsafe 软件包可能取决于 Go 实现的内部属性。我们保留对可能导致程序崩溃的实现进行更改的权利。  当然包名称暗示 unsafe 包是不安全的。但这个包有多危险呢？让我们先看看 unsafe 包的作用。\nUnsafe 包的作用 #  直到现在（Go1.7），unsafe 包含以下资源：\n 三个函数：  func Alignof(variable ArbitraryType) uintptr func Offsetof(selector ArbitraryType) uintptr func Sizeof(variable ArbitraryType) uintptr   和一种类型：  类型 Pointer *ArbitraryType    这里，ArbitraryType 不是一个真正的类型，它只是一个占位符。\n与 Golang 中的大多数函数不同，上述三个函数的调用将始终在编译时求值，而不是运行时。这意味着它们的返回结果可以分配给常量。\n unsafe 包中的函数中非唯一调用将在编译时求值。 当传递给 len 和 cap 的参数是一个数组值时，内置函数和 cap 函数的调用也可以在编译时被求值。   除了这三个函数和一个类型外，指针在 unsafe 包也为编译器服务。\n出于安全原因，Golang 不允许以下之间的直接转换：\n 两个不同指针类型的值，例如 int64 和 float64。 指针类型和 uintptr 的值。  但是借助 unsafe.Pointer，我们可以打破 Go 类型和内存安全性，并使上面的转换成为可能。 这怎么可能发生？让我们阅读 unsafe 包文档中列出的规则：\n 任何类型的指针值都可以转换为 unsafe.Pointer。 unsafe.Pointer 可以转换为任何类型的指针值。 uintptr 可以转换为 unsafe.Pointer。 unsafe.Pointer 可以转换为 uintptr。  这些规则与 Go 规范一致：\n底层类型 uintptr 的任何指针或值都可以转换为指针类型，反之亦然。  规则表明 unsafe.Pointer 类似于 c 语言中的 void* 。当然，void* 在 C 语言里是危险的！\n在上述规则下，对于两种不同类型 T1 和 T2，可以使 T1 值与 unsafe.Pointer 值一致， 然后将 unsafe.Pointer 值转换为 T2 值（或 uintptr 值）。 通过这种方式可以绕过 Go 类型系统和内存安全性。当然，滥用这种方式是很危险的。\n举个例子：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { var n int64 = 5 var pn = \u0026amp;n var pf = (*float64)(unsafe.Pointer(pn)) // now, pn and pf are pointing at the same memory address  fmt.Println(*pf) // 2.5e-323  *pf = 3.14159 fmt.Println(n) // 4614256650576692846 } 在这个例子中的转换可能是无意义的，但它是安全和合法的（为什么它是安全的？）。\n因此，资源在 unsafe 包中的作用是为 Go 编译器服务，unsafe.Pointer 类型的作用是绕过 Go 类型系统和内存安全。\nunsafe.Pointer 和 uintptr #  这里有一些关于 unsafe.Pointer 和 uintptr 的事实：\n uintptr 是一个整数类型。  即使 uintptr 变量仍然有效，由 uintptr 变量表示的地址处的数据也可能被 GC 回收。   unsafe.Pointer是一个指针类型。  但是 unsafe.Pointer 值不能被取消引用。 如果 unsafe.Pointer 变量仍然有效，则由 unsafe.Pointer 变量表示的地址处的数据不会被 GC 回收。 unsafe.Pointer 是一个通用的指针类型，就像 *int 等。    由于 uintptr 是一个整数类型，uintptr 值可以进行算术运算。 所以通过使用 uintptr 和 unsafe.Pointer，我们可以绕过限制，*T 值不能在 Golang 中计算偏移量：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { a := [4]int{0, 1, 2, 3} p1 := unsafe.Pointer(\u0026amp;a[1]) p3 := unsafe.Pointer(uintptr(p1) + 2 * unsafe.Sizeof(a[0])) *(*int)(p3) = 6 fmt.Println(\u0026#34;a =\u0026#34;, a) // a = [0 1 2 6]  // ...  type Person struct { name string age int gender bool } who := Person{\u0026#34;John\u0026#34;, 30, true} pp := unsafe.Pointer(\u0026amp;who) pname := (*string)(unsafe.Pointer(uintptr(pp) + unsafe.Offsetof(who.name))) page := (*int)(unsafe.Pointer(uintptr(pp) + unsafe.Offsetof(who.age))) pgender := (*bool)(unsafe.Pointer(uintptr(pp) + unsafe.Offsetof(who.gender))) *pname = \u0026#34;Alice\u0026#34; *page = 28 *pgender = false fmt.Println(who) // {Alice 28 false} } unsafe 包有多危险 #  关于 unsafe 包，Ian，Go 团队的核心成员之一，已经确认：\n 在 unsafe 包中的函数的签名将不会在以后的 Go 版本中更改， 并且 unsafe.Pointer 类型将在以后的 Go 版本中始终存在。  所以，unsafe 包中的三个函数看起来不危险。 go team leader 甚至想把它们放在别的地方。 unsafe 包中这几个函数唯一不安全的是它们调用结果可能在后来的版本中返回不同的值。 很难说这种不安全是一种危险。\n看起来所有的 unsafe 包的危险都与使用 unsafe.Pointer 有关。 unsafe 包 docs 列出了一些使用 unsafe.Pointer 合法或非法的情况。 这里只列出部分非法使用案例：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) // case A: conversions between unsafe.Pointer and uintptr // don\u0026#39;t appear in the same expression func illegalUseA() { fmt.Println(\u0026#34;===================== illegalUseA\u0026#34;) pa := new([4]int) // split the legal use  // p1 := unsafe.Pointer(uintptr(unsafe.Pointer(pa)) + unsafe.Sizeof(pa[0]))  // into two expressions (illegal use):  ptr := uintptr(unsafe.Pointer(pa)) p1 := unsafe.Pointer(ptr + unsafe.Sizeof(pa[0])) // \u0026#34;go vet\u0026#34; will make a warning for the above line:  // possible misuse of unsafe.Pointer  // the unsafe package docs, https://golang.org/pkg/unsafe/#Pointer,  // thinks above splitting is illegal.  // but the current Go compiler and runtime (1.7.3) can\u0026#39;t detect  // this illegal use.  // however, to make your program run well for later Go versions,  // it is best to comply with the unsafe package docs.  *(*int)(p1) = 123 fmt.Println(\u0026#34;*(*int)(p1) :\u0026#34;, *(*int)(p1)) // } // case B: pointers are pointing at unknown addresses func illegalUseB() { fmt.Println(\u0026#34;===================== illegalUseB\u0026#34;) a := [4]int{0, 1, 2, 3} p := unsafe.Pointer(\u0026amp;a) p = unsafe.Pointer(uintptr(p) + uintptr(len(a)) * unsafe.Sizeof(a[0])) // now p is pointing at the end of the memory occupied by value a.  // up to now, although p is invalid, it is no problem.  // but it is illegal if we modify the value pointed by p  *(*int)(p) = 123 fmt.Println(\u0026#34;*(*int)(p) :\u0026#34;, *(*int)(p)) // 123 or not 123  // the current Go compiler/runtime (1.7.3) and \u0026#34;go vet\u0026#34;  // will not detect the illegal use here.  // however, the current Go runtime (1.7.3) will  // detect the illegal use and panic for the below code.  p = unsafe.Pointer(\u0026amp;a) for i := 0; i \u0026lt;= len(a); i++ { *(*int)(p) = 123 // Go runtime (1.7.3) never panic here in the tests  fmt.Println(i, \u0026#34;:\u0026#34;, *(*int)(p)) // panic at the above line for the last iteration, when i==4.  // runtime error: invalid memory address or nil pointer dereference  p = unsafe.Pointer(uintptr(p) + unsafe.Sizeof(a[0])) } } func main() { illegalUseA() illegalUseB() } 编译器很难检测 Go 程序中非法的 unsafe.Pointer 使用。 运行 go vet 可以帮助找到一些潜在的错误，但不是所有的都能找到。 同样是 Go 运行时，也不能检测所有的非法使用。 非法 unsafe.Pointer 使用可能会使程序崩溃或表现得怪异（有时是正常的，有时是异常的）。 这就是为什么使用不安全的包是危险的。\n转换 T1 为 T2 #  对于将 T1 转换为 unsafe.Pointer，然后转换为 T2，unsafe 包 docs 说：\n如果 T2 比 T1 大，并且两者共享等效内存布局，则该转换允许将一种类型的数据重新解释为另一类型的数据。  这种“等效内存布局”的定义是有一些模糊的。看起来 Go 团队故意如此。这使得使用 unsafe 包更危险。\n由于 Go 团队不愿意在这里做出准确的定义，本文也不尝试这样做 这里，列出了已确认的合法用例的一小部分。\n合法用例 1：在 []T 和 []MyT 之间转换 #  在这个例子里，我们用 int 作为 T：\ntype MyInt int 在 Golang 中，[]int 和 []MyInt 是两种不同的类型，它们的底层类型是自身。 因此，[]int 的值不能转换为 []MyInt，反之亦然。 但是在 unsafe.Pointer 的帮助下，转换是可能的：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { type MyInt int a := []MyInt{0, 1, 2} // b := ([]int)(a) // error: cannot convert a (type []MyInt) to type []int  b := *(*[]int)(unsafe.Pointer(\u0026amp;a)) b[0]= 3 fmt.Println(\u0026#34;a =\u0026#34;, a) // a = [3 1 2]  fmt.Println(\u0026#34;b =\u0026#34;, b) // b = [3 1 2]  a[2] = 9 fmt.Println(\u0026#34;a =\u0026#34;, a) // a = [3 1 9]  fmt.Println(\u0026#34;b =\u0026#34;, b) // b = [3 1 9] } 合法用例 2：调用 sync/atomic 包中指针相关的函数 #  sync/atomic 包中的以下函数的大多数参数和结果类型都是 unsafe.Pointer 或 *unsafe.Pointer：\n func CompareAndSwapPointer(addr *unsafe.Pointer, old，new unsafe.Pointer) (swapped bool) func LoadPointer(addr *unsafe.Pointer) (val unsafe.Pointer) func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer)  要使用这些功能，必须导入 unsafe 包。\n 注意： unsafe.Pointer 是一般类型，因此 unsafe.Pointer 的值可以转换为 unsafe.Pointer，反之亦然。\n package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;unsafe\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;math/rand\u0026#34; ) var data *string // get data atomically func Data() string { p := (*string)(atomic.LoadPointer( (*unsafe.Pointer)(unsafe.Pointer(\u0026amp;data)), )) if p == nil { return \u0026#34;\u0026#34; } else { return *p } } // set data atomically func SetData(d string) { atomic.StorePointer( (*unsafe.Pointer)(unsafe.Pointer(\u0026amp;data)), unsafe.Pointer(\u0026amp;d), ) } func main() { var wg sync.WaitGroup wg.Add(200) for range [100]struct{}{} { go func() { time.Sleep(time.Second * time.Duration(rand.Intn(1000)) / 1000) log.Println(Data()) wg.Done() }() } for i := range [100]struct{}{} { go func(i int) { time.Sleep(time.Second * time.Duration(rand.Intn(1000)) / 1000) s := fmt.Sprint(\u0026#34;#\u0026#34;, i) log.Println(\u0026#34;====\u0026#34;, s) SetData(s) wg.Done() }(i) } wg.Wait() fmt.Println(\u0026#34;final data = \u0026#34;, *data) } 结论 #   unsafe 包用于 Go 编译器，而不是 Go 运行时。 使用 unsafe 作为程序包名称只是让你在使用此包是更加小心。 使用 unsafe.Pointer 并不总是一个坏主意，有时我们必须使用它。 Golang 的类型系统是为了安全和效率而设计的。 但是在 Go 类型系统中，安全性比效率更重要。 通常 Go 是高效的，但有时安全真的会导致 Go 程序效率低下。 unsafe 包用于有经验的程序员通过安全地绕过 Go 类型系统的安全性来消除这些低效。 unsafe 包可能被滥用并且是危险的。  "});index.add({'id':1,'href':'/docs/golang/data-structure/chan/','title':"chan",'section':"数据结构",'content':"1. 引言 #  Go 语言中最常见的、也是经常被人提及的设计模式就是 —— 不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存。\n先入先出\n目前的 Channel 收发操作均遵循了先入先出（FIFO）的设计，具体规则如下：\n 先从 Channel 读取数据的 Goroutine 会先接收到数据； 先向 Channel 发送数据的 Goroutine 会得到先发送数据的权利；  2. 数据结构 #  type hchan struct { qcount uint // 当前队列中剩余元素个数 \tdataqsiz uint // 环形队列长度，即可以存放的元素个数 \tbuf unsafe.Pointer // 环形队列指针 \telemsize uint16 // 每个元素的大小 \tclosed uint32 // 标识关闭状态 \telemtype *_type // 元素类型 \tsendx uint // 发送操作处理到的位置 \trecvx uint // 接收操作处理到的位置 \trecvq waitq // 等待读消息的 goroutine 队列 \tsendq waitq // 等待写消息的 goroutine 队列 \tlock mutex // 互斥锁，chan 不允许并发读写 } 从数据结构可以看出 channel 由队列、类型信息、goroutine 等待队列组成，下面分别说明其原理。\n2.1 环形队列 #  chan 内部实现了一个环形队列作为其缓冲区，队列的长度是创建 chan 时指定的。\n下图展示了一个可缓存 6 个元素的 channel 示意图：\n+-------------+ | hchan | +-------------+ | qcount=2 | +-------------+ | dataqsiz=62 | +-------------+ +-----------------------+ | buf |------\u0026gt;| 0 | 1 | 1 | 0 | 0 | 0 | +-------------+ +-----^-------^---------+ | sendx=3 |-------------|-------| +-------------+ | | recvq=1 |-------------+ +-------------+  dataqsiz 指示了队列长度为 6，即可缓存 6 个元素； buf 指向队列的内存，队列中还剩余两个元素； qcount 表示队列中还有两个元素； sendx 指示后续写入的数据存储的位置，取值 [0, 6)； recvx 指示从该位置读取数据，取值 [0, 6)；  2.2 等待队列 #  从 channel 读数据，如果 channel 缓冲区为空或者没有缓冲区，当前 goroutine 会被阻塞。 向 channel 写数据，如果 channel 缓冲区已满或者没有缓冲区，当前 goroutine 会被阻塞。\n被阻塞的 goroutine 将会挂在 channel 的等待队列中：\n 因读阻塞的 goroutine 会被向 channel 写入数据的 goroutine 唤醒； 因写阻塞的 goroutine 会被从 channel 读数据的 goroutine 唤醒；  下图展示了一个没有缓冲区的 channel，有几个 goroutine 阻塞等待读数据：\n+-------------+ | hchan | +-------------+ | qcount=0 | +-------------+ | dataqsiz=0 | +-------------+ | buf | +-------------+ | sendx=0 | +-------------+ | recvx=0 | +-------------+ +---+ +---+ +---+ | sendq |---\u0026gt;| G |---\u0026gt;| G |---\u0026gt;| G | +-------------+ +---+ +---+ +---+ | recvq | +-------------+ 注意，一般情况下 recvq 和 sendq 至少有一个为空。只有一个例外， 那就是同一个 goroutine 使用 select 语句向 channel 一边写数据，一边读数据。\n2.3 类型信息 #  一个 channel 只能传递一种类型的值，类型信息存储在 hchan 数据结构中。\n elemtype 代表类型，用于数据传递过程中的赋值； elemsize 代表类型大小，用于在 buf 中定位元素位置。  2.4 锁 #  一个 channel 同时仅允许被一个 goroutine 读写。\n3. channel 读写 #  3.1 创建 channel #  创建 channel 的过程实际上是初始化 hcha 结构。其中类型信息和缓冲区长度由 make 语句传入， buf 的大小则与元素 大小和缓冲区长度共同决定。\n创建 channel 的部分代码如下所示，只保留了核心创建逻辑：\nfunc makechan(t *chantype, size int) *hchan { elem := t.elem // ...  mem, overflow := math.MulUintptr(elem.size, uintptr(size)) // ...  var c *hchan switch { case mem == 0: c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(\u0026amp;c.lock, lockRankHchan) // ...  return c } 3.2 发送数据 #  向一个 channel 中发送数据简单过程如下：\n 如果等待接收队列 recvq 不为空，说明缓冲区中没有数据或者没有缓冲区， 此时直接从 recvq 取出 G, 并把数据写入，最后把该 G 唤醒，结束发送过程； 如果缓冲区中有空余位置，将数据写入缓冲区，结束发送过程； 如果缓冲区中没有空余位置，将待发送数据写入 G，将当前 G 加入 sendq，进入睡眠，等待被读 goroutine 唤醒；  简单的流程图总结如下：  mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph TD start(开始发送) -- B{recq非空?} B -- |Y| C[从recvq取出一个G] C -- C1[数据写入G] C1 -- C2[唤醒G] C2 -- H(结束发送) B -- |N| D{buf有空位?} D -- |Y| E[将数据写入buf队尾] E -- H D -- |N| F[将当前goroutine加入senq,等待被唤醒] F -.- G[被唤醒,数据被取走] G -.- H 3.3 接收数据 #  从一个 channel 接收数据简单过程如下：\n 如果等待发送队列 sendq 不为空，且没有缓冲区，直接从 sendq 中取出 G， 把 G 中数据读出，最后把 G 唤醒，结束读取过程； 如果等待发送队列 sendq 不为空，此时说明缓冲区已满，从缓冲区中首部读出数据， 把 G 中数据写入缓冲区尾部，把 G 唤醒，结束读取过程； 如果缓冲区中有数据，则从缓冲区取出数据，结束读取过程； 将当前 goroutine 加入 recvq，进入睡眠，等待被写 goroutine 唤醒；  简单的流程图总结如下： graph TD A(开始接收) -- B{sendq非空?} B -- |N| F{qcount0?} F -- |Y| f0[从buf队头取数据] f0 -- Z F -- |N| f1[将当前goroutine假如有recq,等待被唤醒] f1 -.- f2[被唤醒,数据已写入] f2 -.- Z B -- |Y| C{有缓冲区?} C -- |Y| d0[从buf队头取数据] d0 -- d1[从sendq中取出G] d1 -- d2[把G中数据写入buf队尾] d2 -- d3[唤醒G] d3 -- Z(结束接收) C -- |N| d1 d1 -- e0[从G中读出数据] e0 -- d3 3.4 关闭 channel #  关闭 channel 时会把 recvq 中的 G 全部唤醒，本该写入 G 的数据位置为 nil。把 sendq 中的 G 全部唤醒，但这些 G 会 panic。 除此之外，panic 出现的常见场景还有：\n 关闭值为 nil 的 channel 关闭已经被关闭的 channel 向已经关闭的 channel 写数据  4. 参考资料 #    Go Channel 实现原理精要  Go 专家编程：1.1 chan  "});index.add({'id':2,'href':'/docs/leetcode/0321/','title':"321. 拼接最大数",'section':"LeetCode",'content':"321. 拼接最大数 #   leetcode 链接： https://leetcode-cn.com/problems/create-maximum-number/\n  给定长度分别为 m 和 n 的两个数组，其元素由 0-9 构成，表示两个自然数各位上的数字。现在从这两个数组中选出 k (k \u0026lt;= m + n) 个数字拼接成一个新的数，要求从同一个数组中取出的数字保持其在原数组中的相对顺序。\n求满足该条件的最大数。结果返回一个表示该最大数的长度为 k 的数组。\n说明：请尽可能地优化你算法的时间和空间复杂度。\n示例 1:\n输入： nums1 = [3, 4, 6, 5] nums2 = [9, 1, 2, 5, 8, 3] k = 5 输出： [9, 8, 6, 5, 3] 示例 2:\n输入： nums1 = [6, 7] nums2 = [6, 0, 4] k = 5 输出： [6, 7, 6, 0, 4] 示例 3:\n输入： nums1 = [3, 9] nums2 = [8, 9] k = 3 输出： [9, 8, 9]  方法一：单调栈\n为了求得长度为 k 的最大子序列，需要从两个数组中分别选出最大的子序列，然后将这两个子序列合并得到最大数。两个子序列的长度最小为 0，最大不能超过 k 且不能超过对应的数组长度。所以整个算法就分成三步：\n 分别计算 num1 和 nums2 的最大子序列 按照字典序降序合并子序列 保存对应的整数较大的子序列  func maxNumber(nums1 []int, nums2 []int, k int) []int { initialSize := 0 if len(nums2) \u0026lt; k { initialSize = k - len(nums2) } var ans []int for size := initialSize; size \u0026lt;= k \u0026amp;\u0026amp; size \u0026lt;= len(nums1); size++ { sub1 := maxSubSequence(nums1, size) sub2 := maxSubSequence(nums2, k-size) merged := merge(sub1, sub2) if lexicographicalLess(ans, merged) { ans = merged } } return ans } // 按照字典序降序合并 func merge(sub1, sub2 []int) []int { var merged = make([]int, len(sub1)+len(sub2)) for i := range merged { if lexicographicalLess(sub1, sub2) { merged[i], sub2 = sub2[0], sub2[1:] } else { merged[i], sub1 = sub1[0], sub1[1:] } } return merged } // 判断字典序大小 func lexicographicalLess(sub1, sub2 []int) bool { for i := 0; i \u0026lt; len(sub1) \u0026amp;\u0026amp; i \u0026lt; len(sub2); i++ { if sub1[i] != sub2[i] { return sub1[i] \u0026lt; sub2[i] } } return len(sub1) \u0026lt; len(sub2) } // 单调栈求指定长度的最大子序列 func maxSubSequence(nums []int, k int) []int { var sub []int for i := range nums { for len(sub) \u0026gt; 0 \u0026amp;\u0026amp; nums[i] \u0026gt; sub[len(sub)-1] \u0026amp;\u0026amp; len(sub)+len(nums)-i-1 \u0026gt;= k { sub = sub[:len(sub)-1] } if len(sub) \u0026lt; k { sub = append(sub, nums[i]) } } return sub } "});index.add({'id':3,'href':'/docs/kubernetes/kube-apiserver/authentication/','title':"认证机制",'section':"kube-apisever",'content':"1. Kubernetes 中的用户 #  所有 Kubernetes 集群都有两类用户：由 Kubernetes 管理的 ServiceAccount 和普通用户。\n对于与普通用户，Kuernetes 使用以下方式管理：\n 负责分发私钥的管理员 类似 Keystone 或者 Google Accounts 这类用户数据库 包含用户名和密码列表的文件  因此，kubernetes 并不提供普通用户的定义，普通用户是无法通过 API 调用写入到集群中的。\n尽管如此，通过集群的证书机构签名的合法证书的用户，kubernetes 依旧可以认为是合法用户。基于此，kubernetes 使用证书中的 subject.CommonName 字段来确定用户名，接下来，通过 RBAC 确认用户对某资源是否存在要求的操作权限。\n与此不同的 ServiceAccount，与 Namespace 绑定，与一组 Secret 所包含的凭据有关。这些凭据会挂载到 Pod 中，从而允许访问 kubernetes 的 API。\nAPI 请求要么与普通用户相关，要么与 ServiceAccount 相关，其他的视为匿名请求。这意味着集群内和集群外的每个进程向 kube-apiserver 发起请求时，都必须通过身份认证，否则会被视为匿名用户。\n2. 认证机制 #  目前 kubernetes 提供的认证机制丰富多样，尤其是身份验证，更是五花八门：\n 身份验证  X509 Client Cert Static Token File Bootstrap Tokens Static Password File（deprecated in v1.16） ServiceAccount Token OpenID Connect Token Webhook Token Authentication Proxy   匿名请求 用户伪装 client-go 凭据插件  2.1 身份验证策略 #  2.1.1 X509 Client Cert #  X509 客户端证书认证，也被称为 TLS 双向认证，即为服务端和客户端互相验证证书的正确性。使用此认证方式，只要是 CA 签名过的证书都能通过认证。\n  启用\nkube-apiserver 通过指定 --client-ca-file 参数启用此认证方式。\n  认证接口\n// staging/src/k8s.io/apiserver/pkg/authentication/authenticator/interfaces.go type Request interface { AuthenticateRequest(req *http.Request) (*Response, bool, error) } 该方法接收客户端请求。若验证失败，bool 返回 false，验证成功，bool 返回 true，Response 中携带身份验证用户的信息，例如 Name、UID、Groups、Extra。\n  认证实现\n// staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go func (a *Authenticator) AuthenticateRequest(req *http.Request) (*authenticator.Response, bool, error) { if req.TLS == nil || len(req.TLS.PeerCertificates) == 0 { return nil, false, nil } // Use intermediates, if provided  optsCopy, ok := a.verifyOptionsFn() // if there are intentionally no verify options, then we cannot authenticate this request  if !ok { return nil, false, nil } if optsCopy.Intermediates == nil \u0026amp;\u0026amp; len(req.TLS.PeerCertificates) \u0026gt; 1 { optsCopy.Intermediates = x509.NewCertPool() for _, intermediate := range req.TLS.PeerCertificates[1:] { optsCopy.Intermediates.AddCert(intermediate) } } remaining := req.TLS.PeerCertificates[0].NotAfter.Sub(time.Now()) clientCertificateExpirationHistogram.Observe(remaining.Seconds()) // 校验证书，如果通过，可解析 user 信息  chains, err := req.TLS.PeerCertificates[0].Verify(optsCopy) if err != nil { return nil, false, err } var errlist []error for _, chain := range chains { user, ok, err := a.user.User(chain) if err != nil { errlist = append(errlist, err) continue } if ok { return user, ok, err } } return nil, false, utilerrors.NewAggregate(errlist) }   2.1.2 Static Token File #  Token 也被称为令牌，服务端为了验证客户端身份，需要客户端向服务端提供一个可靠的验证信息，这个验证信息就是 Token。目前，令牌会长期有效，并且在不重启 API 服务器的情况下 无法更改令牌列表。\n  启用\nkube-apiserver 通过指定 --token-auth-file 参数启用，令牌文件是一个 CSV 文件，包含至少 3 个列：令牌、用户名和用户的 UID。 其余列被视为可选的组名。示例如下：\ntoken,user,uid,\u0026quot;group1,group2,group3\u0026quot;   请求头配置\n在 HTTP 请求头中，设置 Authentication 的值，格式为 Bearer $TOKEN，格式如下：\nAuthorization: Bearer 31ada4fd-adec-460c-809a-9e56ceb75269   认证实现\n// staging/src/k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go func (a *TokenAuthenticator) AuthenticateToken(ctx context.Context, value string) (*authenticator.Response, bool, error) { user, ok := a.tokens[value] if !ok { return nil, false, nil } return \u0026amp;authenticator.Response{User: user}, true, nil } 该认证方式相对简单，a.tokens 保存了服务端 Token 列表，通过 map 查询客户端提供的 Token 是否存在，存在即认证成功，反之则认证失败。\n  2.1.3 Bootstrap Tokens #  Bootstrap Token 是一种简单的 Bearer Token，这种令牌是在新建集群或者在现在集群中加入节点时使用。一般是由 kubeadm 管理，以 secret 形式保存在 kube-system 命名空间，可以动态地创建删除，并且 kube-controller-manager 中 TokenCleaner 会在 Token 过期时删除。该能力目前依旧是 alpha 阶段，但官方预期也不会有大的突破性变化。\n  启用\nkube-apiserver 设置 --enable-bootstrap-token 启动 Bootstrap Token 身份认证，并且依赖 kube-controller-manager 设置 --controllers=*,tokencleaner,bootstrapsigner 启动 TokenCleaner 和 BootstrapSigner。\n  请求头配置\nToken 的格式为 [a-z0-9]{6}.[a-z0-9]{16}，第一部分是 token id，第二部分是 token 的 secret。可以用如下方式设置 HTTP Header：\nAuthorization: Bearer 781292.db7bc3a58fc5f07e   认证实现\n// plugin/pkg/auth/authenticator/token/bootstrap/bootstrap.go func (t *TokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { // 1. 校验 token 格式  tokenID, tokenSecret, err := bootstraptokenutil.ParseToken(token) if err != nil { return nil, false, nil } // 2. 拼接 secret name，获取 secret 对象  secretName := bootstrapapi.BootstrapTokenSecretPrefix + tokenID secret, err := t.lister.Get(secretName) if err != nil { if errors.IsNotFound(err) { klog.V(3).Infof(\u0026#34;No secret of name %s to match bootstrap bearer token\u0026#34;, secretName) return nil, false, nil } return nil, false, err } // 3. 校验 secret 有效，不在删除中  if secret.DeletionTimestamp != nil { tokenErrorf(secret, \u0026#34;is deleted and awaiting removal\u0026#34;) return nil, false, nil } // 4. 校验 secret 类型必须是 bootstrap.kubernetes.io/token  if string(secret.Type) != string(bootstrapapi.SecretTypeBootstrapToken) || secret.Data == nil { tokenErrorf(secret, \u0026#34;has invalid type, expected %s.\u0026#34;, bootstrapapi.SecretTypeBootstrapToken) return nil, false, nil } // 5. 校验 token secret 有效  ts := bootstrapsecretutil.GetData(secret, bootstrapapi.BootstrapTokenSecretKey) if subtle.ConstantTimeCompare([]byte(ts), []byte(tokenSecret)) != 1 { tokenErrorf(secret, \u0026#34;has invalid value for key %s, expected %s.\u0026#34;, bootstrapapi.BootstrapTokenSecretKey, tokenSecret) return nil, false, nil } // 6. 校验 token id 有效  id := bootstrapsecretutil.GetData(secret, bootstrapapi.BootstrapTokenIDKey) if id != tokenID { tokenErrorf(secret, \u0026#34;has invalid value for key %s, expected %s.\u0026#34;, bootstrapapi.BootstrapTokenIDKey, tokenID) return nil, false, nil } // 7. 校验 token 是否过期  if bootstrapsecretutil.HasExpired(secret, time.Now()) { // logging done in isSecretExpired method.  return nil, false, nil } // 8. 校验 secret 对象的 data 字段中，key 为 usage-bootstrap-authentication，value 为 true  if bootstrapsecretutil.GetData(secret, bootstrapapi.BootstrapTokenUsageAuthentication) != \u0026#34;true\u0026#34; { tokenErrorf(secret, \u0026#34;not marked %s=true.\u0026#34;, bootstrapapi.BootstrapTokenUsageAuthentication) return nil, false, nil } // 9. 获取 secret.data[auth-extra-groups]，与 default group 组合  groups, err := bootstrapsecretutil.GetGroups(secret) if err != nil { tokenErrorf(secret, \u0026#34;has invalid value for key %s: %v.\u0026#34;, bootstrapapi.BootstrapTokenExtraGroupsKey, err) return nil, false, nil } return \u0026amp;authenticator.Response{ User: \u0026amp;user.DefaultInfo{ Name: bootstrapapi.BootstrapUserPrefix + string(id), Groups: groups, }, }, true, nil }   2.1.4 ServiceAccount Token #  其他认证方式都是从 kubernetes 集群外部访问 kube-apiserver 组件，而 ServiceAccount 是从 Pod 内部访问，提供给 Pod 中的进程使用。ServiceAccount 包含了 3 个部分的内容：\n Namespace：指定 Pod 所在的命名空间 CA：kube-apiserver CA 公钥证书，是 Pod 内部进程对 kube-apiserver 进行验证的证书 Token：用于身份验证，通过 kube-apiserver 私钥签发经过 Base64 编码的 Bearer Token  他们都通过 mount 命令挂载到 Pod 的文件系统中，Namespace 存储在 /var/run/secrets/kubernetes.io/serviceaccount/namespace，经过 Base64 加密；CA 的存储路径 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt；Token 存储在 /var/run/secrets/kubernetes.io/serviceaccount/token 文件中。\n  启用\nkube-apiserver 指定以下参数启用\n --service-account-key-file ：包含用来给 Bearer Token 签名的 PEM 编码密钥，如果未指定，使用 kube-apiserver 的 TLS 私钥。 --service-account-lookup：用于验证 service account token 是否存在 etcd 中，默认为 true。    配置\nServiceAccount 通常是 kube-apiserver 自动创建，并通过准入控制器关联到 Pod 中。当然也可以在 Pod.spec.serviceAccountName 显示地指定。\n  认证实现\n// pkg/serviceaccount/jwt.go func (j *jwtTokenAuthenticator) AuthenticateToken(ctx context.Context, tokenData string) (*authenticator.Response, bool, error) { // 1. 校验 token 格式正确  if !j.hasCorrectIssuer(tokenData) { return nil, false, nil } // 2. 解析 JWT 对象  tok, err := jwt.ParseSigned(tokenData) ... public := \u0026amp;jwt.Claims{} private := j.validator.NewPrivateClaims() // TODO: Pick the key that has the same key ID as `tok`, if one exists.  var ( found bool errlist []error ) // 3. 使用--service-account-key-file 提供的密钥，反序列化 JWT  for _, key := range j.keys { if err := tok.Claims(key, public, private); err != nil { errlist = append(errlist, err) continue } found = true break } ... // 4. 验证 namespace 是否正确、serviceAccountName、serviceAccountID 是否存在，token 是否失效  sa, err := j.validator.Validate(ctx, tokenData, public, private) if err != nil { return nil, false, err } return \u0026amp;authenticator.Response{ User: sa.UserInfo(), Audiences: auds, }, true, nil } 服务账号被身份认证后，所确定的用户名为 system:serviceaccount:\u0026lt;NAMESPACE\u0026gt;:\u0026lt;SERVICEACCOUNT\u0026gt;， 并被分配到用户组 system:serviceaccounts 和 system:serviceaccounts:\u0026lt;NAMESPACE\u0026gt;。\n  2.1.5 OpenID Connect Token #  OpenID Connect Token(OIDC) 是一套基于 OAuth2.0 协议的轻量级认证规范，其提供了通过 API 进行身份交互的框架。OIDC 认证除了认证请求外，还会标明请求的用户身份（ID Token）。其中 Token 被称为 ID Token，此 ID Token 是 JWT，具有服务器签名的相关字段。认证流程如下：\n 用户想要访问 kube-apiserver，先通过认证服务（Auth Service，例如 Google Accounts 服务）认证自己，得到 access_token、id_token 和 refresh_token。 用户把 access_token、id_token 和 refresh_token 配置到客户端应用程序，例如：kubectl 或者 dashboard 工具 客户端使用 Token 以用户身份访问 kube-apiserver  kube-apiserver 和 Auth Service 没有直接交互，而是鉴定客户端发送过来的 Token 是否合法。完整的 OIDC 认证过程如下图所示：\n mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) sequenceDiagram participant user as 用户 participant idp as 身份提供者 participant kube as Kubectl participant api as API 服务器 user - idp: 1. 登录到 IdP activate idp idp -- user: 2. 提供 access_token,\nid_token, 和 refresh_token deactivate idp activate user user - kube: 3. 调用 Kubectl 并\n设置 --token 为 id_token\n或者将令牌添加到 .kube/config deactivate user activate kube kube - api: 4. Authorization: Bearer... deactivate kube activate api api - api: 5. JWT 签名合法么？ api - api: 6. JWT 是否已过期？(iat+exp) api - api: 7. 用户被授权了么？ api -- kube: 8. 已授权：执行\n操作并返回结果 deactivate api activate kube kube --x user: 9. 返回结果 deactivate kube  登录到身份服务（即 Auth Server） 身份服务将为你提供 access_token、id_token 和 refresh_token 用户在使用 kubectl 时，将 id_token 设置为 --token 标志值，或者将其直接添加到 kubeconfig 中 kubectl 将 id_token 设置为 Authorization 的请求头，发送给 API 服务器 API 服务器将负责通过检查配置中引用的证书来确认 JWT 的签名是合法的 检查确认 id_token 尚未过期 确认用户有权限执行操作 鉴权成功之后，API 服务器向 kubectl 返回响应 kubectl 向用户提供反馈信息  kube-apiserver 不与 Auth Service 交互就可以认证 Token 的合法性，关键在于第 5 步，所有 JWT 都由颁发给它的 Auth Service 进行了数字签名，只需要在 kube-apiserver 的启动参数中，配置信任的 Auth Server 证书，用它来验证 id_token 是否合法。\n 启用   --oidc-ca-file：指向一个 CA 证书的路径，该 CA 负责对你的身份服务的 Web 证书提供签名。默认值为宿主系统的根 CA（/etc/kubernetes/ssl/kc-ca.pem）。 --oidc-client-id：所有令牌都应发放给此客户 ID。 --oidc-groups-claim：JWT 声明的用户组名称。 --oidc-groups-prefix：添加到组申领的前缀，用来避免与现有用户组名（如：system: 组）发生冲突。例如，此标志值为 oidc: 时，所得到的用户组名形如 oidc:engineering 和 oidc:infra。 --oidc-issuer-url：允许 API 服务器发现公开的签名密钥的服务的 URL。只接受模式为 https:// 的 URL。此值通常设置为服务的发现 URL，不含路径。例如：\u0026ldquo;https://accounts.google.com\u0026rdquo; 或 \u0026ldquo;https://login.salesforce.com\u0026rdquo;。此 URL 应指向 .well-known/openid-configuration 下一层的路径。 --oidc-required-claim：取值为一个 key=value 偶对，意为 ID 令牌中必须存在的申领。如果设置了此标志，则 ID 令牌会被检查以确定是否包含取值匹配的申领。此标志可多次重复，以指定多个申领。 --oidc-username-claim：JWT 声明的用户名称。默认情况下使用 sub 值，即最终用户的一个唯一的标识符。 --oidc-username-prefix：要添加到用户名申领之前的前缀，用来避免与现有用户名（例如：system: 用户）发生冲突。  认证实现  // staging/src/k8s.io/apiserver/plugin/pkg/authenticator/token/oidc/oidc.go func (a *Authenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { ... idToken, err := verifier.Verify(ctx, token) ... return \u0026amp;authenticator.Response{User: info}, true, nil } 整个认证逻辑，大体上是解析 id_token，把其中的 user、group 信息取出，组成 User 对象返回。在返回之前，要对针对 kube-apiserver 的各个 reqiured_claims 入参校验，看看从 id_token 中的值是否匹配。\n2.1.6 Webhook Token #  webhook 也被称为钩子，是一种基于 HTTP 协议的回调机制，当客户端发送的认证请求到达 kube-apiserver 时，kubbe-apiserver 回调钩子方法，将验证信息发送给远程的 webhook 服务器进行验证，让根据返回的状态码判断是否认证通过。\n 启用   --authentication-token-webhook-config-file：指向一个配置文件，其中描述 如何访问远程的 Webhook 服务。 --authentication-token-webhook-cache-ttl：用来设定身份认证决定的缓存时间。默认时长为 2 分钟。  配置文件使用 kubeconfig 文件的格式。文件中，clusters 指代远程服务，users 指代远程 API 服务 Webhook。下面是一个例子：\n# Kubernetes API 版本 apiVersion: v1 # API 对象类别 kind: Config # clusters 指代远程服务 clusters: - name: name-of-remote-authn-service cluster: certificate-authority: /path/to/ca.pem  # 用来验证远程服务的 CA server: https://authn.example.com/authenticate # 要查询的远程服务 URL。必须使用 \u0026#39;https\u0026#39;。 # users 指代 API 服务的 Webhook 配置 users: - name: name-of-api-server user: client-certificate: /path/to/cert.pem # Webhook 插件要使用的证书 client-key: /path/to/key.pem  # 与证书匹配的密钥 # kubeconfig 文件需要一个上下文（Context），此上下文用于本 API 服务器 current-context: webhook contexts: - context: cluster: name-of-remote-authn-service user: name-of-api-sever name: webhook 当客户端尝试在 API 服务器上使用持有者令牌完成身份认证（ 如前所述）时， 身份认证 Webhook 会用 POST 请求发送一个 JSON 序列化的对象到远程服务。 该对象是 authentication.k8s.io/v1beta1 组的 TokenReview 对象， 其中包含持有者令牌。 Kubernetes 不会强制请求提供此 HTTP 头部。\n要注意的是，Webhook API 对象和其他 Kubernetes API 对象一样，也要受到同一 版本兼容规则约束。 实现者要了解对 Beta 阶段对象的兼容性承诺，并检查请求的 apiVersion 字段， 以确保数据结构能够正常反序列化解析。此外，API 服务器必须启用 authentication.k8s.io/v1beta1 API 扩展组 （--runtime-config=authentication.k8s.io/v1beta1=true）。\n认证实现  // staging/src/k8s.io/apiserver/pkg/authentication/token/cache/cached_token_authenticator.go func (a *cachedTokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { record := a.doAuthenticateToken(ctx, token) if !record.ok || record.err != nil { return nil, false, record.err } for key, value := range record.annotations { audit.AddAuditAnnotation(ctx, key, value) } return record.resp, true, nil } a.doAuthenticateToken(ctx, token) 是认证过程的核心，首先从缓存中查找是否已认证，有则直接返回，没有调用远程 webhook 服务验证。\n// staging/src/k8s.io/apiserver/plugin/pkg/authenticator/token/webhook/webhook.go func (w *WebhookTokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { ... webhook.WithExponentialBackoff(ctx, w.initialBackoff, func() error { result, err = w.tokenReview.Create(ctx, r, metav1.CreateOptions{}) return err }, webhook.DefaultShouldRetry) ... if !r.Status.Authenticated { var err error if len(r.Status.Error) != 0 { err = errors.New(r.Status.Error) } return nil, false, err } ... return \u0026amp;authenticator.Response{ User: \u0026amp;user.DefaultInfo{ Name: r.Status.User.Username, UID: r.Status.User.UID, Groups: r.Status.User.Groups, Extra: extra, }, Audiences: auds, }, true, nil } 通过 w.tokenReview.Create 发送 POST 请求到远程 webhook 服务，并在 body 体中携带认真信息，根据返回值 Status.Authenticated 判断是否认证通过。\n2.1.7 Authentication Proxy #  API 服务器可以配置成从请求的头部字段值（如 X-Remote-User）中辩识用户。这一设计是用来与某身份认证代理一起使用 API 服务器，代理负责设置请求的头部字段值。\n认证代理有几个列表，\n 用户名列表：建议设置为 \u0026ldquo;X-Remote-User\u0026rdquo;。必选 组列表：建议设置为 \u0026ldquo;X-Remote-Group\u0026rdquo;。可选 额外列表：建议设置为 \u0026ldquo;X-Remote-Extra-\u0026quot;。可选   启用   --requestheader-client-ca-file：指定有效的客户端 CA 证书。 --requestheader-allowed-names：指定通用名称（Common Name） --requestheader-username-headers：指定用户名列表 --requestheader-group-headers：指定组名列表 --requestheader-extra-headers-prefix：指定额外列表  认证  // staging/src/k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader.go func (a *requestHeaderAuthRequestHandler) AuthenticateRequest(req *http.Request) (*authenticator.Response, bool, error) { // 用户信息 \tname := headerValue(req.Header, a.nameHeaders.Value()) if len(name) == 0 { return nil, false, nil } // 组信息 \tgroups := allHeaderValues(req.Header, a.groupHeaders.Value()) // 额外信息 \textra := newExtra(req.Header, a.extraHeaderPrefixes.Value()) ... return \u0026amp;authenticator.Response{ User: \u0026amp;user.DefaultInfo{ Name: name, Groups: groups, Extra: extra, }, }, true, nil } 在进行认证代理认证时，requestHeader 就是实现方式，分别从 HTTP Header 读出用户、组和额外信息，返回给客户端。\n2.2 其他 #   有关匿名请求、用户伪装和 client-go 插件代理，请移步官网： 用户认证\n "});index.add({'id':4,'href':'/docs/kubernetes/kube-controller-manager/code-analysis-of-garbagecollector-controller/','title':"GC Controller 源码分析",'section':"kube-controller-manager",'content':"1. 序言 #  垃圾回收相关，可参考 这里\n2. 源码解析 #  GarbageCollectorController 负责回收集群中的资源对象，要做到这一点，首先得监控所有资源。gc controller 会监听集群中所有可删除资源的事件，这些事件会放到一个队列中，然后启动多个 worker 协程处理。对于删除事件，则根据删除策略删除对象；其他事件，更新对象之间的依赖关系。\n2.1 startGarbageCollectorController() #  首先来看 gc controller 的入口方法，也就是 kube-controller-manager 是如何启动它的。它的主要逻辑：\n 判断是否启用 gc controller，默认是 true 初始化 clientset，使用 discoveryClient 获取集群中所有资源 注册不考虑 gc 的资源，默认为空 调用 garbagecollector.NewGarbageCollector() 方法 初始化 gc controller 对象 调用 garbageCollector.Run() 启动 gc controller，workers 默认是 20 调用 garbageCollector.Sync() 监听集群中的资源，当出现新的资源时，同步到 minitors 中 调用 garbagecollector.NewDebugHandler() 注册 debug 接口，用来提供集群内所有对象的关联关系；  cmd/kube-controller-manager/app/core.go:538\nfunc startGarbageCollectorController(ctx ControllerContext) (http.Handler, bool, error) { // 1. 判断是否启用 gc controller，默认是 true \tif !ctx.ComponentConfig.GarbageCollectorController.EnableGarbageCollector { return nil, false, nil } // 2. 初始化 clientset \tgcClientset := ctx.ClientBuilder.ClientOrDie(\u0026#34;generic-garbage-collector\u0026#34;) config := ctx.ClientBuilder.ConfigOrDie(\u0026#34;generic-garbage-collector\u0026#34;) metadataClient, err := metadata.NewForConfig(config) if err != nil { return nil, true, err } // 3. 注册不考虑 gc 的资源，默认为空 \tignoredResources := make(map[schema.GroupResource]struct{}) for _, r := range ctx.ComponentConfig.GarbageCollectorController.GCIgnoredResources { ignoredResources[schema.GroupResource{Group: r.Group, Resource: r.Resource}] = struct{}{} } // 4. 初始化 gc controller 对象 \tgarbageCollector, err := garbagecollector.NewGarbageCollector( metadataClient, ctx.RESTMapper, ignoredResources, ctx.ObjectOrMetadataInformerFactory, ctx.InformersStarted, ) if err != nil { return nil, true, fmt.Errorf(\u0026#34;failed to start the generic garbage collector: %v\u0026#34;, err) } // 5. 启动 gc，workers 默认是 20 \tworkers := int(ctx.ComponentConfig.GarbageCollectorController.ConcurrentGCSyncs) go garbageCollector.Run(workers, ctx.Stop) // Periodically refresh the RESTMapper with new discovery information and sync \t// the garbage collector. \t// 6. 监听集群中的资源，周期更新 \tgo garbageCollector.Sync(gcClientset.Discovery(), 30*time.Second, ctx.Stop) // 7. 注册 debug 接口 \treturn garbagecollector.NewDebugHandler(garbageCollector), true, nil } 在 startGarbageCollectorController() 中主要调用了 4 个方法：\n garbagecollector.NewGarbageCollector() garbageCollector.Run() garbageCollector.Sync() garbagecollector.NewDebugHandler() 其中 garbagecollector.NewGarbageCollector() 只是初始化 GarbageCollector 和 GraphBuilder 对象，核心逻辑都在 Run() 和 Sync() 中，下面分别来看这几个方法分别做了那些事。  2.1.1 garbageCollector.Run() #  garbageCollector.Run() 方法主要作用是启动生产者和消费者。生产者就是 monitors，监听集群中的资源对象，将产生的新事件分别放入 attemptToDelete 和 attemptToOrphan 两个队列中。消费者就是处理这 2 个队列中的事件，要么删除对象，要么更新对象依赖关系。该方法的核心在于 gc.dependencyGraphBuilder.Run() 启动生产者和 for 循环启动消费者。\npkg/controller/garbagecollector/garbagecollector.go:122\nfunc (gc *GarbageCollector) Run(workers int, stopCh \u0026lt;-chan struct{}) { ... // 1. 启动所有 monitors 即 informers，监听集群资源 \t// 并启动一个协程，处理 graphChanges 中事件， \t// 然后经过处理，分别放入 GraphBuilder 的 attemptToDelete 和 attemptToOrphan 两个队列中 \tgo gc.dependencyGraphBuilder.Run(stopCh) // 2. 等待 informers 的 cache 同步完成 \tif !cache.WaitForNamedCacheSync(\u0026#34;garbage collector\u0026#34;, stopCh, gc.dependencyGraphBuilder.IsSynced) { return } klog.Infof(\u0026#34;Garbage collector: all resource monitors have synced. Proceeding to collect garbage\u0026#34;) // gc workers \tfor i := 0; i \u0026lt; workers; i++ { // 3. 启动多个协程，处理 attemptToDelete 队列中的事件 \tgo wait.Until(gc.runAttemptToDeleteWorker, 1*time.Second, stopCh) // 4. 启动多个协程，处理 attemptToOrphan 队列中的事件 \tgo wait.Until(gc.runAttemptToOrphanWorker, 1*time.Second, stopCh) } \u0026lt;-stopCh } GraphBuilder GraphBuilder 在整个垃圾收集的过程中，起到了承上启下的作用。首先看下它的结构：\npkg/controller/garbagecollector/graph_builder.go:73\ntype GraphBuilder struct { restMapper meta.RESTMapper // monitor 就是 informer，一个 monitor list/watch 一种资源 \tmonitors monitors monitorLock sync.RWMutex // 当 kube-controller-manager 中所有的 controllers 都启动后，就会被 close 掉 \tinformersStarted \u0026lt;-chan struct{} // stopCh 是接收 shutdown 信号 \tstopCh \u0026lt;-chan struct{} // 当调用 GraphBuilder 的 run 方法时，running 会被设置为 true \trunning bool metadataClient metadata.Interface // monitors 监听到的事件会放在 graphChanges 中 \tgraphChanges workqueue.RateLimitingInterface // uidToNode 维护所有对象的依赖关系 \tuidToNode *concurrentUIDToNode // GarbageCollector 作为消费者要处理 attemptToDelete 和 attemptToOrphan 两个队列中的事件 \tattemptToDelete workqueue.RateLimitingInterface attemptToOrphan workqueue.RateLimitingInterface // absentOwnerCache 存放已知不存在的对象 \tabsentOwnerCache *UIDCache sharedInformers controller.InformerFactory // 不需要被 gc 的资源，默认是空 \tignoredResources map[schema.GroupResource]struct{} } 其中 uidToNode 字段作为维护对象之间依赖关系，比如创建一个 Deployment 时，会创建 ReplicaSet，ReplicaSet 才会创建 Pod。那么 Pod 的 owner 是 ReplicaSet，ReplicaSet 的 owner 是 Deployment。uidToNode 字段的结构定义如下：\npkg/controller/garbagecollector/graph.go:162\ntype concurrentUIDToNode struct { uidToNodeLock sync.RWMutex uidToNode map[types.UID]*node } type node struct { identity objectReference dependentsLock sync.RWMutex // dependents 保存的是 owner 是自己的对象集合 \tdependents map[*node]struct{} // this is set by processGraphChanges() if the object has non-nil DeletionTimestamp \t// and has the FinalizerDeleteDependents. \tdeletingDependents bool deletingDependentsLock sync.RWMutex // this records if the object\u0026#39;s deletionTimestamp is non-nil. \tbeingDeleted bool beingDeletedLock sync.RWMutex // 如果 virtual 为 true，表示这个对象不是通过 informer 观察到的，是虚拟构建的 \tvirtual bool virtualLock sync.RWMutex // owners 保存的是自己的 owner \towners []metav1.OwnerReference } GraphBuilder 主要有三个功能：\n 监控集群中所有的可删除资源； 基于 informers 中的资源在 uidToNode 数据结构中维护着所有对象的依赖关系； 处理 graphChanges 中的事件并放到 attemptToDelete 和 attemptToOrphan 两个队列中；  gc.dependencyGraphBuilder.Run() 继续回到 gc.dependencyGraphBuilder.Run() 方法，它的功能上文已经提到，就是启动生产者。代码如下：\npkg/controller/garbagecollector/graph_builder.go:290\nfunc (gb *GraphBuilder) Run(stopCh \u0026lt;-chan struct{}) { klog.Infof(\u0026#34;GraphBuilder running\u0026#34;) defer klog.Infof(\u0026#34;GraphBuilder stopping\u0026#34;) // 1. 设置 stop channel \tgb.monitorLock.Lock() gb.stopCh = stopCh gb.running = true gb.monitorLock.Unlock() // 2. 启动 monitor，除非 stop channel 收到信号 \tgb.startMonitors() // 调用 gb.runProcessGraphChanges，分类事件，放入 2 个队列中 \t// 此处为死循环，除非收到 stopCh 信号，否则下面的代码不会被执行到 \twait.Until(gb.runProcessGraphChanges, 1*time.Second, stopCh) // 代码走到这里，说明 stopCh 收到了信号，停止所有 monitor \tgb.monitorLock.Lock() defer gb.monitorLock.Unlock() monitors := gb.monitors stopped := 0 for _, monitor := range monitors { if monitor.stopCh != nil { stopped++ close(monitor.stopCh) } } // reset monitors so that the graph builder can be safely re-run/synced. \tgb.monitors = nil klog.Infof(\u0026#34;stopped %d of %d monitors\u0026#34;, stopped, len(monitors)) } 继续看 gb.runProcessGraphChanges() 方法，这里是生产者的核心逻辑。总结一下：\n 从 graphChanges 队列中取出一个 item 即 event； 获取 event 的 accessor，accessor 是一个 object 的 meta.Interface，里面包含访问 object meta 中所有字段的方法； 通过 accessor 获取 UID 判断 uidToNode 中是否存在该 object；  根据对象是否存在以及事件类型，分成三种情况\n 若 uidToNode 中不存在该 node 且该事件是 addEvent 或 updateEvent  则为该 object 创建对应的 node，并调用 gb.insertNode() 将该 node 加到 uidToNode 中，然后将该 node 添加到其 owner 的 dependents 中，执行完 gb.insertNode() 中的操作后再调用 gb.processTransitions() 方法判断该对象是否处于删除状态，若处于删除状态会判断该对象是以 orphan 模式删除还是以 foreground 模式删除，若以 orphan 模式删除，则将该 node 加入到 attemptToOrphan 队列中，若以 foreground 模式删除则将该对象以及其所有 dependents 都加入到 attemptToDelete 队列中；\n 若 uidToNode 中存在该 node 且该事件是 addEvent 或 updateEvent  此时可能是一个 update 操作，调用 referencesDiffs() 方法检查该对象的 OwnerReferences 字段是否有变化，若有变化\n 调用 gb.addUnblockedOwnersToDeleteQueue() 将被删除以及更新的 owner 对应的 node 加入到 attemptToDelete 中，因为此时该 node 中已被删除或更新的 owner 可能处于删除状态且阻塞在该 node 处，此时有三种方式避免该 node 的 owner 处于删除阻塞状态，一是等待该 node 被删除，二是将该 node 自身对应 owner 的 OwnerReferences 字段删除，三是将该 node 的 OwnerReferences 字段中对应 owner 的 BlockOwnerDeletion 设置为 false； 更新该 node 的 owners 列表； 若有新增的 owner，将该 node 加入到新 owner 的 dependents 中； 若有被删除的 owner，将该 node 从已删除 owner 的 dependents 中删除；以上操作完成后，检查该 node 是否处于删除状态并进行标记，最后调用 gb.processTransitions() 方法检查该 node 是否要被删除；  举个例子，若以 foreground 模式删除 deployment 时，deployment 的 dependents 列表中有对应的 rs，那么 deployment 的删除会阻塞住等待其依赖 rs 的删除，此时 rs 有三种方法不阻塞 deployment 的删除操作，一是 rs 对象被删除，二是删除 rs 对象 OwnerReferences 字段中对应的 deployment，三是将 rs 对象 OwnerReferences 字段中对应的 deployment 配置 BlockOwnerDeletion 设置为 false，文末会有示例演示该操作。\n 若该事件为 deleteEvent 首先从 uidToNode 中删除该对象，然后从该 node 所有 owners 的 dependents 中删除该对象，将该 node 所有的 dependents 加入到 attemptToDelete 队列中，最后检查该 node 的所有 owners，若有处于删除状态的 owner，此时该 owner 可能处于删除阻塞状态正在等待该 node 的删除，将该 owner 加入到 attemptToDelete 中；  pkg/controller/garbagecollector/graph_builder.go:530\nfunc (gb *GraphBuilder) runProcessGraphChanges() { for gb.processGraphChanges() { } } func (gb *GraphBuilder) processGraphChanges() bool { // 1. 从 graphChanges 取出一个 event \titem, quit := gb.graphChanges.Get() if quit { return false } defer gb.graphChanges.Done(item) event, ok := item.(*event) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;expect a *event, got %v\u0026#34;, item)) return true } obj := event.obj accessor, err := meta.Accessor(obj) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;cannot access obj: %v\u0026#34;, err)) return true } klog.V(5).Infof(\u0026#34;GraphBuilder process object: %s/%s, namespace %s, name %s, uid %s, event type %v\u0026#34;, event.gvk.GroupVersion().String(), event.gvk.Kind, accessor.GetNamespace(), accessor.GetName(), string(accessor.GetUID()), event.eventType) // 2. 若存在 node 对象，从 uidToNode 中取出该 event 的 node 对象 \texistingNode, found := gb.uidToNode.Read(accessor.GetUID()) if found { existingNode.markObserved() } switch { // 3.1 若 event 为 add 或 update 类型且对应的 node 对象不存在时，表示新建对象 \tcase (event.eventType == addEvent || event.eventType == updateEvent) \u0026amp;\u0026amp; !found: // 3.1.1 为 node 创建 event 对象 \tnewNode := \u0026amp;node{ identity: objectReference{ OwnerReference: metav1.OwnerReference{ APIVersion: event.gvk.GroupVersion().String(), Kind: event.gvk.Kind, UID: accessor.GetUID(), Name: accessor.GetName(), }, Namespace: accessor.GetNamespace(), }, dependents: make(map[*node]struct{}), owners: accessor.GetOwnerReferences(), deletingDependents: beingDeleted(accessor) \u0026amp;\u0026amp; hasDeleteDependentsFinalizer(accessor), beingDeleted: beingDeleted(accessor), } // 3.1.2 在 uidToNode 中添加该 node 对象 \tgb.insertNode(newNode) // 3.1.3 检查对象的删除策略，并放到对应的队列中 \tgb.processTransitions(event.oldObj, accessor, newNode) // 3.2 若 event 为 add 或 update 类型且对应的 node 对象存在时，表示更新对象 \tcase (event.eventType == addEvent || event.eventType == updateEvent) \u0026amp;\u0026amp; found: // 3.2.1 检查当前对象和老对象的 owner 的不同 \tadded, removed, changed := referencesDiffs(existingNode.owners, accessor.GetOwnerReferences()) if len(added) != 0 || len(removed) != 0 || len(changed) != 0 { // 3.2.2 检查依赖变更是否解除了处于等待依赖删除的 owner 的阻塞状态 \tgb.addUnblockedOwnersToDeleteQueue(removed, changed) // 3.2.3 更新 node 本身 \texistingNode.owners = accessor.GetOwnerReferences() // 3.2.4 添加 node 到它的 owner 依赖中 \tgb.addDependentToOwners(existingNode, added) // 3.2.5 删除老的 owners 中的依赖 \tgb.removeDependentFromOwners(existingNode, removed) } if beingDeleted(accessor) { existingNode.markBeingDeleted() } // 3.2.6 检查对象的删除策略，并放到对应的队列中 \tgb.processTransitions(event.oldObj, accessor, existingNode) // 3.3 若为 delete event \tcase event.eventType == deleteEvent: if !found { klog.V(5).Infof(\u0026#34;%v doesn\u0026#39;t exist in the graph, this shouldn\u0026#39;t happen\u0026#34;, accessor.GetUID()) return true } // 3.3.1 从 uidToNode 中删除该 node \tgb.removeNode(existingNode) existingNode.dependentsLock.RLock() defer existingNode.dependentsLock.RUnlock() if len(existingNode.dependents) \u0026gt; 0 { gb.absentOwnerCache.Add(accessor.GetUID()) } // 3.3.2 删除该 node 的 dependents \tfor dep := range existingNode.dependents { gb.attemptToDelete.Add(dep) } // 3.3.2 删除该 node 处于删除阻塞状态的 owner \tfor _, owner := range existingNode.owners { ownerNode, found := gb.uidToNode.Read(owner.UID) if !found || !ownerNode.isDeletingDependents() { continue } gb.attemptToDelete.Add(ownerNode) } } return true } 2.1.2 gc.runAttemptToDeleteWorker() #  gc.runAttemptToDeleteWorker() 方法就是将 attemptToDelete 队列中的对象取出，并删除，如果删除失败则重进队列重试。\npkg/controller/garbagecollector/garbagecollector.go:285\nfunc (gc *GarbageCollector) runAttemptToDeleteWorker() { for gc.attemptToDeleteWorker() { } } func (gc *GarbageCollector) attemptToDeleteWorker() bool { // 取出对象  item, quit := gc.attemptToDelete.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit { return false } defer gc.attemptToDelete.Done(item) n, ok := item.(*node) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;expect *node, got %#v\u0026#34;, item)) return true } // 实际执行删除的方法 gc.attemptToDeleteItem() \terr := gc.attemptToDeleteItem(n) if err != nil { if _, ok := err.(*restMappingError); ok { klog.V(5).Infof(\u0026#34;error syncing item %s: %v\u0026#34;, n, err) } else { utilruntime.HandleError(fmt.Errorf(\u0026#34;error syncing item %s: %v\u0026#34;, n, err)) } // 删除失败则重新进入队列等待重试 \tgc.attemptToDelete.AddRateLimited(item) } else if !n.isObserved() { klog.V(5).Infof(\u0026#34;item %s hasn\u0026#39;t been observed via informer yet\u0026#34;, n.identity) // 如果 node 不是 informer 发现的，则也要删除 \tgc.attemptToDelete.AddRateLimited(item) } return true } gc.runAttemptToDeleteWorker() 中调用了 gc.attemptToDeleteItem() 执行实际的删除操作。下面继续来看 gc.attemptToDeleteItem() 的实现细节：\n 判断 node 是否处于删除状态； 从 apiserver 获取该 node 最新的状态，该 node 可能为 virtual node，若为 virtual node 则从 apiserver 中获取不到该 node 的对象，此时会将该 node 重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 判断该 node 最新状态的 uid 是否等于本地缓存中的 uid，若不匹配说明该 node 已更新过此时将其设置为 virtual node 并重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 通过 node 的 deletingDependents 字段判断该 node 当前是否处于删除 dependents 的状态，若该 node 处于删除 dependents 的状态则调用 processDeletingDependentsItem 方法检查 node 的 blockingDependents 是否被完全删除，若 blockingDependents 已完全被删除则删除该 node 对应的 finalizer，若 blockingDependents 还未删除完，将未删除的 blockingDependents 加入到 attemptToDelete 中；上文中在 GraphBuilder 处理 graphChanges 中的事件时，若发现 node 处于删除状态，会将 node 的 dependents 加入到 attemptToDelete 中并标记 node 的 deletingDependents 为 true； 调用 gc.classifyReferences 将 node 的 ownerReferences 分类为 solid, dangling, waitingForDependentsDeletion 三类：dangling(owner 不存在）、waitingForDependentsDeletion(owner 存在，owner 处于删除状态且正在等待其 dependents 被删除）、solid（至少有一个 owner 存在且不处于删除状态）；对以上分类进行不同的处理：  第一种情况是若 solid 不为 0 即当前 node 至少存在一个 owner，该对象还不能被回收，此时需要将 dangling 和 waitingForDependentsDeletion 列表中的 owner 从 node 的 ownerReferences 删除，即已经被删除或等待删除的引用从对象中删掉； 第二种情况是该 node 的 owner 处于 waitingForDependentsDeletion 状态并且 node 的 dependents 未被完全删除，该 node 需要等待删除完所有的 dependents 后才能被删除； 第三种情况就是该 node 已经没有任何 dependents 了，此时按照 node 中声明的删除策略调用 apiserver 的接口删除即可；    pkg/controller/garbagecollector/garbagecollector.go:409\nfunc (gc *GarbageCollector) attemptToDeleteItem(item *node) error { klog.V(2).InfoS(\u0026#34;Processing object\u0026#34;, \u0026#34;object\u0026#34;, klog.KRef(item.identity.Namespace, item.identity.Name), \u0026#34;objectUID\u0026#34;, item.identity.UID, \u0026#34;kind\u0026#34;, item.identity.Kind) // 1. 判断 node 是否处于删除状态 \tif item.isBeingDeleted() \u0026amp;\u0026amp; !item.isDeletingDependents() { klog.V(5).Infof(\u0026#34;processing item %s returned at once, because its DeletionTimestamp is non-nil\u0026#34;, item.identity) return nil } // 2. 从 apiserver 获取该 node 最新的状态 \tlatest, err := gc.getObject(item.identity) switch { case errors.IsNotFound(err): klog.V(5).Infof(\u0026#34;item %v not found, generating a virtual delete event\u0026#34;, item.identity) gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil case err != nil: return err } // 3. 判断该 node 最新状态的 uid 是否等于本地缓存中的 uid \tif latest.GetUID() != item.identity.UID { klog.V(5).Infof(\u0026#34;UID doesn\u0026#39;t match, item %v not found, generating a virtual delete event\u0026#34;, item.identity) gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil } // 4. 判断该 node 当前是否处于删除 dependents 状态中 \tif item.isDeletingDependents() { return gc.processDeletingDependentsItem(item) } // 5. 检查 node 是否还存在 ownerReferences \townerReferences := latest.GetOwnerReferences() if len(ownerReferences) == 0 { klog.V(2).Infof(\u0026#34;object %s\u0026#39;s doesn\u0026#39;t have an owner, continue on next item\u0026#34;, item.identity) return nil } // 6. 对 ownerReferences 进行分类 \tsolid, dangling, waitingForDependentsDeletion, err := gc.classifyReferences(item, ownerReferences) if err != nil { return err } klog.V(5).Infof(\u0026#34;classify references of %s.\\nsolid: %#v\\ndangling: %#v\\nwaitingForDependentsDeletion: %#v\\n\u0026#34;, item.identity, solid, dangling, waitingForDependentsDeletion) switch { // 7.1 存在不处于删除状态的 owner \tcase len(solid) != 0: klog.V(2).Infof(\u0026#34;object %#v has at least one existing owner: %#v, will not garbage collect\u0026#34;, item.identity, solid) if len(dangling) == 0 \u0026amp;\u0026amp; len(waitingForDependentsDeletion) == 0 { return nil } klog.V(2).Infof(\u0026#34;remove dangling references %#v and waiting references %#v for object %s\u0026#34;, dangling, waitingForDependentsDeletion, item.identity) ownerUIDs := append(ownerRefsToUIDs(dangling), ownerRefsToUIDs(waitingForDependentsDeletion)...) patch := deleteOwnerRefStrategicMergePatch(item.identity.UID, ownerUIDs...) _, err = gc.patch(item, patch, func(n *node) ([]byte, error) { return gc.deleteOwnerRefJSONMergePatch(n, ownerUIDs...) }) return err // 7.2 node 的 owner 处于 waitingForDependentsDeletion 状态并且 node 的 dependents 未被完全删除 \tcase len(waitingForDependentsDeletion) != 0 \u0026amp;\u0026amp; item.dependentsLength() != 0: deps := item.getDependents() // 删除 dependents \tfor _, dep := range deps { if dep.isDeletingDependents() { klog.V(2).Infof(\u0026#34;processing object %s, some of its owners and its dependent [%s] have FinalizerDeletingDependents, to prevent potential cycle, its ownerReferences are going to be modified to be non-blocking, then the object is going to be deleted with Foreground\u0026#34;, item.identity, dep.identity) patch, err := item.unblockOwnerReferencesStrategicMergePatch() if err != nil { return err } if _, err := gc.patch(item, patch, gc.unblockOwnerReferencesJSONMergePatch); err != nil { return err } break } } klog.V(2).Infof(\u0026#34;at least one owner of object %s has FinalizerDeletingDependents, and the object itself has dependents, so it is going to be deleted in Foreground\u0026#34;, item.identity) // 以 Foreground 模式删除 node 对象 \tpolicy := metav1.DeletePropagationForeground return gc.deleteObject(item.identity, \u0026amp;policy) // 7.3 该 node 已经没有任何依赖了，按照 node 中声明的删除策略调用 apiserver 的接口删除 \tdefault: var policy metav1.DeletionPropagation switch { case hasOrphanFinalizer(latest): policy = metav1.DeletePropagationOrphan case hasDeleteDependentsFinalizer(latest): policy = metav1.DeletePropagationForeground default: policy = metav1.DeletePropagationBackground } klog.V(2).InfoS(\u0026#34;Deleting object\u0026#34;, \u0026#34;object\u0026#34;, klog.KRef(item.identity.Namespace, item.identity.Name), \u0026#34;objectUID\u0026#34;, item.identity.UID, \u0026#34;kind\u0026#34;, item.identity.Kind, \u0026#34;propagationPolicy\u0026#34;, policy) return gc.deleteObject(item.identity, \u0026amp;policy) } } 2.1.3 gc.runAttemptToOrphanWorker() #  gc.runAttemptToOrphanWorker() 是处理以 Orphan 模式删除的 node，主要逻辑为：\n 调用 gc.orphanDependents() 删除 owner 所有 dependents OwnerReferences 中的 owner 字段； 调用 gc.removeFinalizer() 删除 owner 的 orphan Finalizer； 以上两步中若有失败的会进行重试；  pkg/controller/garbagecollector/garbagecollector.go:602\nfunc (gc *GarbageCollector) attemptToOrphanWorker() bool { item, quit := gc.attemptToOrphan.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit { return false } defer gc.attemptToOrphan.Done(item) owner, ok := item.(*node) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;expect *node, got %#v\u0026#34;, item)) return true } // we don\u0026#39;t need to lock each element, because they never get updated \towner.dependentsLock.RLock() dependents := make([]*node, 0, len(owner.dependents)) for dependent := range owner.dependents { dependents = append(dependents, dependent) } owner.dependentsLock.RUnlock() err := gc.orphanDependents(owner.identity, dependents) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;orphanDependents for %s failed with %v\u0026#34;, owner.identity, err)) gc.attemptToOrphan.AddRateLimited(item) return true } // update the owner, remove \u0026#34;orphaningFinalizer\u0026#34; from its finalizers list \terr = gc.removeFinalizer(owner, metav1.FinalizerOrphanDependents) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;removeOrphanFinalizer for %s failed with %v\u0026#34;, owner.identity, err)) gc.attemptToOrphan.AddRateLimited(item) } return true } 2.1.4 小结 #  上面的业务逻辑不算复杂，但是方法嵌套有点多，整理一下方法的调用链：\n mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph LR\rp1(garbageCollector.Run) -- p11(gc.dependencyGraphBuilder.Run)\rp11 -- p111(gb.startMonitors)\rp11 -- p112(gb.runProcessGraphChanges)\rp112 -- p1121(gb.processGraphChanges)\rp1121 -- p11211(gb.processTransitions)\rp1 -- p12(gc.runAttemptToDeleteWorker)\rp12 -- p121(gc.attemptToDeleteWorker)\rp121 -- p1211(gc.attemptToDeleteItem)\rp1 -- p13(gc.runAttemptToOrphanWorker)\rp13 -- p131(gc.orphanDependents)\rp13 -- p132(gc.removeFinalizer)\r2.2 garbageCollector.Sync() #  garbageCollector.Sync() 方法主要是周期性地查询集群中的所有资源，过滤出 deletableResource，然后对比已经监控的 deletableResource 是否一致，如果不一致，则更新 GraphBuilder 的 monitors，并重启 monitors 监控新拿到的 deletableResource。主要逻辑：\n 通过调用 GetDeletableResources() 获取集群内所有的 deletableResources 作为 newResources，deletableResources 指支持 “delete”, “list”, “watch” 三种操作的 resource，包括自定义资源 检查 oldResources, newResources 是否一致，不一致则需要同步； 调用 gc.resyncMonitors() 同步 newResources，在 gc.resyncMonitors() 中会重新调用 GraphBuilder 的 syncMonitors() 和 startMonitors() 两个方法完成 monitors 的刷新； 等待 newResources informer 中的 cache 同步完成； 将 newResources 作为 oldResources，继续进行下一轮的同步；  pkg/controller/garbagecollector/garbagecollector.go:168\nfunc (gc *GarbageCollector) Sync(discoveryClient discovery.ServerResourcesInterface, period time.Duration, stopCh \u0026lt;-chan struct{}) { oldResources := make(map[schema.GroupVersionResource]struct{}) wait.Until(func() { // 1. 获取集群内所有的 deletableResources 作为 newResources \tnewResources := GetDeletableResources(discoveryClient) // 正常情况下是不会走到这里，除非发生内部错误 \tif len(newResources) == 0 { klog.V(2).Infof(\u0026#34;no resources reported by discovery, skipping garbage collector sync\u0026#34;) return } // 2. 判断集群中的资源是否有变化 \tif reflect.DeepEqual(oldResources, newResources) { klog.V(5).Infof(\u0026#34;no resource updates from discovery, skipping garbage collector sync\u0026#34;) return } // 加锁保证在 informer 同步完成后处理 event \tgc.workerLock.Lock() defer gc.workerLock.Unlock() // 3. 开始更新 GraphBuilder 中的 monitors \tattempt := 0 wait.PollImmediateUntil(100*time.Millisecond, func() (bool, error) { attempt++ // 尝试次数大于 1，先判断 deletableResource 是否改变 \tif attempt \u0026gt; 1 { newResources = GetDeletableResources(discoveryClient) if len(newResources) == 0 { klog.V(2).Infof(\u0026#34;no resources reported by discovery (attempt %d)\u0026#34;, attempt) return false, nil } } klog.V(2).Infof(\u0026#34;syncing garbage collector with updated resources from discovery (attempt %d): %s\u0026#34;, attempt, printDiff(oldResources, newResources)) gc.restMapper.Reset() klog.V(4).Infof(\u0026#34;reset restmapper\u0026#34;) // 4. 调用 gc.resyncMonitors 同步 newResources \tif err := gc.resyncMonitors(newResources); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;failed to sync resource monitors (attempt %d): %v\u0026#34;, attempt, err)) return false, nil } klog.V(4).Infof(\u0026#34;resynced monitors\u0026#34;) // 5. 等待所有 monitors 的 cache 同步完成 \tif !cache.WaitForNamedCacheSync(\u0026#34;garbage collector\u0026#34;, waitForStopOrTimeout(stopCh, period), gc.dependencyGraphBuilder.IsSynced) { utilruntime.HandleError(fmt.Errorf(\u0026#34;timed out waiting for dependency graph builder sync during GC sync (attempt %d)\u0026#34;, attempt)) return false, nil } return true, nil }, stopCh) // 6. 更新 oldResources \toldResources = newResources klog.V(2).Infof(\u0026#34;synced garbage collector\u0026#34;) }, period, stopCh) } 方法主要调用了 GetDeletableResources() 和 gc.resyncMonitors() 两个方法。前者获取集群中可删除资源，后者更新 monitors。\n2.2.1 GetDeletableResources() #  GetDeletableResources() 中首先通过调用 discoveryClient.ServerPreferredResources() 方法获取集群内所有的 resource 信息，然后通过调用 discovery.FilteredBy() 过滤出支持 “delete”, “list”, “watch” 三种方法的 resource 作为 deletableResources。\npkg/controller/garbagecollector/garbagecollector.go:658\nfunc GetDeletableResources(discoveryClient discovery.ServerResourcesInterface) map[schema.GroupVersionResource]struct{} { // 获取集群内所有的 resource 信息 \tpreferredResources, err := discoveryClient.ServerPreferredResources() ... if preferredResources == nil { return map[schema.GroupVersionResource]struct{}{} } // 过滤出 deletableResources \tdeletableResources := discovery.FilteredBy(discovery.SupportsAllVerbs{Verbs: []string{\u0026#34;delete\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;}}, preferredResources) deletableGroupVersionResources := map[schema.GroupVersionResource]struct{}{} for _, rl := range deletableResources { gv, err := schema.ParseGroupVersion(rl.GroupVersion) if err != nil { klog.Warningf(\u0026#34;ignoring invalid discovered resource %q: %v\u0026#34;, rl.GroupVersion, err) continue } for i := range rl.APIResources { deletableGroupVersionResources[schema.GroupVersionResource{Group: gv.Group, Version: gv.Version, Resource: rl.APIResources[i].Name}] = struct{}{} } } return deletableGroupVersionResources } 2.2.2 gc.resyncMonitors() #  gc.resyncMonitors() 的功能主要是更新 GraphBuilder 的 monitors 并重新启动 monitors 监控所有的 deletableResources，GraphBuilder 的 startMonitors() 方法在前面的流程中已经分析过，此处不再详细说明。syncMonitors() 只不过是拿最新的 deletableResources，把老的 monitors 字段值更新，该删的删，该加的加而已。\npkg/controller/garbagecollector/garbagecollector.go:113\nfunc (gc *GarbageCollector) resyncMonitors(deletableResources map[schema.GroupVersionResource]struct{}) error { if err := gc.dependencyGraphBuilder.syncMonitors(deletableResources); err != nil { return err } gc.dependencyGraphBuilder.startMonitors() return nil } 2.3 garbagecollector.NewDebugHandler() #  garbagecollector.NewDebugHandler() 主要功能是对外提供一个接口供用户查询当前集群中所有资源的依赖关系，依赖关系可以以图表的形式展示。\nfunc NewDebugHandler(controller *GarbageCollector) http.Handler { return \u0026amp;debugHTTPHandler{controller: controller} } func (h *debugHTTPHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) { if req.URL.Path != \u0026#34;/graph\u0026#34; { http.Error(w, \u0026#34;\u0026#34;, http.StatusNotFound) return } var graph graph.Directed if uidStrings := req.URL.Query()[\u0026#34;uid\u0026#34;]; len(uidStrings) \u0026gt; 0 { uids := []types.UID{} for _, uidString := range uidStrings { uids = append(uids, types.UID(uidString)) } graph = h.controller.dependencyGraphBuilder.uidToNode.ToGonumGraphForObj(uids...) } else { graph = h.controller.dependencyGraphBuilder.uidToNode.ToGonumGraph() } data, err := dot.Marshal(graph, \u0026#34;full\u0026#34;, \u0026#34;\u0026#34;, \u0026#34; \u0026#34;) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/vnd.graphviz\u0026#34;) w.Header().Set(\u0026#34;X-Content-Type-Options\u0026#34;, \u0026#34;nosniff\u0026#34;) w.Write(data) w.WriteHeader(http.StatusOK) } 使用该服务的方法如下：\ncurl http://192.168.199.100:10252/debug/controllers/garbagecollector/graph \u0026gt; tmp.dot curl http://192.168.199.100:10252/debug/controllers/garbagecollector/graph?uid=f9555d53-2b5f-4702-9717-54a313ed4fe8 \u0026gt; tmp.dot // 生成 svg 文件 $ dot -Tsvg -o graph.svg tmp.dot // 然后在浏览器中打开 svg 文件 依赖关系如下图所示（点击图片查看更多）：\n \n2.4 总结 #  GarbageCollectorController 是一种典型的生产者消费者模型，所有 deletableResources 的 informer 都是生产者，每种资源的 informer 监听到变化后都会将对应的事件 push 到 graphChanges 中，graphChanges 是 GraphBuilder 对象中的一个数据结构，GraphBuilder 会启动另外的 goroutine 对 graphChanges 中的事件进行分类并放在其 attemptToDelete 和 attemptToOrphan 两个队列中，garbageCollector 会启动多个 goroutine 对 attemptToDelete 和 attemptToOrphan 两个队列中的事件进行处理，处理的结果就是回收一些需要被删除的对象。最后，再用一个流程图总结一下 GarbageCollectorController 的主要流程： graph LR\ra[mirrors] --|produce| b(graphChanges)\rb -- c(processGraphChanges)\rc -- d(attemptToDelete)\rc -- e(attemptToOrphan)\rd --|consume| f[AttemptToDeleteWorker]\re --|consume| g[AttemptToOrphanWorker]\r3. 参考资料 #    垃圾收集  garbage collector controller 源码分析  Kubernetes API 资源对象的删除和 GarbageCollector Controller  "});index.add({'id':5,'href':'/docs/kubernetes/kube-controller-manager/code-analysis-of-daemonset-controller/','title':"DaemonSet Controller 源码分析",'section':"kube-controller-manager",'content':"1. DaemonSet 简介 #  我们知道，Deployment 是用来部署一定数量的 Pod。但是，当你希望 Pod 在集群中的每个节点上运行，并且每个节点上都需要一个 Pod 实例时，Deployment 就无法满足需求。\n这类需求包括 Pod 执行系统级别与基础结构相关的操作，比如：希望在每个节点上运行日志收集器和资源监控组件。另一个典型的例子，就是 Kubernetes 自己的 kube-proxy 进程，它需要在所有节点上都运行，才能使得 Service 正常工作。\n如此，DaemonSet 应运而生。它能确保集群中每个节点或者是满足某些特性的一组节点都运行一个 Pod 副本。当有新节点加入时，也会立即为它部署一个 Pod；当有节点从集群中删除时，Pod 也会被回收。删除 DaemonSet，也会删除所有关联的 Pod。\n1.1 应用场景 #   在每个节点上运行集群存守护进程 在每个节点上运行日志收集守护进程 在每个节点上运行监控守护进程  一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。 一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志，并且对不同硬件类型具有不同的内存、CPU 等要求。\n1.2 基本功能 #   创建 删除  级联删除：kubectl delete ds/nginx-ds 非级联删除：kubectl delete ds/nginx-ds --cascade=false   更新  RollingUpdate OnDelete   回滚  1.3 示例 #  apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can\u0026#39;t run pods - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 2. 源码分析 #   kubernetes version: v1.19\n 2.1 startDaemonSetController() #  与其他资源的 Controller 启动方式一致，在 startDaemonSetController() 中初始化 DaemonSetController 对象，并调用 Run() 方法启动。从该方法可以看出，DaemonSet Controller 关心的是 daemonset、controllerrevision、pod、node 四种资源的变动，其中 ConcurrentDaemonSetSyncs 默认是 2。\ncmd/kube-controller-manager/app/apps.go:36\nfunc startDaemonSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \u0026#34;apps\u0026#34;, Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;daemonsets\u0026#34;}] { return nil, false, nil } dsc, err := daemon.NewDaemonSetsController( ctx.InformerFactory.Apps().V1().DaemonSets(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Core().V1().Nodes(), ctx.ClientBuilder.ClientOrDie(\u0026#34;daemon-set-controller\u0026#34;), flowcontrol.NewBackOff(1*time.Second, 15*time.Minute), ) if err != nil { return nil, true, fmt.Errorf(\u0026#34;error creating DaemonSets controller: %v\u0026#34;, err) } go dsc.Run(int(ctx.ComponentConfig.DaemonSetController.ConcurrentDaemonSetSyncs), ctx.Stop) return nil, true, nil } 2.2 Run() #  Run() 方法中执行 2 个核心操作：sync 和 gc。其中 sync 操作是 controller 的核心代码，响应上述所有操作。在初始化 controller 对象时，指定了 failedPodsBackoff 的参数，defaultDuration = 1s，maxDuration = 15min；gc 的主要作用是发现 daemonset 的 pod 的 phase 为 failed，就会重启该 Pod，如果已经超时（2*15min）会删除该条记录。\npkg/controller/daemon/daemon_controller.go:281\nfunc (dsc *DaemonSetsController) Run(workers int, stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() defer dsc.queue.ShutDown() klog.Infof(\u0026#34;Starting daemon sets controller\u0026#34;) defer klog.Infof(\u0026#34;Shutting down daemon sets controller\u0026#34;) if !cache.WaitForNamedCacheSync(\u0026#34;daemon sets\u0026#34;, stopCh, dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) { return } for i := 0; i \u0026lt; workers; i++ { // sync \tgo wait.Until(dsc.runWorker, time.Second, stopCh) } // gc \tgo wait.Until(dsc.failedPodsBackoff.GC, BackoffGCInterval, stopCh) \u0026lt;-stopCh } 2.3 syncDaemonSet() #  DaemonSet 的 Pod 的创建/删除都是和 Node 相关，所以每次 sync 操作，需要遍历所有的 Node 进行判断。syncDaemonSet() 的主要逻辑为：\n 通过 key 获取 ns 和 name 从 dsLister 获取 ds 对象 从 nodeLister 获取全部 node 对象 获取 dsKey， 即：\u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; 判断 ds 是否处于删除中，如果正在删除，则等待删除完毕后再次进入 sync 获取 cur 和 old controllerrevision 判断是否满足 expectation 机制，expectation 机制就是为了减少不必要的 sync 操作 调用 dsc.manage()，执行实际的 sync 操作 判断是否为更新操作，并执行 调用 dsc.cleanupHistory() 根据 spec.revisionHistroyLimit 清理过期的 controllerrevision 调用 dsc.updateDaemonSetStatus()，更新 status 子资源  pkg/controller/daemon/daemon_controller.go:1129\nfunc (dsc *DaemonSetsController) syncDaemonSet(key string) error { ... // 1. 通过 key 获取 ns 和 name \tnamespace, name, err := cache.SplitMetaNamespaceKey(key) ... // 2. 从 dsLister 获取 ds 对象 \tds, err := dsc.dsLister.DaemonSets(namespace).Get(name) ... // 3. 从 nodeLister 获取全部 node 对象 \tnodeList, err := dsc.nodeLister.List(labels.Everything()) ... // 4. 获取 dsKey， 即：\u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; \tdsKey, err := controller.KeyFunc(ds) ... // 5. 判断 ds 是否处于删除中，如果正在删除，则等待删除完毕后再次进入 sync \tif ds.DeletionTimestamp != nil { return nil } // 6. 获取 cur 和 old controllerrevision \tcur, old, err := dsc.constructHistory(ds) // hash 就是当前 controllerrevision 的 controller-revision-hash 值 \thash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey] // 7. 判断是否满足 expectation 机制 \tif !dsc.expectations.SatisfiedExpectations(dsKey) { // 不满足，只更新 status 子资源 \treturn dsc.updateDaemonSetStatus(ds, nodeList, hash, false) } // 8. 实际执行的 sync 操作 \terr = dsc.manage(ds, nodeList, hash) ... // 9. 判断是否为更新操作，并执行 \tif dsc.expectations.SatisfiedExpectations(dsKey) { switch ds.Spec.UpdateStrategy.Type { case apps.OnDeleteDaemonSetStrategyType: case apps.RollingUpdateDaemonSetStrategyType: err = dsc.rollingUpdate(ds, nodeList, hash) } if err != nil { return err } } // 10. 清理过期的 controllerrevision \terr = dsc.cleanupHistory(ds, old) if err != nil { return fmt.Errorf(\u0026#34;failed to clean up revisions of DaemonSet: %v\u0026#34;, err) } // 11. 最后更新 status 子资源 \treturn dsc.updateDaemonSetStatus(ds, nodeList, hash, true) } 2.4 dsc.manage() #  dsc.manage() 是为了保证 ds 的 Pod 正常运行在应该存在的节点，该方法做了这几件事：\n 调用 dsc.getNodesToDaemonPods()，获取当前的节点和 daemon pod 的映射关系（map[nodeName][]*v1.Pod） 遍历所有节点，判断每个节点是需要创建还是删除 daemonset pod 从 1.12 开始，daemon pod 已经由 kube-scheduler 负责调度，可能会出现把 daemon pod 调度到不存在的节点上，如果存在这种情况，就要删除该 pod 调用 dsc.syncNodes() 为对应的 node 创建 daemon pod 以及删除多余的 pods；  pkg/controller/daemon/daemon_controller.go:881\nfunc (dsc *DaemonSetsController) manage(ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error { // 1. 找到节点和 ds 创建的 Pod 的映射关系（nodeName:[]Pod{}） \tnodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) ... // 2. 检查每个节点是否应当运行该 daemonset 的 Pod，如果不该运行就要删掉，反之就要创建 \tvar nodesNeedingDaemonPods, podsToDelete []string for _, node := range nodeList { nodesNeedingDaemonPodsOnNode, podsToDeleteOnNode, err := dsc.podsShouldBeOnNode( node, nodeToDaemonPods, ds) if err != nil { continue } nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, nodesNeedingDaemonPodsOnNode...) podsToDelete = append(podsToDelete, podsToDeleteOnNode...) } // 3. 调用 getUnscheduledPodsWithoutNode() 方法找到把调度到不存在的节点的 Pod \tpodsToDelete = append(podsToDelete, getUnscheduledPodsWithoutNode(nodeList, nodeToDaemonPods)...) // 4. 调用 dsc.syncNodes()，删除多余的 Pod，为应该运行 daemonset Pod 的 node 创建 Pod \tif err = dsc.syncNodes(ds, podsToDelete, nodesNeedingDaemonPods, hash); err != nil { return err } return nil } 下面继续看下 dsc.podsShouldBeOnNode() 和 dsc.syncNodes() 两个方法的具体逻辑：\n其中，podsShouldBeOnNode() 主要是为了确定在给定节点上的是需要创建还是删除 daemon pod。主要逻辑为：\n 调用 dsc.nodeShouldRunDaemonPod 判断该 node 是否要运行以及是否能继续运行 ds pod 获取该节点上的 daemon pod 列表 根据 shouldRun、shouldContinueRunning 和 exists（daemon pod 的存在状态），进行下一步  如果节点应该运行而没有运行，则创建该 Pod 如果 daemon pods 应该继续在此节点上运行，遍历每个 daemon pod，且  pod 在删除中，暂且不管 pod 处于 failed 状态，则删除 pod daemon pods 数量 \u0026gt; 1，则保留最早创建的 pod，其余都删除   如果 pod 不需要继续运行但 pod 已存在，则需要删除   最终返回需要运行 daemon pod 的节点集合和待删除 pod 的集合  func (dsc *DaemonSetsController) podsShouldBeOnNode( node *v1.Node, nodeToDaemonPods map[string][]*v1.Pod, ds *apps.DaemonSet, ) (nodesNeedingDaemonPods, podsToDelete []string, err error) { // 1. 判断该 node 是否要运行以及是否能继续运行 ds pod \tshouldRun, shouldContinueRunning, err := dsc.nodeShouldRunDaemonPod(node, ds) if err != nil { return } // 2. 获取该节点上的该 daemon pod 列表 \tdaemonPods, exists := nodeToDaemonPods[node.Name] switch { case shouldRun \u0026amp;\u0026amp; !exists: // 3.1 如果节点应该运行而没有运行，则创建该 Pod \tnodesNeedingDaemonPods = append(nodesNeedingDaemonPods, node.Name) case shouldContinueRunning: // 3.2. 如果 daemon pod 应该继续在此节点上运行 \tvar daemonPodsRunning []*v1.Pod for _, pod := range daemonPods { // 3.2.1 pod 在删除中，暂且不管 \tif pod.DeletionTimestamp != nil { continue } // 3.2.2 pod 处于 failed 状态，则删除 pod \tif pod.Status.Phase == v1.PodFailed { // This is a critical place where DS is often fighting with kubelet that rejects pods. \t// We need to avoid hot looping and backoff. \tbackoffKey := failedPodsBackoffKey(ds, node.Name) now := dsc.failedPodsBackoff.Clock.Now() inBackoff := dsc.failedPodsBackoff.IsInBackOffSinceUpdate(backoffKey, now) if inBackoff { delay := dsc.failedPodsBackoff.Get(backoffKey) klog.V(4).Infof(\u0026#34;Deleting failed pod %s/%s on node %s has been limited by backoff - %v remaining\u0026#34;, pod.Namespace, pod.Name, node.Name, delay) dsc.enqueueDaemonSetAfter(ds, delay) continue } dsc.failedPodsBackoff.Next(backoffKey, now) msg := fmt.Sprintf(\u0026#34;Found failed daemon pod %s/%s on node %s, will try to kill it\u0026#34;, pod.Namespace, pod.Name, node.Name) klog.V(2).Infof(msg) // Emit an event so that it\u0026#39;s discoverable to users. \tdsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, FailedDaemonPodReason, msg) podsToDelete = append(podsToDelete, pod.Name) } else { daemonPodsRunning = append(daemonPodsRunning, pod) } } // 3.2.3 如果 daemon pod 应该在此节点上运行，但存在的 pod 数 \u0026gt; 1，则保留最早创建的 pod，其余删除 \tif len(daemonPodsRunning) \u0026gt; 1 { sort.Sort(podByCreationTimestampAndPhase(daemonPodsRunning)) for i := 1; i \u0026lt; len(daemonPodsRunning); i++ { podsToDelete = append(podsToDelete, daemonPodsRunning[i].Name) } } case !shouldContinueRunning \u0026amp;\u0026amp; exists: // 3.3 如果 pod 不需要继续运行但 pod 已存在则需要删除 pod \tfor _, pod := range daemonPods { if pod.DeletionTimestamp != nil { continue } podsToDelete = append(podsToDelete, pod.Name) } } // 4. 最终返回需要运行 daemon pod 的节点集合和待删除 pod 集合 \treturn nodesNeedingDaemonPods, podsToDelete, nil } 继续看 dsc.syncNode()，该方法主要是为需要 daemon pod 的 node 创建 pod 以及删除多余的 pod，其主要逻辑为：\n 将 createDiff 和 deleteDiff 与 burstReplicas 进行比较，burstReplicas 默认值为 250，即每个 syncLoop 中创建或者删除的 pod 数最多为 250 个，若超过其值则剩余需要创建或者删除的 pod 在下一个 syncLoop 继续操作； 将 createDiff 和 deleteDiff 写入到 expectations 中； 并发创建 pod，通过 nodeAffinity 来保证每个节点都运行一个 pod； 并发删除 deleteDiff 中的所有 pod；  func (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error { // 1. 取 ds 的 key (namespace/name) \tdsKey, err := controller.KeyFunc(ds) ... // 2. 设置创删过程中可超过的副本数 \tcreateDiff := len(nodesNeedingDaemonPods) deleteDiff := len(podsToDelete) if createDiff \u0026gt; dsc.burstReplicas { createDiff = dsc.burstReplicas } if deleteDiff \u0026gt; dsc.burstReplicas { deleteDiff = dsc.burstReplicas } // 3. 设置 expectation \tdsc.expectations.SetExpectations(dsKey, createDiff, deleteDiff) // error channel to communicate back failures. make the buffer big enough to avoid any blocking \terrCh := make(chan error, createDiff+deleteDiff) createWait := sync.WaitGroup{} generation, err := util.GetTemplateGeneration(ds) ... template := util.CreatePodTemplate(ds.Spec.Template, generation, hash) // 4. 并发创建 pod，创建的 pod 数依次为 1, 2, 4, 8, ... \tbatchSize := integer.IntMin(createDiff, controller.SlowStartInitialBatchSize) for pos := 0; createDiff \u0026gt; pos; batchSize, pos = integer.IntMin(2*batchSize, createDiff-(pos+batchSize)), pos+batchSize { errorCount := len(errCh) createWait.Add(batchSize) for i := pos; i \u0026lt; pos+batchSize; i++ { go func(ix int) { defer createWait.Done() podTemplate := template.DeepCopy() // 5. 使用节点亲和性完成 daemon pod 调度 \tpodTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity( podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix]) // 6. 创建 Pod 带上 ControllerRef \terr := dsc.podControl.CreatePodsWithControllerRef(ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind)) if err != nil { if errors.HasStatusCause(err, v1.NamespaceTerminatingCause) { // 如果此时 namespace 被删除，这里错误可以忽略 \treturn } } if err != nil { klog.V(2).Infof(\u0026#34;Failed creation, decrementing expectations for set %q/%q\u0026#34;, ds.Namespace, ds.Name) dsc.expectations.CreationObserved(dsKey) errCh \u0026lt;- err utilruntime.HandleError(err) } }(i) } createWait.Wait() // 6. 将创建失败的 Pod 记录到 expectation 中 \tskippedPods := createDiff - (batchSize + pos) if errorCount \u0026lt; len(errCh) \u0026amp;\u0026amp; skippedPods \u0026gt; 0 { klog.V(2).Infof(\u0026#34;Slow-start failure. Skipping creation of %d pods, decrementing expectations for set %q/%q\u0026#34;, skippedPods, ds.Namespace, ds.Name) dsc.expectations.LowerExpectations(dsKey, skippedPods, 0) // skippedPod 会在下轮 sync 时重试 \tbreak } } klog.V(4).Infof(\u0026#34;Pods to delete for daemon set %s: %+v, deleting %d\u0026#34;, ds.Name, podsToDelete, deleteDiff) deleteWait := sync.WaitGroup{} deleteWait.Add(deleteDiff) for i := 0; i \u0026lt; deleteDiff; i++ { // 7. 并发删除 deleteDiff 中的 pod \tgo func(ix int) { defer deleteWait.Done() if err := dsc.podControl.DeletePod(ds.Namespace, podsToDelete[ix], ds); err != nil { dsc.expectations.DeletionObserved(dsKey) if !apierrors.IsNotFound(err) { klog.V(2).Infof(\u0026#34;Failed deletion, decremented expectations for set %q/%q\u0026#34;, ds.Namespace, ds.Name) errCh \u0026lt;- err utilruntime.HandleError(err) } } }(i) } deleteWait.Wait() // 收集错误返回 \terrors := []error{} close(errCh) for err := range errCh { errors = append(errors, err) } return utilerrors.NewAggregate(errors) } 到此，总结一下，dsc.manage() 方法的主要流程：\n mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph LR op1(dsc.manage) -- op2(dsc.getNodesToDaemonPods) op1 -- op3(dsc.podsShouldBeOnNode) op1 -- op4(dsc.syncNodes) op3 -- op5(dsc.nodeShouldRunDaemonPod) 2.5 dsc.rollingUpdate() #  daemonset update 的方式有两种 OnDelete 和 RollingUpdate，当为 OnDelete 时需要用户手动删除每一个 pod 后完成更新操作，当为 RollingUpdate 时，daemonset controller 会自动控制升级进度。\n当为 RollingUpdate 时，主要逻辑为：\n 获取 node 与 daemon pods 的映射关系 根据 controllerrevision 的 hash 值获取所有未更新的 pods； 获取 maxUnavailable, numUnavailable 的 pod 数值，maxUnavailable 是从 ds 的 rollingUpdate 字段中获取的默认值为 1，numUnavailable 的值是通过 daemonset pod 与 node 的映射关系计算每个 node 下是否有 available pod 得到的； 通过 oldPods 获取 oldAvailablePods, oldUnavailablePods 的 pod 列表； 遍历 oldUnavailablePods 列表将需要删除的 pod 追加到 oldPodsToDelete 数组中。oldUnavailablePods 列表中的 pod 分为两种，一种处于更新中，即删除状态，一种处于未更新且异常状态，处于异常状态的都需要被删除； 遍历 oldAvailablePods 列表，此列表中的 pod 都处于正常运行状态，根据 maxUnavailable 值确定是否需要删除该 pod 并将需要删除的 pod 追加到 oldPodsToDelete 数组中； 调用 dsc.syncNodes() 删除 oldPodsToDelete 数组中的 pods，syncNodes 方法在 manage 阶段已经分析过，此处不再赘述；  pkg/controller/daemon/update.go:44\nfunc (dsc *DaemonSetsController) rollingUpdate(ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error { // 1. 获取 node 与 daemon pods 的映射关系 \tnodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) if err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t get node to daemon pod mapping for daemon set %q: %v\u0026#34;, ds.Name, err) } // 2. 获取所有未更新的 pods \t_, oldPods := dsc.getAllDaemonSetPods(ds, nodeToDaemonPods, hash) // 3. 计算 maxUnavailable, numUnavailable 的 pod 数值 \tmaxUnavailable, numUnavailable, err := dsc.getUnavailableNumbers(ds, nodeList, nodeToDaemonPods) if err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t get unavailable numbers: %v\u0026#34;, err) } // 4. 把未更新的 pods 分成 available 和 unavailable \toldAvailablePods, oldUnavailablePods := util.SplitByAvailablePods(ds.Spec.MinReadySeconds, oldPods) // 5. 将 unavailable 状态且没有删除标记的 pods 加入到 oldPodsToDelete 中 \tvar oldPodsToDelete []string klog.V(4).Infof(\u0026#34;Marking all unavailable old pods for deletion\u0026#34;) for _, pod := range oldUnavailablePods { // Skip terminating pods. We won\u0026#39;t delete them again \tif pod.DeletionTimestamp != nil { continue } klog.V(4).Infof(\u0026#34;Marking pod %s/%s for deletion\u0026#34;, ds.Name, pod.Name) oldPodsToDelete = append(oldPodsToDelete, pod.Name) } // 6. 根据 maxUnavailable 值确定是否需要删除 pod \tklog.V(4).Infof(\u0026#34;Marking old pods for deletion\u0026#34;) for _, pod := range oldAvailablePods { if numUnavailable \u0026gt;= maxUnavailable { klog.V(4).Infof(\u0026#34;Number of unavailable DaemonSet pods: %d, is equal to or exceeds allowed maximum: %d\u0026#34;, numUnavailable, maxUnavailable) break } klog.V(4).Infof(\u0026#34;Marking pod %s/%s for deletion\u0026#34;, ds.Name, pod.Name) oldPodsToDelete = append(oldPodsToDelete, pod.Name) numUnavailable++ } // 7. 调用 syncNodes 方法删除 oldPodsToDelete 数组中的 pods \treturn dsc.syncNodes(ds, oldPodsToDelete, []string{}, hash) } 2.6 dsc.updateDaemonSetStatus() #  dsc.updateDaemonSetStatus() 是 sync 动作的最后一步，主要是用来更新 DaemonSet 的 status 子资源。ds.Status 的各个字段如下：\nstatus: collisionCount: 0 # hash 冲突数 currentNumberScheduled: 1 # 已经运行了 DaemonSet Pod 的节点数量 desiredNumberScheduled: 1 # 需要运行该 DaemonSet Pod 的节点数量 numberAvailable: 1 # DaemonSet Pod 状态为 Ready 且运行时间超过 Spec.MinReadySeconds 的节点数量 numberUnavailable: 0 # desiredNumberScheduled - numberAvailable 的节点数量 numberMisscheduled: 0 # 不需要运行 DeamonSet Pod 但是已经运行了的节点数量 numberReady: 1 # DaemonSet Pod 状态为 Ready 的节点数量 observedGeneration: 1 updatedNumberScheduled: 1 # 已经完成 DaemonSet Pod 更新的节点数量 主要逻辑为：\n 调用 dsc.getNodesToDaemonPods() 获取 node 与已存在的 daemon pods 的映射关系； 遍历所有 node，调用 dsc.nodeShouldRunDaemonPod() 判断该 node 是否需要运行 daemon pod，然后计算 status 中的部分字段值； 调用 storeDaemonSetStatus() 更新 ds.status； 判断 ds 是否需要 resync；  pkg/controller/daemon/daemon_controller.go:1075\nfunc (dsc *DaemonSetsController) updateDaemonSetStatus(ds *apps.DaemonSet, nodeList []*v1.Node, hash string, updateObservedGen bool) error { klog.V(4).Infof(\u0026#34;Updating daemon set status\u0026#34;) // 1. 获取 node 与 daemon pods 的映射关系 \tnodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) if err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t get node to daemon pod mapping for daemon set %q: %v\u0026#34;, ds.Name, err) } var desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable int for _, node := range nodeList { // 2. 判断该 node 是否需要运行 daemon pod \tshouldRun, _, err := dsc.nodeShouldRunDaemonPod(node, ds) if err != nil { return err } scheduled := len(nodeToDaemonPods[node.Name]) \u0026gt; 0 // 3. 计算 status 中的字段值 \tif shouldRun { desiredNumberScheduled++ if scheduled { currentNumberScheduled++ // 按照创建时间排序，最早创建在最前 \tdaemonPods, _ := nodeToDaemonPods[node.Name] sort.Sort(podByCreationTimestampAndPhase(daemonPods)) pod := daemonPods[0] if podutil.IsPodReady(pod) { numberReady++ if podutil.IsPodAvailable(pod, ds.Spec.MinReadySeconds, metav1.Now()) { numberAvailable++ } } // If the returned error is not nil we have a parse error. \t// The controller handles this via the hash. \tgeneration, err := util.GetTemplateGeneration(ds) if err != nil { generation = nil } if util.IsPodUpdated(pod, hash, generation) { updatedNumberScheduled++ } } } else { if scheduled { numberMisscheduled++ } } } numberUnavailable := desiredNumberScheduled - numberAvailable // 4. 更新 ds.status \terr = storeDaemonSetStatus(dsc.kubeClient.AppsV1().DaemonSets(ds.Namespace), ds, desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable, numberUnavailable, updateObservedGen) if err != nil { return fmt.Errorf(\u0026#34;error storing status for daemon set %#v: %v\u0026#34;, ds, err) } // Resync the DaemonSet after MinReadySeconds as a last line of defense to guard against clock-skew. \t// 5. 判断 ds 是否需要 resync \tif ds.Spec.MinReadySeconds \u0026gt; 0 \u0026amp;\u0026amp; numberReady != numberAvailable { dsc.enqueueDaemonSetAfter(ds, time.Duration(ds.Spec.MinReadySeconds)*time.Second) } return nil } 2.7 源码分析小结 #  graph LR op(startDaeonSetController) -- op0(dsc.Run) op0 -- op1(dsc.syncDaemonSet) op1 -- op1.1(dsc.manage) op1 -- op1.2(dsc.rollingUpdate) op1 -- op1.3(dsc.updateDaemonSetStatus) op1.1 -- op1.1.1(dsc.getNodesToDaemonPods) op1.1 -- op1.1.2(dsc.podsShouldBeOnNode) op1.1 -- op1.1.3(dsc.syncNodes) op1.1.2 -- op1.1.2.1(dsc.nodeShouldRunDaemonPod) 3. 总结 #  在 daemonset controller 中可以看到许多功能都是 deployment 和 statefulset 已有的。在创建 pod 的流程与 replicaset controller 创建 pod 的流程是相似的，都使用了 expectations 机制并且限制了在一个 syncLoop 中最多创建或删除的 pod 数。更新方式与 statefulset 一样都有 OnDelete 和 RollingUpdate 两种， OnDelete 方式与 statefulset 相似，都需要手动删除对应的 pod，而 RollingUpdate 方式与 statefulset 和 deployment 都有点区别，RollingUpdate 方式更新时不支持暂停操作并且 pod 是先删除再创建的顺序进行。版本控制方式与 statefulset 的一样都是使用 controllerRevision。最后要说的一点是在 v1.12 及以后的版本中，使用 daemonset 创建的 pod 已不再使用直接指定 .spec.nodeName 的方式绕过调度器进行调度，而是走默认调度器通过 nodeAffinity 的方式调度到每一个节点上。\n4. 参考资料 #    DaemonSet 概念  DaemonSet 滚动更新  daemonset controller 源码分析  "});index.add({'id':6,'href':'/docs/leetcode/0416/','title':"416. 分割等和子集",'section':"LeetCode",'content':"416. 分割等和子集 #   leetcode 链接： https://leetcode-cn.com/problems/partition-equal-subset-sum/\n  给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。\n注意：\n 每个数组中的元素不会超过 100 数组的大小不会超过 200  示例 1:\n输入：[1, 5, 11, 5] 输出：true 解释：数组可以分割成 [1, 5, 5] 和 [11]. 示例 2:\n输入：[1, 2, 3, 5] 输出：false 解释：数组不能分割成两个元素和相等的子集。  方法一：0-1 背包问题\n// 0-1 背包问题 // dp[i][target] 表示 nums[0, i] 区间内是否能找到和为 target 的组合 // 对于每个 nums[i]，如果 nums[i] \u0026lt;= target，可以选择 or 不选，但只要有一个为 true，dp[i][target]=true // dp[i][target] = dp[i-1][target] || dp[i][target-nums[i]] // 如果 nums[i] \u0026gt; target，只能不选，故： // dp[i][target] = dp[i-1][target] func canPartition(nums []int) bool { n := len(nums) if n \u0026lt; 2 { return false } sum := 0 maxNum := 0 for _, num := range nums { sum += num if num \u0026gt; maxNum { maxNum = num } } // 和为奇数 \tif sum%2 != 0 { return false } // 最大值超过和的一半 \ttarget := sum / 2 if maxNum \u0026gt; target { return false } var dp = make([][]bool, n) for i := range dp { dp[i] = make([]bool, target+1) // nums 数组均为正数，如果不选取任何 nums[i]，则被选取的正整数的和等于 0 \tdp[i][0] = true } // 只有一个元素 nums[0] 可取时，构成初始值 dp[0][nums[0]] 为 true \tdp[0][nums[0]] = true for i := 1; i \u0026lt; n; i++ { for j := 1; j \u0026lt;= target; j++ { if nums[i] \u0026lt;= j { dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i]] } else { dp[i][j] = dp[i-1][j] } // 剪枝 \tif dp[i][target] { return true } } } return dp[n-1][target] } 方法 2：动态规划降维\nfunc canPartition(nums []int) bool { n := len(nums) if n \u0026lt; 2 { return false } sum := 0 maxNum := 0 for _, num := range nums { sum += num if num \u0026gt; maxNum { maxNum = num } } // 和为奇数 \tif sum%2 != 0 { return false } // 最大值超过和的一半 \ttarget := sum / 2 if maxNum \u0026gt; target { return false } var dp = make([]bool, target+1) dp[0] = true dp[nums[0]] = true for i := 1; i \u0026lt; len(nums); i++ { // 采用逆序，如果采用正序 dp[j-nums[i]] 会被之前的操作更新为新值 \t// dp[j](new) = dp[j](old) || dp[j - nums[i]](old) \tfor j := target; j \u0026gt;= nums[i]; j-- { if dp[target] { return true } dp[j] = dp[j] || dp[j-nums[i]] } } return dp[target] } "});index.add({'id':7,'href':'/docs/kubernetes/kube-apiserver/garbage-collector/','title':"垃圾回收",'section':"kube-apisever",'content':"1. 序言 #  1.1 什么是垃圾回收 #  参考 Java 中的概念，垃圾回收（Garbage Collection）是 JVM 垃圾回收器提供的一种用于在空闲时间不定时回收无任何对象引用的对象占据的内存空间的一种机制。 垃圾回收回收的是无任何引用的对象占据的内存空间而不是对象本身。换言之，垃圾回收只会负责释放那些对象占有的内存。 对象是个抽象的词，包括引用和其占据的内存空间。当对象没有任何引用时其占据的内存空间随即被收回备用，此时对象也就被销毁。\n因此，垃圾回收关注的是无任何引用的对象。在 kubernetes 中，对象的引用关系又是怎样的呢？\n1.2 k8s 中的对象引用 #  某些 kubernetes 对象是其他一些对象的属主。例如一个 ReplicaSet 是一组 Pod 的属主；反之这组 Pod 就是此 ReplicaSet 的附属。 每个附属对象具有一个指向属主对象的 metadata.ownerReference 字段。\nKubernetes 会自动为 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、Job 和 CronJob 自动设置 ownerReference 的值。 也可以通过手动设置 ownerReference 的值，来指定属主和附属之间的关系。\n先看一个 Pod 的详细信息，例如下面的配置显示 Pod 的属主是名为 my-replicaset 的 ReplicaSet：\napiVersion: v1 kind: Pod metadata: ... ownerReferences: - apiVersion: apps/v1 controller: true blockOwnerDeletion: true kind: ReplicaSet name: my-rs uid: d9607e19-f88f-11e6-a518-42010a800195 ... 下面的源码是 ownerReference 的结构：\ntype OwnerReference struct { APIVersion string `json:\u0026#34;apiVersion\u0026#34; protobuf:\u0026#34;bytes,5,opt,name=apiVersion\u0026#34;` Kind string `json:\u0026#34;kind\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=kind\u0026#34;` Name string `json:\u0026#34;name\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=name\u0026#34;` UID types.UID `json:\u0026#34;uid\u0026#34; protobuf:\u0026#34;bytes,4,opt,name=uid,casttype=k8s.io/apimachinery/pkg/types.UID\u0026#34;` Controller *bool `json:\u0026#34;controller,omitempty\u0026#34; protobuf:\u0026#34;varint,6,opt,name=controller\u0026#34;` BlockOwnerDeletion *bool `json:\u0026#34;blockOwnerDeletion,omitempty\u0026#34; protobuf:\u0026#34;varint,7,opt,name=blockOwnerDeletion\u0026#34;` } 上面的结构中没有Namespace 属性，这是为什么呢？\n 根据设计，kubernetes 不允许跨命名空间指定属主。也就是说：\n 有 Namespace 属性的附属，只能指定同一 Namespace 中的或者集群范围的属主。 Cluster 级别的附属，只能指定集群范围的属主，不能指定命名空间范围的属主。   如此一来，有 Namespace 属性的对象，它的属主要么是与自己是在同一个 Namespace 下，可以复用；要么是集群级别的对象，没有 Namespace 属性。 而 Cluster 级别的对象，它的属主也必须是 Cluster 级别，没有 Namespace 属性。 因此，OwnerReference 结构中不需要 Namespace 属性。\n上面 OwnerReference 的结构体中，最后一个字段 BlockOwnerDeletion 字面意思就是阻止属主删除，那么 k8s 在删除对象上与垃圾收集有什么关系？ 垃圾收集具体是如何执行的呢？后文继续分析。\n2. k8s 中的垃圾回收 #  删除对象时，可以指定该对象的附属是否也自动删除。Kubernetes 中有三种删除模式：\n 级联删除  Foreground 模式 Background 模式   非级联删除  Orphan 模式    在 kubernetes v1.9 版本之前，大部分 controller 的默认删除策略为 Orphan，从 v1.9 开始，对 apps/v1 下的资源默认使用 Background 模式。\n针对 Pod 存在特有的删除方式：Gracefully terminate，允许优雅地终止容器。 先发送 TERM 信号，过了宽限期还未终止，则发送 KILL 信号。 kube-apiserver 先设置 ObjectMeta.DeletionGracePeriodSeconds，默认为 30s， 再由 kubelet 发送删除请求，请求参数中 DeleteOptions.GracePeriodSeconds = 0， kube-apiserver 判断到 lastGraceful = options.GracePeriodSeconds = 0，就直接删除对象了。\n2.1 Foreground 模式 #  在 Foreground 模式下，待删除对象首先进入 deletion in progress 状态。 在此状态下存在如下的场景：\n 对象仍然可以通过 REST API 获取。 会设置对象的 deletionTimestamp 字段。 对象的 metadata.finalizers 字段包含值 foregroundDeletion。  只有对象被设置为 deletion in progress 状态时，垃圾收集器才会删除对象的所有附属。 垃圾收集器在删除了所有有阻塞能力的附属（对象的 ownerReference.blockOwnerDeletion=true） 之后，再删除属主对象。\n 注意，在 Foreground 模式下，只有设置了 ownerReference.blockOwnerDeletion=true 的附属才能阻止属主对象被删除。 在 Kubernetes 1.7 版本增加了准入控制器，基于属主对象上的删除权限来限制用户设置 ownerReference.blockOwnerDeletion=true， 这样未经授权的附属不能够阻止属主对象的删除。\n 如果一个对象的 ownerReference 字段被一个控制器（例如 Deployment 或 ReplicaSet）设置， blockOwnerDeletion 也会被自动设置，不需要手动修改。\n2.2 Background 模式 #  在 Background 模式下，Kubernetes 会立即删除属主对象，之后垃圾收集器会在后台删除其附属对象。\n2.3 Orphan 模式 #  与 Foreground 模式类似，待删除对象首先进入 deletion in progress 状态。 在此状态下存在如下的场景：\n 对象仍然可以通过 REST API 获取。 会设置对象的 deletionTimestamp 字段。 对象的 metadata.finalizers 字段包含值 orphan。  与 Foreground 模式不同的是，不会自动删除它的依赖或者是子对象，这些残留的依赖被称作是原对象的孤儿对象。\n例如执行以下命令时，会使用 Orphan 策略进行删除，此时 ds 的依赖对象 controllerrevision 不会被删除：\nkubectl delete rs my-rs --cascade=false 或者使用 curl 命令：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-rs \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; 2.4 finalizer 机制 #  finalizer 是在对象删除之前需要执行的逻辑。每当 finalizer 成功运行之后，就会将它自己从 Finalizers 数组中删除， 当最后一个 finalizer 被删除之后，API Server 就会删除该对象。finalizer 提供了一个通用的 API， 它的功能不只是用于阻止级联删除，还能过通过它在对象删除之前加入钩子。\ntype ObjectMeta struct { // ... \tFinalizers []string } k8s 中默认有两种 finalizer：OrphanFinalizer 和 ForegroundFinalizer，分别对应 Orphan 模式和 Foreground 模式。 当然，finalizer 不仅仅支持以上两种字段，在使用自定义 controller 时，也可以在 CustomResource 中设置自定义的 finalizer 标识。\n在默认情况下，删除一个对象会删除它的全部依赖，但是在一些特定情况下，我们只是想删除当前对象本身并不想造成复杂地级联删除， 如此 OrphanFinalizer 应运而生。OrphanFinalizer 会监听对象的更新事件并将它自己从它全部依赖对象的 OwnerReferences 数组中删除， 与此同时会删除所有依赖对象中已经失效的 OwnerReferences 并将 OrphanFinalizer 从 Finalizers 数组中删除。\n通过 OrphanFinalizer 我们能够在删除一个 Kubernetes 对象时保留它的全部依赖，为使用者提供一种更灵活的方法来保留和删除对象。\n3. 参考资料 #    垃圾收集  garbage collector controller 源码分析  Kubernetes API 资源对象的删除和 GarbageCollector Controller  "});index.add({'id':8,'href':'/docs/kubernetes/kube-controller-manager/code-analysis-of-statefulset-controller/','title':"Statefulset Controller 源码分析",'section':"kube-controller-manager",'content':"1. StatefulSet 简介 #  Statefulset 是为了解决有状态服务的问题，而产生的一种资源类型（Deployment 和 ReplicaSet 是解决无状态服务而设计的）。\n这里可能有人说，MySQL 是有状态服务吧，但我使用的是 Deploment 资源类型，MySQL 的数据通过 PV 的方式存储在第三方文件系统中，也能解决 MySQL 数据存储问题。\n是的，如果你的 MySQL 是单节点，使用 Deployment 类型确实可以解决数据存储问题。但是如果你的有状态服务是集群，且每个节点分片存储的情况下，Deployment 则不适用这种场景，因为 Deployment 不会保证 Pod 的有序性，集群通常需要主节点先启动，从节点在加入集群，Statefulset 则可以保证，其次 Deployment 资源的 Pod 内的 PVC 是共享存储的，而 Statefulset 下的 Pod 内 PVC 是不共享存储的，每个 Pod 拥有自己的独立存储空间，正好满足了分片的需求，实现分片的需求的前提是 Statefulset 可以保证 Pod 重新调度后还是能访问到相同的持久化数据。\n适用 Statefulset 常用的服务有 Elasticsearch 集群，Mogodb集群，Redis 集群等等。\n1.1 特点 #    稳定、唯一的网络标识符\n如: Redis 集群，在 Redis 集群中，它是通过槽位来存储数据的，假如：第一个节点是 0~1000，第二个节点是 1001~2000，第三个节点 2001~3000……，这就使得 Redis 集群中每个节点要通过 ID 来标识自己，如：第二个节点宕机了，重建后它必须还叫第二个节点，或者说第二个节点叫 R2，它必须还叫 R2，这样在获取 1001~2000 槽位的数据时，才能找到数据，否则Redis集群将无法找到这段数据。\n  稳定、持久的存储\n可实现持久存储，新增或减少 Pod，存储不会随之发生变化。\n  有序的、平滑的部署、扩容\n如 MySQL 集群，要先启动主节点， 若从节点没有要求，则可一起启动，若从节点有启动顺序要求，可先启动第一个从节点，接着第二从节点等；这个过程就是有顺序，平滑安全的启动。\n  有序的、自动的缩容和删除\n我们先终止从节点，若从节点是有启动顺序的，那么关闭时，也要按照逆序终止，即启动时是从 S1~S4 以此启动，则关闭时，则是先关闭 S4，然后是 S3，依次关闭，最后在关闭主节点。\n  有序的、自动的滚动更新\nMySQL 在更新时，应该先更新从节点，全部的从节点都更新完了，最后在更新主节点，因为新版本一般可兼容老版本，但是一定要注意，若新版本不兼容老版本就很很麻烦\n  1.2 限制 #   Pod的存储必须使用 PersistVolume 删除或者缩容时，不会删除关联的卷 使用 headless service 关联 Pod，需要手动创建 Pod 的管理策略为 OrderedReady 时使用滚动更新能力，可能需要人工干预  1.3 基本功能 #   创建 删除  级联删除 非级连删除   扩容/缩容：扩容为顺序，缩容则为逆运算，即逆序 更新：与无状态应用不同的是，StatefulSet 是基于 ControllerRevision 保存更新记录  RollingUpdate OnDelete   Pod 管理策略：对于某些分布式系统来说，StatefulSet 的顺序性保证是不必要的，所以引入了 statefulset.spec.podManagementPolicy 字段  OrderedReady Parallel：允许StatefulSet Controller 并行终止所有 Pod，不必按顺序启动或删除 Pod    1.4 示例 #  apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None # headless service selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; podManagementPolicy: \u0026#34;OrderedReady\u0026#34; # default is OrderedReady replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi 2. 源码解析 #   kubernetes version: v1.19\n 2.1 startStatefulSetController() #  startStatefulSetController() 是 StatefulSet Controller 的启动方法，其中调用 statefulset.NewStatefulSetController() 方法进行初始化，然后调用对象的 Run() 方法启动 Controller。其中 ConcurrentStatefulSetSyncs 默认是5，即默认启动 5 个协程处理StatefulSet 相关业务。\n可以看到 StatefulSetController 初始化时直接相关的对象类型分别是 Pod、StatefulSet、PVC 和 ControllerRevision。印证了之前提到的 StatefulSet 的特殊之处：使用 PV 作为存储；ControllerRevision 表示升级/回滚记录。\nfunc startStatefulSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \u0026#34;apps\u0026#34;, Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;statefulsets\u0026#34;}] { return nil, false, nil } go statefulset.NewStatefulSetController( ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Apps().V1().StatefulSets(), ctx.InformerFactory.Core().V1().PersistentVolumeClaims(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.ClientBuilder.ClientOrDie(\u0026#34;statefulset-controller\u0026#34;), ).Run(int(ctx.ComponentConfig.StatefulSetController.ConcurrentStatefulSetSyncs), ctx.Stop) return nil, true, nil } 2.2 ssc.sync() #  run() 方法会通过 informer 同步 cache 并监听 pod、statefulset、pvc 和 controllerrevision 对象的变更事件，然后启动 5 个 worker 协程，每个 worker 调用 sync() 方法，正式进入业务逻辑处理。\nfunc (ssc *StatefulSetController) sync(key string) error { ... // 1. 解析 namespace 和 name \tnamespace, name, err := cache.SplitMetaNamespaceKey(key) ... // 2. 根据 ns 和 name，获取 sts 对象 \tset, err := ssc.setLister.StatefulSets(namespace).Get(name) ... // 3. 获取 selector 对象，用于筛选 Pod \tselector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector) ... // ssc.adoptOrphanRevisions()  // -\u0026gt; ssc.control.AdoptOrphanRevisions()  // -\u0026gt; ssc.controllerHistory.AdoptControllerRevision()  // -\u0026gt; Patch()  // 4、筛选 sts 的孤儿 controllerrevisions，并尝试与 sts 重新关联（添加 ControllerRef） \tif err := ssc.adoptOrphanRevisions(set); err != nil { return err } // 5. 获取 sts 所有关联的 pod  // ssc.getPodsForStatefulSet()  // -\u0026gt; cm.ClaimPods()  // -\u0026gt; m.ClaimObject()  // -\u0026gt; release()/adopt() \tpods, err := ssc.getPodsForStatefulSet(set, selector) ... // 6. 真正执行 sync 操作 \treturn ssc.syncStatefulSet(set, pods) } 则，sync() 的主要逻辑为：\n 根据 ns/name 获取 sts 对象； 获取 sts 的 selector； 调用 ssc.adoptOrphanRevisions() 检查是否有孤儿 controllerrevisions 对象，若有且能匹配 selector 的则添加 ownerReferences 进行关联； 调用 ssc.getPodsForStatefulSet 通过 selector 获取 sts 关联的 pod，若有孤儿 pod 的 label 与 sts 的能匹配则进行关联，若已关联的 pod label 有变化则解除与 sts 的关联关系； 最后调用 ssc.syncStatefulSet 执行真正的 sync 操作；  2.3 ssc.syncStatefulSet() #  在 syncStatefulSet() 中仅仅是调用了 ssc.control.UpdateStatefulSet() 方法进行处理。ssc.control.UpdateStatefulSet() 会调用 ssc.performUpdate() 方法，最终走到更新逻辑 ssc.updateStatefulSet() 和 ssc.updateStatefulSetStatus()。\nfunc (ssc *StatefulSetController) syncStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error { ... // ssc.control.UpdateStatefulSet()  // -\u0026gt; ssc.performUpdate()  // -\u0026gt; ssc.updateStatefulSet() + ssc.updateStatefulSetStatus() \tif err := ssc.control.UpdateStatefulSet(set.DeepCopy(), pods); err != nil { return err } ... return nil } func (ssc *defaultStatefulSetControl) UpdateStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error { // 1. 获取历史 revisions，并排序 \trevisions, err := ssc.ListRevisions(set) ... history.SortControllerRevisions(revisions) // 2. 计算 currentRevision 和 updateRevision \tcurrentRevision, updateRevision, err := ssc.performUpdate(set, pods, revisions) if err != nil { return utilerrors.NewAggregate([]error{err, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)}) } // 3. 清理过期的历史版本 \treturn ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision) } func (ssc *defaultStatefulSetControl) performUpdate( set *apps.StatefulSet, pods []*v1.Pod, revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, error) { // 2.1. 计算 currentRevision 和 updateRevision \tcurrentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions) if err != nil { return currentRevision, updateRevision, err } // 2.2. 执行实际的 sync 操作 \tstatus, err := ssc.updateStatefulSet(set, currentRevision, updateRevision, collisionCount, pods) if err != nil { return currentRevision, updateRevision, err } // 2.3. 更新 sts 状态 \terr = ssc.updateStatefulSetStatus(set, status) if err != nil { return currentRevision, updateRevision, err } ... return currentRevision, updateRevision, nil } 整体来看，ssc.control.UpdateStatefulSet() 方法的主要逻辑为：\n 获取历史 revisions； 计算 currentRevision 和 updateRevision，若 sts 处于更新过程中则 currentRevision 和 updateRevision 值不同； 调用 ssc.updateStatefulSet() 执行实际的 sync 操作； 调用 ssc.updateStatefulSetStatus() 更新 status 子资源； 根据 sts 的 spec.revisionHistoryLimit 字段清理过期的 controllerrevision；  2.4 ssc.updateStatefulSet() #  sts 通过 controllerrevision 保存历史版本，类似于 deployment 的 replicaset，与 replicaset 不同的是 controllerrevision 仅用于回滚阶段，在 sts 的滚动升级过程中是通过 currentRevision 和 updateRevision 进行控制并不会用到 controllerrevision。\nfunc (ssc *defaultStatefulSetControl) updateStatefulSet(...) (*apps.StatefulSetStatus, error) { // 1. 分别获取 currentRevision 和 updateRevision 对应的的 statefulset object \tcurrentSet, err := ApplyRevision(set, currentRevision) ... updateSet, err := ApplyRevision(set, updateRevision) ... // 设置 status 的 generation 和 revisions \tstatus := apps.StatefulSetStatus{} status.ObservedGeneration = set.Generation status.CurrentRevision = currentRevision.Name status.UpdateRevision = updateRevision.Name status.CollisionCount = new(int32) *status.CollisionCount = collisionCount // 3. 将 statefulset 的 pods 按序分到 replicas 和 condemned 两个切片 \treplicaCount := int(*set.Spec.Replicas) // 用于保存序号在 [0,replicas) 范围内的 pod \treplicas := make([]*v1.Pod, replicaCount) // 用于序号大于 replicas 的 Pod \tcondemned := make([]*v1.Pod, 0, len(pods)) unhealthy := 0 firstUnhealthyOrdinal := math.MaxInt32 var firstUnhealthyPod *v1.Pod // 4. 计算 status 字段中的值，将 pod 分配到 replicas 和 condemned两个数组中 \tfor i := range pods { status.Replicas++ // 计算 Ready 的副本数 \tif isRunningAndReady(pods[i]) { status.ReadyReplicas++ } // 计算当前的副本数和已经更新的副本数 \tif isCreated(pods[i]) \u0026amp;\u0026amp; !isTerminating(pods[i]) { if getPodRevision(pods[i]) == currentRevision.Name { status.CurrentReplicas++ } if getPodRevision(pods[i]) == updateRevision.Name { status.UpdatedReplicas++ } } if ord := getOrdinal(pods[i]); 0 \u0026lt;= ord \u0026amp;\u0026amp; ord \u0026lt; replicaCount { // 保存序号在 [0,replicas) 范围内的 Pod \treplicas[ord] = pods[i] } else if ord \u0026gt;= replicaCount { // 序号大于 replicas 的 Pod，则保存在 condemned \tcondemned = append(condemned, pods[i]) } // 其余的 Pod 忽略 \t} // 5. 检查 replicas数组中 [0,set.Spec.Replicas) 下标是否有缺失的 pod，若有缺失的则创建对应的 Pod \t// 在 newVersionedStatefulSetPod 中会判断是使用 currentSet 还是 updateSet 来创建 \tfor ord := 0; ord \u0026lt; replicaCount; ord++ { if replicas[ord] == nil { replicas[ord] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, ord) } } // 6. 对 condemned 数组进行排序 \tsort.Sort(ascendingOrdinal(condemned)) // 7、根据 ordinal 在 replicas 和 condemned 数组中找出第一个处于 NotReady 或 Terminating 的 Pod \tfor i := range replicas { if !isHealthy(replicas[i]) { unhealthy++ if ord := getOrdinal(replicas[i]); ord \u0026lt; firstUnhealthyOrdinal { firstUnhealthyOrdinal = ord firstUnhealthyPod = replicas[i] } } } for i := range condemned { if !isHealthy(condemned[i]) { unhealthy++ if ord := getOrdinal(condemned[i]); ord \u0026lt; firstUnhealthyOrdinal { firstUnhealthyOrdinal = ord firstUnhealthyPod = condemned[i] } } } if unhealthy \u0026gt; 0 { klog.V(4).Infof(\u0026#34;StatefulSet %s/%s has %d unhealthy Pods starting with %s\u0026#34;, set.Namespace, set.Name, unhealthy, firstUnhealthyPod.Name) } // 8. 如果 sts 正在删除中，只要更新 status 即可 \tif set.DeletionTimestamp != nil { return \u0026amp;status, nil } // 9. 默认设置为非 Parallel \tmonotonic := !allowsBurst(set) // 10. 确保 replicas 数组中所有的 pod 是 running 的 \tfor i := range replicas { // 11. 对于 failed 的 pod 删除并重建 \tif isFailed(replicas[i]) { ssc.recorder.Eventf(set, v1.EventTypeWarning, \u0026#34;RecreatingFailedPod\u0026#34;, \u0026#34;StatefulSet %s/%s is recreating failed Pod %s\u0026#34;, set.Namespace, set.Name, replicas[i].Name) // 删除 \tif err := ssc.podControl.DeleteStatefulPod(set, replicas[i]); err != nil { return \u0026amp;status, err } if getPodRevision(replicas[i]) == currentRevision.Name { status.CurrentReplicas-- } if getPodRevision(replicas[i]) == updateRevision.Name { status.UpdatedReplicas-- } status.Replicas-- // 新建 \treplicas[i] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, i) } // 12、如果 pod.Status.Phase 不为空，说明该 pod 未创建，则直接重新创建该 pod \tif !isCreated(replicas[i]) { if err := ssc.podControl.CreateStatefulPod(set, replicas[i]); err != nil { return \u0026amp;status, err } status.Replicas++ if getPodRevision(replicas[i]) == currentRevision.Name { status.CurrentReplicas++ } if getPodRevision(replicas[i]) == updateRevision.Name { status.UpdatedReplicas++ } // 13. 如果为 Parallel，直接 return status 结束；如果为 OrderedReady，循环处理下一个pod。 \tif monotonic { return \u0026amp;status, nil } // pod 创建完成，本轮循环结束 \tcontinue } // 14、如果 pod 正在删除中，且 Spec.PodManagementPolicy 不为 Parallel，  // 直接return status结束，结束后会在下一个 syncLoop 继续进行处理，pod 状态的改变会触发下一次 syncLoop \tif isTerminating(replicas[i]) \u0026amp;\u0026amp; monotonic { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to Terminate\u0026#34;, set.Namespace, set.Name, replicas[i].Name) return \u0026amp;status, nil } // 15. 如果 pod 状态不是 Running \u0026amp; Ready，且 Spec.PodManagementPolicy 不为 Parallel  // 则直接return status结束 \tif !isRunningAndReady(replicas[i]) \u0026amp;\u0026amp; monotonic { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to be Running and Ready\u0026#34;, set.Namespace, set.Name, replicas[i].Name) return \u0026amp;status, nil } // 16. 检查 pod 的信息是否与 statefulset 的匹配，若不匹配则更新 pod 的状态 \tif identityMatches(set, replicas[i]) \u0026amp;\u0026amp; storageMatches(set, replicas[i]) { continue } // 深拷贝对象，避免修改缓存 \treplica := replicas[i].DeepCopy() if err := ssc.podControl.UpdateStatefulPod(updateSet, replica); err != nil { return \u0026amp;status, err } } // 17. 逆序处理 condemned 中的 pod \tfor target := len(condemned) - 1; target \u0026gt;= 0; target-- { // 18. 如果 pod 正在删除，检查 Spec.PodManagementPolicy 的值  // 如果为 Parallel，循环处理下一个pod 否则直接退出 \tif isTerminating(condemned[target]) { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to Terminate prior to scale down\u0026#34;, set.Namespace, set.Name, condemned[target].Name) // 如果不为 Parallel，直接 return \tif monotonic { return \u0026amp;status, nil } continue } // 19. 不满足以下条件说明该 pod 是更新前创建的，正处于创建中 \tif !isRunningAndReady(condemned[target]) \u0026amp;\u0026amp; monotonic \u0026amp;\u0026amp; condemned[target] != firstUnhealthyPod { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to be Running and Ready prior to scale down\u0026#34;, set.Namespace, set.Name, firstUnhealthyPod.Name) return \u0026amp;status, nil } klog.V(2).Infof(\u0026#34;StatefulSet %s/%s terminating Pod %s for scale down\u0026#34;, set.Namespace, set.Name, condemned[target].Name) // 20、否则直接删除该 pod \tif err := ssc.podControl.DeleteStatefulPod(set, condemned[target]); err != nil { return \u0026amp;status, err } if getPodRevision(condemned[target]) == currentRevision.Name { status.CurrentReplicas-- } if getPodRevision(condemned[target]) == updateRevision.Name { status.UpdatedReplicas-- } // 21. 如果为 OrderedReady 方式则返回否则继续处理下一个 pod \tif monotonic { return \u0026amp;status, nil } } // 22. 对于 OnDelete 策略直接返回 \tif set.Spec.UpdateStrategy.Type == apps.OnDeleteStatefulSetStrategyType { return \u0026amp;status, nil } // 23. 若为 RollingUpdate 策略，则倒序处理 replicas 数组中下标大于等于  // Spec.UpdateStrategy.RollingUpdate.Partition 的 pod \tupdateMin := 0 if set.Spec.UpdateStrategy.RollingUpdate != nil { updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition) } // 用与更新版本不匹配的最大序号终止 Pod \tfor target := len(replicas) - 1; target \u0026gt;= updateMin; target-- { // 24. 如果 Pod 的 Revision 不等于 updateRevision，且 pod 没有处于删除状态，则直接删除 pod \tif getPodRevision(replicas[target]) != updateRevision.Name \u0026amp;\u0026amp; !isTerminating(replicas[target]) { klog.V(2).Infof(\u0026#34;StatefulSet %s/%s terminating Pod %s for update\u0026#34;, set.Namespace, set.Name, replicas[target].Name) err := ssc.podControl.DeleteStatefulPod(set, replicas[target]) status.CurrentReplicas-- return \u0026amp;status, err } // 25. 如果 pod 非 healthy 状态直接返回 \tif !isHealthy(replicas[target]) { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to update\u0026#34;, set.Namespace, set.Name, replicas[target].Name) return \u0026amp;status, nil } } return \u0026amp;status, nil } 综上，updateStatefulSet() 把 statefulset 的创建、删除、更新、扩缩容的操作都包含在内。主要逻辑为：\n 分别获取 currentRevision 和 updateRevision 所对应的 sts 对象 取出 sts.status 并设置相关新值，用于更新 将 sts 关联的 Pod，按照序号分到 replicas 和 condemned 两个切片中，replicas 保存的是序号在 [0, spec.replicas) 之间的 Pod，表示可用，condemned 保存序号大于 spec.replicas 的 Pod，表示待删除； 找出 replicas 和 condemned 组中的 unhealthy pod，healthy pod 指 running \u0026amp; ready 并且不处于删除状态； 判断 sts 是否处于删除状态； 遍历 replicas，确保其中的 Pod 处于 running \u0026amp; ready 状态，其中处于 Failed 状态的 Pod 删除重建；未创建的容器则直接创建；处于删除中的，等待优雅删除结束，即下一轮循环再处理；最后检查 pod 的信息是否与 statefulset 的匹配，若不匹配则更新 pod。在此过程中每一步操作都会检查 .Spec.podManagementPolicy 是否为 Parallel，若设置了则循环处理 replicas 中的所有 pod，否则每次处理一个 pod，剩余 pod 则在下一个 syncLoop 继续进行处理； 按 pod 名称逆序删除 condemned 数组中的 pod，删除前也要确保 pod 处于 running \u0026amp; ready 状态，在此过程中也会检查 .Spec.podManagementPolicy 是否为 Parallel，以此来判断是顺序删除还是在下一个 syncLoop 中继续进行处理； 判断 sts 的更新策略 .Spec.UpdateStrategy.Type，若为 OnDelete 则直接返回； 此时更新策略为 RollingUpdate，更新序号大于等于 .Spec.UpdateStrategy.RollingUpdate.Partition 的 pod；更新策略为 RollingUpdate，并不会关注 .Spec.podManagementPolicy，都是顺序进行处理，且等待当前 pod 删除成功后才继续逆序删除一下 pod，所以 Parallel 的策略在滚动更新时无法使用。  updateStatefulSet() 这个方法中包含了 statefulset 的创建、删除、扩缩容、更新等操作，在源码层面对于各个功能无法看出明显的界定，没有 deployment sync 方法中写的那么清晰，下面按 statefulset 的功能再分析一下具体的操作：\n 创建：在创建 sts 后，sts 对象已被保存至 etcd 中，此时 sync 操作仅仅是创建出需要的 pod，即执行到第 6 步就会结束； 扩缩容：对于扩若容操作仅仅是创建或者删除对应的 pod，在操作前也会判断所有 pod 是否处于 running \u0026amp; ready 状态，然后进行对应的创建/删除操作，在上面的步骤中也会执行到第 6 步就结束； 更新：可以看出在第 6 步之后的所有操作就是与更新相关，所以更新操作会执行完整个方法，在更新过程中通过 pod 的 currentRevision 和 updateRevision 来计算 currentReplicas、updatedReplicas 的值，最终完成所有 pod 的更新； 删除：删除操作就比较明显，会止于第 5 步，但是在此之前检查 pod 状态以及分组的操作确实是多余的；  3. 参考链接 #    StatefulSet 概念  StatefulSet 基础  StatefulSet 源码解析  "});index.add({'id':9,'href':'/docs/kubernetes/kube-controller-manager/k8s-apps-rolling-update/','title':"无状态应用滚动更新",'section':"kube-controller-manager",'content':"1. 概念 #  滚动更新，通常出现在软件或者是系统中。滚动更新与传统更新的不同之处在于： 滚动更新不但提供了更新服务，而且通常还提供了滚动进度查询，滚动历史记录， 以及最重要的回滚等能力。通俗地说，就是具有系统或是软件的主动降级的能力。\n2. Deployment 滚动更新 #  Deployment 更新方式有 2 种：\n RollingUpdate Recreate  其中，滚动更新是最常见的，阅读代码 pkg/controller/deployment/deployment_controller.go:648， 可以看到 2 种方式分别对应的业务逻辑：\nfunc (dc *DeploymentController) syncDeployment(key string) error { ... switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) } ... } 根据 d.Spec.Strategy.Type，若更新策略为 RollingUpdate， 则执行 dc.rolloutRecreate() 方法，具体逻辑如下：\nfunc (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error { // 1、获取所有的 rs，若没有 newRS 则创建 \tnewRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs := append(oldRSs, newRS) // 2、newRS 执行 scale up 操作 \tscaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d) if err != nil { return err } if scaledUp { // Update DeploymentStatus \treturn dc.syncRolloutStatus(allRSs, newRS, d) } // 3、oldRS 执行 scale down 操作 \tscaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus \treturn dc.syncRolloutStatus(allRSs, newRS, d) } // 4、清理过期的 rs \tif deploymentutil.DeploymentComplete(d, \u0026amp;d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // 5、同步 deployment 状态 \treturn dc.syncRolloutStatus(allRSs, newRS, d) } 2.1 滚动更新概述 #  上面代码中 5 个重要的步骤总结如下：\n 调用 getAllReplicaSetsAndSyncRevision() 获取所有的 rs，若没有 newRS 则创建； 调用 reconcileNewReplicaSet() 判断是否需要对 newRS 进行 scaleUp 操作；如果需要 scaleUp，更新 Deployment 的 status， 添加相关的 condition，该 condition 的 type 是 Progressing，表明该 deployment 正在更新中，然后直接返回； 调用 reconcileOldReplicaSets() 判断是否需要为 oldRS 进行 scaleDown 操作；如果需要 scaleDown， 把 oldRS 关联的 pod 删掉 maxScaledDown 个，然后更新 Deployment 的 status，添加相关的 condition，直接返回。 这样一来就保证了在滚动更新过程中，新老版本的 Pod 都存在； 如果两者都不是则滚动升级很可能已经完成，此时需要检查 deployment.Status 是否已经达到期望状态， 并且根据 deployment.Spec.RevisionHistoryLimit 的值清理 oldRSs； 最后，同步 deployment 的状态，使其与期望一致；  从上面的步骤可以看出，滚动更新的过程主要分成一下三个阶段：\n mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph LR\rstart(（开始）) -- condtion1{newRS need scale up ?}\rcondtion1 -- No -- condtion2{oldRS need scale down ?}\rcondtion2 -- NO -- x3(3. sync deploment status)\rcondtion1 -- YES -- x1(1. newRS scale up)\rx1 -- stop(（结束）)\rcondtion2 -- YES -- x2(2. oldRS scale down)\rx2 -- stop\rx3 -- stop\r2.1.1 newRS scale up #  阅读代码 pkg/controller/deployment/rolling.go:68，详细如下：\nfunc (dc *DeploymentController) reconcileNewReplicaSet(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { // 1、判断副本数是否已达到了期望值 \tif *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) { // Scaling not required. \treturn false, nil } // 2、判断是否需要 scale down 操作 \tif *(newRS.Spec.Replicas) \u0026gt; *(deployment.Spec.Replicas) { // Scale down. \tscaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment) return scaled, err } // 3、计算 newRS 所需要的副本数 \tnewReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS) if err != nil { return false, err } // 4、如果需要 scale ，则更新 rs 的 annotation 以及 rs.Spec.Replicas \tscaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment) return scaled, err } 从上面的源码可以得出，reconcileNewReplicaSet() 的主要逻辑如下：\n 判断 newRS.Spec.Replicas 和 deployment.Spec.Replicas 是否相等， 如果相等则直接返回，说明已经达到期望状态； 若 newRS.Spec.Replicas \u0026gt; deployment.Spec.Replicas， 则说明 newRS 副本数已经超过期望值，调用 dc.scaleReplicaSetAndRecordEvent() 进行 scale down； 此时 newRS.Spec.Replicas \u0026lt; deployment.Spec.Replicas， 调用 deploymentutil.NewRSNewReplicas() 为 newRS 计算所需要的副本数， 计算原则遵守 maxSurge 和 maxUnavailable 的约束； 调用 dc.scaleReplicaSetAndRecordEvent() 更新 newRS 对象，设置 rs.Spec.Replicas、rs.Annotations[DesiredReplicasAnnotation] 以及 rs.Annotations[MaxReplicasAnnotation]；  其中，计算 newRS 的副本数，是滚动更新核心过程的第一步， 阅读源码 pkg/controller/deployment/util/deployment_util.go:816：\nfunc NewRSNewReplicas(deployment *apps.Deployment, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet) (int32, error) { switch deployment.Spec.Strategy.Type { case apps.RollingUpdateDeploymentStrategyType: // 1、计算 maxSurge 值，向上取整 \tmaxSurge, err := intstrutil.GetValueFromIntOrPercent(deployment.Spec.Strategy.RollingUpdate.MaxSurge, int(*(deployment.Spec.Replicas)), true) if err != nil { return 0, err } // 2、累加 rs.Spec.Replicas 获取 currentPodCount \tcurrentPodCount := GetReplicaCountForReplicaSets(allRSs) maxTotalPods := *(deployment.Spec.Replicas) + int32(maxSurge) if currentPodCount \u0026gt;= maxTotalPods { // Cannot scale up. \treturn *(newRS.Spec.Replicas), nil } // 3、计算 scaleUpCount，结果不超过期望值 \tscaleUpCount := maxTotalPods - currentPodCount scaleUpCount = int32(integer.IntMin(int(scaleUpCount), int(*(deployment.Spec.Replicas)-*(newRS.Spec.Replicas)))) return *(newRS.Spec.Replicas) + scaleUpCount, nil case apps.RecreateDeploymentStrategyType: return *(deployment.Spec.Replicas), nil default: return 0, fmt.Errorf(\u0026#34;deployment type %v isn\u0026#39;t supported\u0026#34;, deployment.Spec.Strategy.Type) } } 可知 NewRSNewReplicas() 的主要逻辑如下：\n 判断更新策略； 计算 maxSurge 值； 通过 allRSs 计算 currentPodCount 的值； 最后计算 scaleUpCount 值；  2.1.2 oldRS scale down #  同理，oldRS 规模缩小，阅读源码 pkg/controller/deployment/rolling.go:68:\nfunc (dc *DeploymentController) reconcileOldReplicaSets(allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { // 1、计算 oldPodsCount \toldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs) if oldPodsCount == 0 { // Can\u0026#39;t scale down further \treturn false, nil } // 2、计算 maxUnavailable \tallPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) klog.V(4).Infof(\u0026#34;New replica set %s/%s has %d available pods.\u0026#34;, newRS.Namespace, newRS.Name, newRS.Status.AvailableReplicas) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) // 3、计算 maxScaledDown \tminAvailable := *(deployment.Spec.Replicas) - maxUnavailable newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount if maxScaledDown \u0026lt;= 0 { return false, nil } // 4、清理异常的 rs \toldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(oldRSs, deployment, maxScaledDown) if err != nil { return false, nil } klog.V(4).Infof(\u0026#34;Cleaned up unhealthy replicas from old RSes by %d\u0026#34;, cleanupCount) // 5、缩容 old rs \tallRSs = append(oldRSs, newRS) scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(allRSs, oldRSs, deployment) if err != nil { return false, nil } klog.V(4).Infof(\u0026#34;Scaled down old RSes of deployment %s by %d\u0026#34;, deployment.Name, scaledDownCount) totalScaledDown := cleanupCount + scaledDownCount return totalScaledDown \u0026gt; 0, nil } 通过上面的代码可知，reconcileOldReplicaSets() 的主要逻辑如下：\n 通过 oldRSs 和 allRSs 获取 oldPodsCount 和 allPodsCount； 计算 deployment 的 maxUnavailable、minAvailable、newRSUnavailablePodCount、maxScaledDown 值， 当 deployment 的 maxSurge 和 maxUnavailable 值为百分数时， 计算 maxSurge 向上取整而 maxUnavailable 则向下取整； 清理异常的 rs； 计算 oldRS 的 scaleDownCount； 最后 oldRS 缩容；  2.2 滚动更新总结 #  通过上面的代码可以看出，滚动更新过程中主要是通过调用 reconcileNewReplicaSet() 对 newRS 不断扩容， 调用 reconcileOldReplicaSets() 对 oldRS 不断缩容，最终达到期望状态，并且在整个升级过程中， 都严格遵守 maxSurge 和 maxUnavailable 的约束。\n不论是在 scale up 或者 scale down 中都是调用 scaleReplicaSetAndRecordEvent() 执行， 而 scaleReplicaSetAndRecordEvent() 又会调用 scaleReplicaSet()， 扩缩容都是更新 rs.Annotations 以及 rs.Spec.Replicas。\n整体流程如下图所示：\ngraph LR\rop1(newRS scale up) -- op3[dc.scaleReplicaSetAndRecordEvent]\rop2(oldRS scale down) -- op3(dc.scaleReplicaSetAndRecordEvent)\rop3(dc.scaleReplicaSetAndRecordEvent) -- op4(dc.scaleReplicaSet)\r2.3 滚动更新示例 #   创建一个 deployment，replica = 10  apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 10 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.18.0 ports: - containerPort: 80 10 个 Pod 创建成功后如下所示：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-67dfd6c8f9 10 10 10 70s 更新 nginx-deployment 的镜像，默认使用滚动更新的方式  $ kubectl set image deploy/nginx-deployment nginx-deployment=nginx:1.19.1 此时通过源码可知会计算该 deployment 的 maxSurge=3，maxUnavailable=2，maxAvailable=13，计算方法如下所示：\n// 向上取整 maxSurge = 10 * 0.25 = 3 maxSurge = replicas * deployment.spec.strategy.rollingUpdate.maxSurge // 向下取整 maxUnavailable = 10 * 0.25 = 2 maxUnavailable = replicas * deployment.spec.strategy.rollingUpdate.maxUnavailable // maxAvailable = 10 + 3 = 13 maxAvailable = replicas + MaxSurge 如上面代码所说，更新时首先创建 newRS，然后为其设定 replicas，计算 newRS 的 replicas 值的方法在 NewRSNewReplicas() 中， 此时计算出 replicas 结果为 3，然后更新 deployment 的 annotation，创建 events，本次 syncLoop 完成。 等到下一个 syncLoop 时，所有 rs 的 replicas 已经达到最大值 10 + 3 = 13，此时需要 oldRS 缩容。 scale down 的数量是通过以下公式得到的：\n// 13 = 10 + 3 allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) // 8 = 10 - 2 minAvailable := *(deployment.Spec.Replicas) - maxUnavailable // ??? newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas // 13 - 8 - ??? maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount allPodsCount = 13，minAvailable = 8 ，newRSUnavailablePodCount 此时不确定，但是值在 [0,3] 范围内。 此时假设 newRS 的 3 个 pod 还处于 containerCreating 状态，则 newRSUnavailablePodCount = 3， 根据以上公式计算所知 maxScaledDown = 2，则 oldRS 需要缩容 2 个 pod，其 replicas 需要改为 8，此时该 syncLoop 完成。 下一个 syncLoop 时在 scaleUp 处计算得知 scaleUpCount = 13 - 8 - 3 = 2， 此时 newRS 需要更新 replicase 增加 2。以此轮询直到 newRS 扩容到 10，oldRS 缩容至 0。\n对于上面的示例，可以使用 kubectl get rs -w 进行观察，以下为输出：\n$ kubectl get rs -w NAME DESIRED CURRENT READY AGE nginx-deployment-5bbdfb5879 10 10 5 3s nginx-deployment-67dfd6c8f9 3 3 3 4m47s nginx-deployment-5bbdfb5879 10 10 6 3s nginx-deployment-67dfd6c8f9 2 3 3 4m47s nginx-deployment-67dfd6c8f9 2 2 2 4m47s nginx-deployment-5bbdfb5879 10 10 7 4s nginx-deployment-67dfd6c8f9 1 2 2 4m48s nginx-deployment-67dfd6c8f9 1 1 1 4m48s nginx-deployment-5bbdfb5879 10 10 8 4s nginx-deployment-67dfd6c8f9 0 1 1 4m48s nginx-deployment-67dfd6c8f9 0 0 0 4m48s nginx-deployment-5bbdfb5879 10 10 9 5s nginx-deployment-5bbdfb5879 10 10 10 6s "});index.add({'id':10,'href':'/docs/kubernetes/kube-apiserver/key-design-of-etcd-watch/','title':"watch 关键设计",'section':"kube-apisever",'content':" 注：本文转自 图解 kubernetes 中基于 etcd 的 watch 关键设计\n 本文介绍了 kubernetes 针对 etcd 的 watch 场景，k8s 在性能优化上面的一些设计， 逐个介绍缓存、定时器、序列化缓存、bookmark 机制、forget 机制、 针对数据的索引与 ringbuffer 等组件的场景以及解决的问题， 希望能帮助到那些对 apiserver 中的 watch 机制实现感兴趣的朋友。\n1. 事件驱动与控制器 #   k8s 中并没有将业务的具体处理逻辑耦合在 rest 接口中，rest 接口只负责数据的存储， 通过控制器模式，分离数据存储与业务逻辑的耦合，保证 apiserver 业务逻辑的简洁。\n 控制器通过 watch 接口来感知对应的资源的数据变更，从而根据资源对象中的期望状态与当前状态之间的差异， 来决策业务逻辑的控制，watch 本质上做的事情其实就是将感知到的事件发生给关注该事件的控制器。\n2. Watch 的核心机制 #  这里我们先介绍基于 etcd 实现的基础的 watch 模块。\n2.1 事件类型与 etcd #   一个数据变更本质上无非就是三种类型：新增、更新和删除， 其中新增和删除都比较容易因为都可以通过当前数据获取，而更新则可能需要获取之前的数据， 这里其实就是借助了 etcd 中 revision 和 mvcc 机制来实现，这样就可以获取到之前的状态和更新后的状态， 并且获取后续的通知。\n2.2 事件管道 #   事件管道则是负责事件的传递，在 watch 的实现中通过两级管道来实现消息的分发， 首先通过 watch etcd 中的 key 获取感兴趣的事件，并进行数据的解析， 完成从 bytes 到内部事件的转换并且发送到输入管道 (incomingEventChan) 中， 然后后台会有线程负责输入管道中获取数据，并进行解析发送到输出管道 (resultChan) 中， 后续会从该管道来进行事件的读取发送给对应的客户端。\n2.3 事件缓冲区 #  事件缓冲区是指的如果对应的事件处理程序与当前事件发生的速率不匹配的时候， 则需要一定的 buffer 来暂存因为速率不匹配的事件， 在 go 里面大家通常使用一个有缓冲的 channel 构建。\n 到这里基本上就实现了一个基本可用的 watch 服务，通过 etcd 的 watch 接口监听数据， 然后启动独立 goroutine 来进行事件的消费，并且发送到事件管道供其他接口调用。\n3. Cacher #  kubernetes 中所有的数据和系统都基于 etcd 来实现，如何减轻访问压力呢， 答案就是缓存，watch 也是这样，本节我们来看看如何实现 watch 缓存机制的实现， 这里的 cacher 是针对 watch 的。\n3.1 Reflector #   Reflector 是 client-go 中的一个组件，其通过 listwatch 接口获取数据存储在自己内部的 store 中， cacher 中通过该组件对 etcd 进行 watch 操作，避免为每个组件都创建一个 etcd 的 watcher。\n3.2 watchCache #   wacthCache 负责存储 watch 到的事件，并且将 watch 的事件建立对应的本地索引缓存， 同时在构建 watchCache 还负责将事件的传递， 其将 watch 到的事件通过 eventHandler 来传递给上层的 Cacher 组件。\n3.3 cacheWatcher #   cacheWatcher 顾名思义其是就是针对 cache 的一个 watcher(watch.Interface) 实现， 前端的 watchServer 负责从 ResultChan 里面获取事件进行转发。\n3.4 Cacher #   Cacher 基于 etcd 的 store 结合上面的 watchCache 和 Reflector 共同构建带缓存的 REST store， 针对普通的增删改功能其直接转发给 etcd 的 store 来进行底层的操作，而对于 watch 操作则进行拦截， 构建并返回 cacheWatcher 组件。\n4. Cacher 的优化 #  看完基础组件的实现，接着我们看下针对 watch 这个场景 k8s 中还做了那些优化，学习针对类似场景的优化方案。\n4.1 序列化缓存 #   如果我们有多个 watcher 都 wacth 同一个事件，在最终的时候我们都需要进行序列化， cacher 中在分发的时候，如果发现超过指定数量的 watcher， 则会在进行 dispatch 的时候， 为其构建构建一个缓存函数，针对多个 watcher 只会进行一次的序列化。\n4.2 nonblocking #   在上面我们提到过事件缓冲区，但是如果某个 watcher 消费过慢依然会影响事件的分发， 为此 cacher 中通过是否阻塞（是否可以直接将数据写入到管道中）来将 watcher 分为两类， 针对不能立即投递事件的 watcher， 则会在后续进行重试。\n4.3 TimeBudget #  针对阻塞的 watcher 在进行重试的时候，会通过 dispatchTimeoutBudget 构建一个定时器来进行超时控制， 那什么叫 Budget 呢，其实如果在这段时间内，如果重试立马就成功，则本次剩余的时间， 在下一次进行定时的时候，则可以使用之前剩余的余额，但是后台也还有个线程，用于周期性重置。\n4.4 forget 机制 #   针对上面的 TimeBudget 如果在给定的时间内依旧无法进行重试成功， 则就会通过 forget 来删除对应的 watcher， 由此针对消费特别缓慢的 watcher 则可以通过后续的重试来重新建立 watch， 从而减小对 a piserver 的 watch 压力。\n4.5 bookmark 机制 #   bookmark 机制是大阿里提供的一种优化方案，其核心是为了避免单个某个资源一直没有对应的事件， 此时对应的 informer 的 revision 会落后集群很大， bookmark 通过构建一种 BookMark 类型的事件来进行 revision 的传递， 从而让 informer 在重启后不至于落后特别多。\n4.6 watchCache 中的 ringbuffer #   watchCache 中通过 store 来构建了对应的索引缓存，但是在 listwatch 操作的时候， 则通常需要获取某个 revision 后的所有数据， 针对这类数据 watchCache 中则构建了一个 ringbuffer 来进行历史数据的缓存。\n5. 设计总结 #   本文介绍了 kubernetes 针对 etcd 的 watch 场景，k8s 在性能优化上面的一些设计， 逐个介绍缓存、定时器、序列化缓存、bookmark 机制、forget 机制、 针对数据的索引与 ringbuffer 等组件的场景以及解决的问题， 希望能帮助到那些对 apiserver 中的 watch 机制实现感兴趣的朋友。\n"});index.add({'id':11,'href':'/docs/kubernetes/kubelet/kubelet-topology-manager/','title':"Topology Manager 设计方案",'section':"kubelet",'content':" 注：本文翻译自 Node Topology Manager\n 1. 概要 #  越来越多的系统将 CPU 和硬件加速器组合使用，以支撑高延迟和搞吞吐量的并行计算。 包括电信、科学计算、机器学习、金融服务和数据分析等领域的工作。这种的混血儿构成了一个高性能环境。\n为了达到最优性能，需要对 CPU 隔离、内存和设备的物理位置进行优化。 然而，在 kubernetes 中，这些优化没有一个统一的组件管理。\n本次建议提供一个新机制，可以协同 kubernetes 各个组件，对硬件资源的分配可以有不同的细粒度。\n2. 启发 #  当前，kubelet 中多个组件决定系统拓扑相关的分配：\n CPU 管理器  CPU 管理器限制容器可以使用的 CPU。该能力，在 1.8 只实现了一种策略——静态分配。 该策略不支持在容器的生命周期内，动态上下线 CPU。   设备管理器  设备管理器将某个具体的设备分配给有该设备需求的容器。设备通常是在外围互连线上。 如果设备管理器和 CPU 管理器策略不一致，那么 CPU 和设备之间的所有通信都可能导致处理器互连结构上的额外跳转。   容器运行时（CNI）  网络接口控制器，包括 SR-IOV 虚拟功能（VF）和套接字有亲和关系，socket 不同，性能不同。    相关问题：\n  节点层级的硬件拓扑感知（包括 NUMA）  发现节点的 NUMA 架构  绑定特定 CPU 支持虚拟函数  提议：CPU 亲和与 NUMA 拓扑感知  注意，以上所有的关注点都只适用于多套接字系统。 内核能从底层硬件接收精确的拓扑信息（通常是通过 SLIT 表），是正确操作的前提。 更多信息请参考 ACPI 规范的 5.2.16 和 5.2.17 节。\n2.1 目标 #   根据 CPU 管理器和设备管理器的输入，给容器选择最优的 NUMA 亲和节点。 集成 kubelet 中其他支持拓扑感知的组件，提供一个统一的内部接口。  2.2 非目标 #   设备间连接：根据直接设备互连来决定设备分配。此问题不同于套接字局部性。设备间的拓扑关系， 可以都在设备管理器中考虑，可以做到套接字的亲和性。实现这一策略，可以从逐渐支持任何设备之间的拓扑关系。 大页：本次提议有 2 个前提，一是集群中的节点已经预分配了大页； 二是操作系统能给容器做好本地页分配（只需要本地内存节点上有空闲的大页即可）。 容器网络接口：本次提议不包含修改 CNI。但是，如果 CNI 后续支持拓扑管理， 此次提出的方案应该具有良好的扩展性，以适配网络接口的局部性。对于特殊的网络需求， 可以使用设备插件 API 作为临时方案，以减少网络接口的局限性。  2.3 用户故事 #  故事 1: 快速虚拟化的网络功能\n要求在一个首选的 NUMA 节点上，既要“网络快”，又能自动完成各个组件（大页，cpu 集，网络设备）的协同。 在大多数场景下，只有极少数的 NUMA 节点才能满足。\n故事 2: 加速神经网络训练\nNUMA 节点中的已分配的 CPU 和设备，可满足神经网络训练的加速器和独占的 CPU 的需求，以达到性能最优。\n3. 提议 #  主要思想：两相拓扑一致性协议 拓扑亲和性在容器级别，与设备和 CPU 的亲和类似。在 Pod 准入期间， 一个新组件可以从设备管理器和 CPU 管理器收集 Pod 中每个容器的配置，这个组件名为拓扑管理器。 当这些组件进行资源分配时，拓扑管理器扮演本地对齐的预分配角色。 我们希望各个组件能利用 Pod 中隐含的 QoS 类型进行优先级排序，以满足局部重要性最优。\n3.1 提议修改点 #  3.1.1 新概念：拓扑管理器 #  这个提议主要关注 kubelet 中一个新组件，叫做拓扑管理器。拓扑管理器实现了 Pod 的 Admit() 接口， 并参与 kubelet 的对 Pod 的准入。当 Admit() 方法被调用，拓扑管理器根据 kubelet 标志， 逐个 Pod 或逐个容器收集 kubelet 其他组件的的拓扑信息。\n如果提示不兼容，拓扑管理器可以选择拒绝 Pod，这是由 kubelet 配置的拓扑策略所决定的。 拓扑管理器支持 4 中策略：none（默认）、best-erffort、restricted 和 single-numa-node。\n拓扑信息中包含了对本地资源的偏好。拓扑信息当前由以下组成：\n 位掩码表——可能满足请求的 NUMA 节点 首选属性  属性定义如下：  每个拓扑信息提供者，都有一个满足请求的可能资源分配，这样就可以尽可能减少 NUMA 节点数量（节点为空那样计算） 有一种可能的分配方式，相关 NUMA 节点联合总量，不大于任何个单个资源的的请求量      Pod 的有效资源请求/限制\n所有提供拓扑信息的组件，都应该先考虑资源的请求和限制，再计算得出可靠的拓扑提示， 这个规则是由 init 容器的概念定义的。\nPod 对资源的请求和限制的有效值，由以下 2 个条件中较大的一个决定：\n 所有 init 容器中，请求或限制的最大值（max([]initcontainer.Request)，max([]initcontainer.Limit)） 所有应用的请求和限制的总和（sum([]containers.Request)，sum([]containers.Limit)）  下面这个例子简要说明它是如何工作的：\napiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: appContainer1 resources: requests: cpu: 2 memory: 1G - name: appContainer2 resources: requests: cpu: 1 memory: 1G initContainers: - name: initContainer1 resources: requests: cpu: 2 memory: 1G - name: initContainer2 resources: requests: cpu: 2 memory: 3G #资源请求的有效值：CPU: 3, Memory: 3G debug/ephemeral 容器不能指定资源请求/限制，因为不会影响拓扑提示的结果。\n范围\n拓扑管理器将根据新 kubelet 标志 --topology-manager-scope 的值， 尝试逐个 Pod 或逐个容器地对资源进行对齐。该标志可以显示的值详细如下：\n  Container（默认）：逐个容器地收集拓扑信息。 然后，拓扑策略将为每个容器单独调整资源，只有调整成功，Pod 准入成功。\n  Pod：逐个 Pod 地收集拓扑信息。 然后，拓扑策略将为所有容器集体调整资源，只有调整成功，Pod 准入成功。\n  策略\n none（默认）：kubelet 不会参考拓扑管理器的决定 best-effort：拓扑管理器会基于拓扑信息，给出首选分配。 在此策略下，即使分配结果不合理，Pod 也成功准入。 restricted：与 best-effort 不同，在此策略下，如果分配结果不合理，Pod 会被拒绝。 同时，因准入失败，进入 Terminated 状态。 single-numa-node：拓扑管理器会在 NUMA 节点上强制执行资源分配，如果分配失败，Pod 会被拒绝。 同时，因准入失败，进入 Terminated 状态。  拓扑管理器组件默认被禁用，直到从 alpha 到 beta 级别解禁。\n亲和计算\n拓扑管理策略基于收集的所有拓扑信息，执行亲和计算，然后决定接受或拒绝 Pod。\n亲和算法\n best-effort/restricted （亲和算法相同）   循环遍历所有拓扑信息提供者，并以列表保存每个信息源的返回。 遍历步骤 1 中的列表，执行按位与运算，合并为单个亲和信息。如果循环中任何字段的亲和返回了 false，则最终结果该字段也为 false。 返回的亲和信息的最小集，最小集意味着至少有一个 NUMA 节点满足资源请求。 如果没有找到任何 NUMA 节点集的提示，则返回一个默认提示，该值包含了所有 NUMA 节点，并把首选设置为 false。   single-numa-node   循环遍历所有拓扑信息提供者，并以列表保存每个信息源的返回 过滤步骤 1 中累积的列表，使其只包含具有单个 NUMA 节点和空 NUMA 节点的提示 遍历步骤 1 中的列表，执行按位与运算，合并为单个亲和信息。如果循环中任何字段的亲和返回了 false，则最终结果该字段也为 false 如果没有找到具有单 NUMA 节点集的提示，则返回一个默认提示，该提示包含所有 NUMA 节点集并且首选设置为 false。  策略决断\n best-effort：总是遵循拓扑信息提示，准入 Pod restricted：只有拓扑提示的首选字段为 true，才准入 Pod single-numa-node：既需要拓扑的首选字段为 true，又需要位掩码设置为单个 NUMA 节点，才准入 Pod  新的接口\n清单：拓扑管理器和相关接口\npackage bitmask // BitMask interface allows hint providers to create BitMasks for TopologyHints type BitMask interface { Add(sockets ...int) error Remove(sockets ...int) error And(masks ...BitMask) Or(masks ...BitMask) Clear() Fill() IsEqual(mask BitMask) bool IsEmpty() bool IsSet(socket int) bool IsNarrowerThan(mask BitMask) bool String() string Count() int GetSockets() []int } func NewBitMask(sockets ...int) (BitMask, error) { ... } package topologymanager // Manager interface provides methods for Kubelet to manage Pod topology hints type Manager interface { // Implements Pod admit handler interface  lifecycle.PodAdmitHandler // Adds a hint provider to manager to indicate the hint provider  //wants to be consoluted when making topology hints  AddHintProvider(HintProvider) // Adds Pod to Manager for tracking  AddContainer(Pod *v1.Pod, containerID string) error // Removes Pod from Manager tracking  RemoveContainer(containerID string) error // Interface for storing Pod topology hints  Store } // TopologyHint encodes locality to local resources. Each HintProvider provides // a list of these hints to the TopoologyManager for each container at Pod // admission time. type TopologyHint struct { NUMANodeAffinity bitmask.BitMask // Preferred is set to true when the BitMask encodes a preferred  // allocation for the Container. It is set to false otherwise.  Preferred bool } // HintProvider is implemented by Kubelet components that make // topology-related resource assignments. The Topology Manager consults each // hint provider at Pod admission time. type HintProvider interface { // GetTopologyHints returns a map of resource names with a list of possible  // resource allocations in terms of NUMA locality hints. Each hint  // is optionally marked \u0026#34;preferred\u0026#34; and indicates the set of NUMA nodes  // involved in the hypothetical allocation. The topology manager calls  // this function for each hint provider, and merges the hints to produce  // a consensus \u0026#34;best\u0026#34; hint. The hint providers may subsequently query the  // topology manager to influence actual resource assignment.  GetTopologyHints(Pod v1.Pod, containerName string) map[string][]TopologyHint // GetPodLevelTopologyHints returns a map of resource names with a list of  // possible resource allocations in terms of NUMA locality hints.  // The returned map contains TopologyHint of requested resource by all containers  // in a Pod spec.  GetPodLevelTopologyHints(Pod *v1.Pod) map[string][]TopologyHint // Allocate triggers resource allocation to occur on the HintProvider after  // all hints have been gathered and the aggregated Hint is available via a  // call to Store.GetAffinity().  Allocate(Pod *v1.Pod, container *v1.Container) error } // Store manages state related to the Topology Manager. type Store interface { // GetAffinity returns the preferred affinity as calculated by the  // TopologyManager across all hint providers for the supplied Pod and  // container.  GetAffinity(PodUID string, containerName string) TopologyHint } // Policy interface for Topology Manager Pod Admit Result type Policy interface { // Returns Policy Name  Name() string // Returns a merged TopologyHint based on input from hint providers  // and a Pod Admit Handler Response based on hints and policy type  Merge(providersHints []map[string][]TopologyHint) (TopologyHint, lifecycle.PodAdmitResult) } 图：拓扑管理器组件\n 图：拓扑管理器实例化并出现在 Pod 准入生命周期中\n 3.1.2 特性门禁和 kubelet 启动参数 #  将添加一个特性门禁，控制拓扑管理器特性的启动。此门禁将在 Kubelet 启用，并在 Alpha 版本中默认关闭。\n  门禁定义建议：\n--feature-gate=TopologyManager=true\n  如上所述，kubelet 还新增一个标志，用于标识拓扑管理器策略。默认策略将会是 none。\n  策略标志建议：\n--topology-manager-policy=none|best-effort|restricted|single-numa-node\n  根据选择的策略，以下标志将确定应用策略的范围（逐个 Pod 或逐个容器）。范围的默认值是 container\n  范围标志建议：\n--topology-manager-scope=container|Pod\n  3.1.3 现有组件变更 #   Kubelet 向拓扑管理器咨询 Pod 准入（上面讨论过） 添加两个拓扑管理器接口的实现和一个特性门禁  当功能门被禁用时，尽可能保证拓扑管理器功能失效。 添加一个功能性拓扑管理器，用来查询拓扑信息，以便为每个容器计算首选套接字掩码。   CPU 管理器添加 2 个方法：GetTopologyHints() 和 GetPodLevelTopologyHints()  CPU 管理器的 static 策略在决定 CPU 的亲和性是，调用拓扑管理器的 GetAffinity() 方法   设备管理器添加 2 个方法：GetTopologyHints() 和 GetPodLevelTopologyHints()  在设备插件接口的设备结构中添加 TopologyInfo。 插件在枚举受支持的设备时应该能够确定 NUMA 节点。请参阅下面的协议差异。 设备管理器决定设备分配时，调用拓扑管理器的 GetAffinity() 方法    清单：修改后的设备插件 gRPC 协议\ndiff --git a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto index efbd72c133..f86a1a5512 100644 --- a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto +++ b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto @@ -73,6 +73,10 @@ message ListAndWatchResponse {  repeated Device devices = 1; } +message TopologyInfo { + repeated NUMANode nodes = 1; +} + +message NUMANode { + int64 ID = 1; +} +  /* E.g: * struct Device { * ID: \u0026#34;GPU-fef8089b-4820-abfc-e83e-94318197576e\u0026#34;, * State: \u0026#34;Healthy\u0026#34;, + * Topology: + * Nodes: + * ID: 1 @@ -85,6 +89,8 @@ message Device {  string ID = 1; // Health of the device, can be healthy or unhealthy, see constants.go string health = 2; +\t// Topology details of the device +\tTopologyInfo topology = 3;  } 图：拓扑管理器提示提供者注册\n 图：拓扑管理器从 HintProvider 获取拓扑提示\n 此外，我们提议将设备插件接口扩展为“最后一级”过滤器，以帮助影响设备管理器做出的总体分配决策。 下面的差异显示了提议的改动：\ndiff --git a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto index 758da317fe..1e55d9c541 100644 --- a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto +++ b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto @@ -55,6 +55,11 @@ service DevicePlugin {  // returns the new list rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} + // GetPreferredAllocation returns a preferred set of devices to allocate + // from a list of available ones. The resulting preferred allocation is not + // guaranteed to be the allocation ultimately performed by the + // `devicemanager`. It is only designed to help the `devicemanager` make a + // more informed allocation decision when possible. + rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {} +  // Allocate is called during container creation so that the Device // Plugin can run device specific operations and instruct Kubelet // of the steps to make the Device available in the container @@ -99,6 +104,31 @@ message PreStartContainerRequest {  message PreStartContainerResponse { } +// PreferredAllocationRequest is passed via a call to +// `GetPreferredAllocation()` at Pod admission time. The device plugin should +// take the list of `available_deviceIDs` and calculate a preferred allocation +// of size `size` from them, making sure to include the set of devices listed +// in `must_include_deviceIDs`. +message PreferredAllocationRequest { + repeated string available_deviceIDs = 1; + repeated string must_include_deviceIDs = 2; + int32 size = 3; +} + +// PreferredAllocationResponse returns a preferred allocation, +// resulting from a PreferredAllocationRequest. +message PreferredAllocationResponse { + ContainerAllocateRequest preferred_allocation = 1; +} +  // - Allocate is expected to be called during Pod creation since allocation // failures for any container would result in Pod startup failure. // - Allocate allows kubelet to exposes additional artifacts in a Pod\u0026#39;s 使用这个新的 API 调用，设备管理器将在 Pod 准入时调用一个插件， 要求它从可用设备列表中获得一个给定大小的首选设备分配。Pod 中的每个容器都会调用一次。\n传给 GetPreferredAllocation() 方法的可用设备列表不一定与系统上可用的完整列表相匹配。 相反，在考虑所有 TopologyHint 之后，设备管理器调用 GetPreferredAllocation() 方法， 是最后一次筛选，方法执行结束必须要做出选择。因此，这个可用列表已经经过 TopologyHint 的预筛选。\n首选分配并不保证是最终由设备管理器执行的分配。它的设计只是为了帮助设备管理者在可能的情况下做出更明智的分配决策。\n在决定首选分配时，设备插件可能会考虑设备管理器不知道的内部拓扑约束。分配 NVIDIA 图形处理器对， 总是包括一个 NVLINK，就是一个很好的例子。\n在一台 8 GPU 的机器上，如果需要 2 个 GPU，NVLINK 提供的最佳连接对可能是：\n{{0,3}, {1,2}, {4,7}, {5,6}} 使用 GetPreferredAllocation () ，NVIDIA 设备插件可以将这些首选分配之一转发给设备管理器， 如果仍然有合适的设备集可用的话。如果没有这些额外的信息， 设备管理器最终会在 TopologyHint 过滤之后从可用的 gpu 列表中随机选择 gpu。 这个 API 允许它最终以最小的成本执行更好的分配。\n"});index.add({'id':12,'href':'/docs/kubernetes/kube-scheduler/advanced-scheduling/','title':"高级调度",'section':"kube-scheduler",'content':"1. 使用 taint 和 toleration 阻止节点调度到特定节点 #  1.1 taint 和 toleration #  taint，是在不修改已有 Pod 的前提下，通过在节点上添加污点信息，拒绝 Pod 的部署。 只有当一个 Pod 容忍某个节点的 taint 时，才能被调度到此节点上。\n显示节点 taint 信息\nkubectl describe node master.k8s ... Name: master.k8s Role: Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=master.k8s Annotations: node.alpha.kubernetes.io/ttl=0 Taints: node-role.kubernetes.io/master:NoSchedule ... 主节点上包含一个污点，污点包含一个 key 和 value，以及一个 effect，格式为=:。 上面的污点信息表示，key 为 node-role.kubernetes.io/master，value 是空，effect 是 NoSchedule。\n显示 Pod tolerations\nkubectl describe Pod kube-proxy-as92 -n kube-system ... Tolerations: node-role.kubernetes.io/master:=NoSchedule node.alpha.kubernetes.io/notReady=:Exists:NoExecute node.alpha.kubernetes.io/unreachable=:Exists:NoExecute ... 第一个 toleration 匹配了主节点的 taint，表示允许这个 Pod 调度到主节点上。\n了解污点效果 另外 2 个在 kube-proxy Pod 上容忍定义了当前节点状态是没有 ready 或者是 unreachable 时， 该 Pod 允许运行在该节点上多长时间。污点可以包含以下三种效果：\n NoSchedule，表示如果 pod 没有容忍此污点，将无法调度 PreferNoSchedule，是上面的宽松版本，如果没有其他节点可调度，依旧可以调度到本节点 NoExecute，上 2 个在调度期间起作用，而此设定也会影响正在运行中的 Pod。 如果节点上的 Pod 没有容忍此污点，会被驱逐。  1.2 在节点上定义污点 #  kubectl taint node node1.k8s node-type=production:NoSchedule 此命令给节点添加污点，key 为 node-type，value 为 production，effect 为 NoSchedule。 如果现在部署一个常规 Pod 多个副本，没有一个 Pod 会调度到此节点上。\n1.3 在 Pod 上添加容忍 #  apiVersion: apps/v1 kind: Deployment metadata: name: prod spec: replicas: 5 template: spec: ... tolertations: - key: node-type operator: Equals value: production effect: NoSchedule 部署此 deployment，Pod 即可调度到 node1.k8s 节点上。\n1.4 了解污点和容忍的使用场景 #  调度时使用污点和容忍\n污点可以组织新 Pod 的调度，或者定义非有限调度节点，甚至是将已有 Pod 驱逐。 例如，一个集群分成多个部分，只允许开发团队将 Pod 部署到特定节点上；或者某些特殊 Pod 依赖硬件配置。\n配置节点失效后的 pod 重新调度最长等待时间\n容忍程度也可以配置，当某个 Pod 所在节点 NotReady 或者 Unreachable 是， k8s 可以等待该 Pod 重新调度之前的最长等待时间。\n... tolerations: - effect: NoExecute key: node.alpha.kubernetes.io/notReady operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.alpha.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 ... 上面 2 个容忍表示，该 Pod 容忍所在节点处于 notReady 和 unreachable 状态维持 300s。超时后，再重新调度。\n2. 使用节点亲和将 Pod 调度到指定节点 #  污点可以拒绝调度，亲和允许调度。早期的 k8s 版本，节点亲和，就是 Pod 中的 nodeSelector 字段。 与节点选择器类似，每个节点都可以配置亲和性规则，可以是硬性标注，也可以是偏好。\n检查节点默认标签\nkubectl describe node gke-kubia-default-adj12knzf-2dascjb Name: gke-kubia-default-adj12knzf-2dascjb Role: Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux failure-domain.beta.kubernetes.io/region=europe-west1 failure-domain.beta.kubernetes.io/zone=europe-west1-d kubernetes.io/hostname=gke-kubia-default-adj12knzf-2dascjb 这是一个 gke 的节点，最后三个标签涉及到亲和性，分别表示所在地理地域，可用性区域，和主机名。\n2.1 指定强制节点亲和性规则 #  下面展示了只能被部署到含有 gpu=true 标签的节点上的 Pod：\napiVersion: v1 kind: Pod metadata: name: kubia-gpu spec: affinity: nodeAffinity: requiredDuringSchdulingIgnoredDuringExecution: lableSelectorTerms: - matchExpressions: - key: gpu operator: In values: - \u0026#34;true\u0026#34; 较长的节点亲和性属性名的含义\n requireDuringScheduling\u0026hellip; 表示该字段下定义的规则，为了让 Pod 调度到该节点上，必须满足的标签。 \u0026hellip;IgnoredDuringExecution 表明该字段下的规则，不会影响已经在节点上的 Pod。  了解节点选择器的条件\nlableSelectorTerms 和 matchExpressions 定义了节点的标签必须满足哪一种表达方式。\n2.2 调度 Pod 时优先考虑某些节点 #  节点亲和性和节点选择器相比，最大的好处就是， 当调度某一个 Pod，指定可以优先考某些节点，如果节点均无法满足，调度结果也是可以接受的。 这是由 preferdDuringSchdulingIgnoredDuringExecution 字段实现的。\n添加标签\nkubectl label node node1.k8s availability-zone=zone1 kubectl label node node1.k8s share-type=dedicated kubectl label node node2.k8s availability-zone=zone2 kubectl label node node2.k8s share-type=shared 指定优先级节点亲和性\napiVersion: apps/v1 kind: Deployment metadata: name: pref spec: template: ... spec: affinity: nodeAffinity: prefereddDuringSchdulingIgnoredDuringExecution: - weight: 80 preference: - matchExpressions: - key: availability-zone operator: In values: - zone1 - weight: 20 preference: - matchExpressions: - key: share-type operator: In values: - dedicated 上面描述一个节点亲和优先级，并不是强制要求。想要 Pod 调度到含有 availability-zone=zone1 和 share-type=dedicated 的节点上。第一个优先级相对重要，weight=80，第二个相对不重要，weight=20。\n了解优先级是如何工作的\n如果集群中包含很多节点，上面的 deployment，将会把节点分成四种。2 个标签都包含的，优秀级最高； 只包含 weight=80 的标签节点次之；然后只是包含 weight=20 的节点；剩下的节点排在最后。\n在一个包含 2 个节点的集群中部署\n如果在只有 node1 和 node2 的集群中部署一个 replica=5 的 Deployment，Pod 更多部署到 node1， 有极少数调度到 node2 上。这是因为 kube-schduler 除了节点亲和的优先级函数， 还有其他函数来决定 Pod 调度到哪里。其中之一就是 Selector SpreadPriority 函数， 此函数保证同一个 ReplicaSet 或者 Service 的 Pod，将分散到不同节点上。 避免单个节点失效而出现整个服务挂掉。\n3. 使用亲和/反亲和部署 Pod #  刚才描述了 Pod 和节点之间的亲和关系，也有 Pod 与 Pod 之间的亲和关系， 例如前端 Pod 和后端 Pod，尽可能部署较近，减少网络时延。\n3.1 使用 Pod 亲和将多个 Pod 部署到同一个节点上 #  部署 1 个后端 Pod 和 5 个前端 Pod。先部署后端：\nkubectl run backend -l app=backend --image busybox --sleep 9999 这里没什么区别，只是给 Pod 加了个标签，app=backend。\n在 Pod 中指定亲和性\napiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 template: ... spec: affinity: podAffinity: requireddDuringSchdulingIgnoredDuringExecution: - topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: backend 这里强制要求，frontend 的 Pod 被调度到和其他包含 app=backend 标签的 Pod 所在的相同节点上。 这是通过 topologyKey 指定的。\n了解调度器如何使用 pod 亲和规则\n上面的应用部署后，5 个前端 Pod 和 1 个后端 Pod 都在 node2 上。 假设后端 Pod 被误删，重新调度后，依旧在 node2 上。 虽然后端 Pod 本身没有定义任何亲和性规则，如果调度到其他节点，即会打破已有的亲和规则。\n3.2 将 Pod 部署到同一个机柜、可用性区域或者地理地域 #  同一个可用性区域协调部署\n如果想要 Pod 在同一个 available zone 部署应用， 可以将 topologyKey 设置成 failure-domain.beta.kubernetes.io/zone。\n同一个地域协调部署 如果想要 Pod 在同一个 available zone 部署应用， 可以将 topologyKey 设置成 failure-domain.beta.kubernetes.io/region。\n了解 topologyKey 如何工作 topologyKey 的工作方式很简单，上面提到的三个属性并没有什么特别。可以设置成任意你想要的值。\n当调度器决定决定 Pod 调度到哪里时，首先检查 podAffinity 配置，选择满足条件的 Pod， 接着查询这些 Pod 运行在那些节点上。特别寻找能匹配 podAffinity 配置的 topologyKey 的节点。 接着，会优先选择所有标签匹配的 Pod 的值的节点。\n3.3 Pod 优先级亲和性 #  与节点亲和一样，pod 之间也可以用 prefereddDuringSchdulingIgnoredDuringExecution。\napiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 template: ... spec: affinity: podAffinity: prefereddDuringSchdulingIgnoredDuringExecution: - weight: 80 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: backend 和 nodeAffinity 一样，设置了权重。也需要设置 topologyKey 和 labelSelector。\n3.4 Pod 反亲和分开调度 #  同理，有 nodeAntiAffinity，也有 podAntiAffinity。 这将导致调度器永远不会选择包含 podAntiAffinity 的 Pod 所在的节点。\n使用反亲和分散 Pod\napiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 template: metadata: labels: app: frontend spec: affinity: podAffinity: requiredDuringSchdulingIgnoredDuringExecution: topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: frontend 上面的 Deployment 创建后，5 个实例应当分布在 5 个不同节点上。\n理解 Pod 反亲和优先级\n在这种情况下，可能使用软反亲和策略（prefereddDuringSchdulingIgnoredDuringExecution）。 毕竟 2 个前端 Pod 在同一个节点上也不是什么问题。如果运行在一个节点上出现问题， 那么使用 requiredDuringSchdulingIgnoredDuringExecution 就比较合适了。 与 Pod 亲和一样，topologyKey 决定了 Pod 不能被调度的范围。\n"});index.add({'id':13,'href':'/docs/kubernetes/kubelet/kubelet-eviction-manager/','title':"Eviction Manager 工作机制",'section':"kubelet",'content':"1. 概述 #  在可用计算资源较少时，kubelet 为保证节点稳定性，会主动地结束一个或多个 pod 以回收短缺地资源， 这在处理内存和磁盘这种不可压缩资源时，驱逐 pod 回收资源的策略，显得尤为重要。 下面来具体研究下 Kubelet Eviction Policy 的工作机制。\n kubelet 预先监控本节点的资源使用，防止资源被耗尽，保证节点稳定性。 kubelet 会预先 Fail N(\u0026gt;=1) 个 Pod，以回收出现紧缺的资源。 kubelet 在 Fail 一个 pod 时，kill 掉 pod 内所有 container，并设置 pod.status.phase = Failed。 kubelet 按照事先设定好的 Eviction Threshold 来触发驱逐动作，实现资源回收。  1.1 驱逐信号 #  在源码 pkg/kubelet/eviction/api/types.go 中定义了以下及几种 Eviction Signals：\n   Eviction Signal Description     memory.available := node.status.capacity[memory] - node.stats.memory.workingSet   nodefs.available := node.stats.fs.available   nodefs.inodesFree := node.stats.fs.inodesFree   imagefs.available := node.stats.runtime.imagefs.available   imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree   allocatableMemory.available := pod.allocatable - pod.workingSet   pid.available := node.MaxPID - node.NumOfRunningProcesses    上表主要涉及三个方面，memory、file system 和 pid。其中 kubelet 值支持 2 种文件系统分区：\n nodefs：kubelet 用来存储 volume 和 daemon logs 等 imagesfs：容器运行时 ( docker 等）用来保存镜像和容器的 writable layer  1.2 驱逐阈值 #  kubelet 的入参接收用户定义的 eviction signal 和 eviction threshold 的映射关系，格式如下：\n[eviction-signal] [opterator] [quantity]\n 支持的 signal 如上表所示； operator 是关系运算符，例如\u0026lt;； quantity 是驱逐阈值，合法的值必须是 kubernetes 使用的数量表示，例如 1Gi 和 10% 等；  1.2.1 软驱逐策略 #  Soft Eviction Thresholds，它与以下三个参数配合使用：\n eviction-soft：(e.g. memory.available\u0026lt;1.5Gi) 触发软驱逐的阈值； eviction-soft-grace-period：(e.g. memory.available=1m30s) 当达到软驱逐的阈值，需要等待的时间； 在这段时间内，每 10s 会重新获取监控数据并更新 threshold 值， 如果在等待期间，最后一次的数据仍然超过阈值，才会触发驱逐 pod 的行为。 eviction-max-pod-grace-period：(e.g. 30s) 当满足软驱逐阈值并终止 pod 时允许的最大宽限期值。 如果待 Evict 的 Pod 指定了pod.Spec.TerminationGracePeriodSeconds， 则取min(eviction-max-pod-grace-period, pod.Spec.TerminationGracePeriodSeconds) 作为 Pod Termination 真正的 Grace Period。  因此，在软驱逐策略下，从 kubelet 检测到驱逐信号达到了阈值设定开始，到 pod 真正被 kill 掉， 共花费的时间是：sum(eviction-max-pod-grace-period, min(eviction-max-pod-grace-period, pod.Spec.TerminationGracePeriodSeconds))\n1.2.2 硬驱逐 #  Hard Eviction Thresholds 比 Soft Eviction Thresholds 简单粗暴，没有宽限期， 即使 pod 配置了 pod.Spec.TerminationGracePeriodSeconds，一旦达到阈值配置， kubelet 立马回收关联的短缺资源，并且使用的就立即结束，而不是优雅终止。 此特性已经标记为 Deprecated。\n源码 pkg/kubelet/apis/config/v1beta1/defaults_linux.go 给出了默认的硬驱逐配置：\n memory.available \u0026lt; 100Mi nodefs.available \u0026lt; 10% nodefs.inodesFree \u0026lt; 5% imagefs.available \u0026lt; 15%  1.3 驱逐周期 #  有了驱逐信号和阈值，也有了策略，接下来就是 Eviction Monitoring Interval。 kubelet 对应的监控周期，就通过 cAdvisor 的 housekeeping-interval 配置的，默认 10s。\n1.4 节点状态 #  kubelet 监测到配置的驱逐策略被触发，会将驱逐信号映射到对应的节点状态。 Kubelet 会将对应的 Eviction Signals 映射到对应的 Node Conditions， 源码 [pkg/kubelet/eviction/helpers.go]，其映射关系如下：\n   节点状态 驱逐信号 描述     MemoryPressure memory.avaliable, allocatableMemory.available 节点或 pod 的可用内存触发驱逐阈值   DiskPressure nodefs.avaliable, nodefs.inodesFree, imagefs.available, imagesfs.inodesFree 节点的 root fs 或 i mage fs 上的可用磁盘空间和索引节点已满足收回阈值   PIDPressure pid.available 节点的可用 PID 触发驱逐阈值    kubelet 映射了 Node Condition 之 后，会继续按照--node-status-update-frequency(default 10s) 配置的时间间隔， 周期性的与 kube-apiserver 进行 node status updates。\n1.5 节点状态振荡 #  ​考虑这样一种场景，节点上监控到 soft eviction signal 的值，始终在 eviction threshold 上下波动， 那么 kubelet 就会将该 node 对应的 node condition 在 true 和 false 之间来回切换。 给 kube-scheduler 产生错误的调度结果。\n​因此，kubelet 添加参数 eviction-pressure-transition-period (default 5m0s) 配置， 使 Kubelet 在解除由 Evicion Signal 映射的 Node Pressure 之前，必须等待 5 分钟。\n​驱逐逻辑添加了一步：\n Soft Evction Singal 高于 Soft Eviction Thresholds 时， Kubelet 还是会立刻设置对应的 MemoryPressure 或 DiskPressure 为 True。 当 MemoryPressure 或 DiskPressure 为 True 的前提下， 发生了 Soft Evction Singal 低于 Soft Eviction Thresholds 的情况， 则需要等待 eviction-pressure-transition-period(default 5m0s) 配置的这么长时间， 才会将 condition pressure 切换回 False。  一句话总结：Node Condition Pressure 成为 True 容易，切换回 False 则要等eviction-pressure-transition-period。\n1.6 回收节点层级资源 #  如果满足驱逐阈值并超过了宽限期，kubelet 将启动回收压力资源的过程， 直到它发现低于设定阈值的信号为止。 kubelet 将尝试在驱逐终端用户 pod 前回收节点层级资源。 发现磁盘压力时，如果节点针对容器运行时配置有独占的 imagefs， kubelet 回收节点层级资源的方式将会不同。\n1.6.1 使用 imagefs #   如果 nodefs 文件系统满足驱逐阈值，kubelet 通过驱逐 pod 及其容器来释放磁盘空间。 如果 imagefs 文件系统满足驱逐阈值，kubelet 通过删除所有未使用的镜像来释放磁盘空间。  1.6.2 未使用 imagefs #   删除停止运行的 pod/container 删除全部没有被使用的镜像  1.7 驱逐策略 #  ​kubelet 根据 Pod 的 QoS Class 实现了一套默认的 Evication 策略， kubelet 首先根据他们对短缺资源的使用是否超过请求来排除 pod 的驱逐行为， 然后通过优先级，然后通过相对于 pod 的调度请求消耗急需的计算资源。图解如下：\n ​对于每一种 Resource 都可以将容器分为 3 种 QoS Classes: Guaranteed, Burstable 和 Best-Effort， 它们的 QoS 级别依次递减。\n BestEffort，按照短缺资源占用量排序，占用量越高，被 kill 的优先级越高； Burstable，对使用量高于请求量的 pod 排序，占用越多，回收优先级越高； 如果没有 pod 的使用超过请求，按照 BestEffort 策略回收； Guaranteed，Guaranteed pod 只有为所有的容器指定了要求和限制并且它们相等时才能得到保证。 由于另一个 pod 的资源消耗，这些 pod 保证永远不会被驱逐。 如果系统守护进程（例如 kubelet、docker、和 journald） 消耗的资源多于通过 system-reserved 或 kube-reserved 分配保留的资源， 并且该节点只有 Guaranteed 或 Burstable pod 使用少于剩余的请求， 然后节点必须选择驱逐这样的 pod 以保持节点的稳定性并限制意外消耗对其他 pod 的影响。 在这种情况下，它将首先驱逐优先级最低的 pod。  1.8 最小驱逐回收 #  有些情况下，可能只回收一小部分的资源就能使得 Evication Signal 的值低于 eviction thresholds。 但是，可能随着资源使用的波动或者新的调度 Pod 使得在该 Node 上很快又会触发 evict pods 的动作， eviction 毕竟是耗时的动作，所以应该尽量避免这种情况的发生。\n为了减少这类问题，每次 Evict Pods 后，Node 上对应的 Resource 不仅要比 Eviction Thresholds 低， 还要保证最少比 Eviction Thresholds，再低 --eviction-minimum-reclaim 中配置的数量。\n例如使用下面的配置：\n--eviction-hard=memory.available\u0026lt;500Mi,nodefs.available\u0026lt;1Gi,imagefs.available\u0026lt;100Gi --eviction-minimum-reclaim=\u0026#34;memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi\u0026#34; 如果 memory.available 驱逐阈值被触发，kubelet 将保证 memory.available 至少为 500Mi。 对于 nodefs.available，kubelet 将保证 nodefs.available 至少为 1.5Gi。 对于 imagefs.available，kubelet 将保证 imagefs.available 至少为 102Gi， 直到不再有相关资源报告压力为止。\n所有资源的默认 eviction-minimum-reclaim 值为 0。\n"});index.add({'id':14,'href':'/docs/kubernetes/kube-proxy/learn-about-Service/','title':"深入了解 Service",'section':"kube-proxy",'content':"1. 基本概念 #  1.1 Service 定义详解 #  Service 是对一组提供相同功能的 Pods 的抽象，并为它们提供一个统一的入口。借助 Service， 应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service 通过标签来选取服务后端， 一般配合 Replication Controller 或者 Deployment 来保证后端容器的正常运行。 这些匹配标签的 Pod IP 和端口列表组成 endpoints， 由 kube-proxy 负责将服务 IP 负载均衡到这些 endpoints 上。\napiVersion: v1 kind: Service metadata: name: string namespace: string labels: - name: string annotations: - name: string spec: selector: [] # ClusterIP、NodePort、LoadBalancer type: string # type=ClusterIP, 有自动分配的能力；type=LoadBalancer，需指定  clusterIP: string  # 是否支持 session，默认为空，可选值 ClutserIP，同一个 client 的 request，都发送到同一个后端 Pod  sessionAffinity: string  ports: - name: string # tcp、udp，默认 tcp protocol: string  port: int targetPort: int nodePort: int # spec.type=LoadBalancer, 设置外部负载均衡器地址，用于公有云环境 status: loadBalancer: ingress: ip: string hostname: string 1.2 Service 分类 #   ClusterIP：默认类型，自动分配一个仅 cluster 内部可以访问的虚拟 IP NodePort：在 ClusterIP 基础上为 Service 在每台机器上绑定一个端口， 这样就可以通过 http://\u0026lt;NodeIP\u0026gt;:NodePort 来访问该服务。 如果 kube-proxy 设置了 --nodeport-addresses=10.240.0.0/16（v1.10 支持）， 那么仅该 NodePort 仅对设置在范围内的 IP 有效。 LoadBalancer：在 NodePort 的基础上，借助 cloud provider 创建一个外部的负载均衡器， 并将请求转发到 \u0026lt;NodeIP\u0026gt;:NodePort ExternalName：将服务通过 DNS CNAME 记录方式转发到指定的域名（通过 spec.externlName 设定）。 需要 kube-dns 版本在 1.7 以上。  2. Service 基本用法 #  一般来说，对外提供服务的应用程序需要通过某种机制来实现， 对于容器应用最简便的方式就是通过 TCP/IP 机制及监听 IP 和端口号来实现。\n直接通过 Pod 的 IP 地址和端口号可以访问到容器应用内的服务，但是 Pod 的 IP 地址是不可靠的， 例如当 Pod 所在的 Node 发生故障时，Pod 将被 Kubernetes 重新调度到另一个 Node， Pod 的 IP 地址将发生变化。 更重要的是，如果容器应用本身是分布式的部署方式，通过多个实例共同提供服务， 就需要在这些实例的前端设置一个负载均衡器来实现请求的分发。 Kubernetes 中的 Service 就是用于解决这些问题的核心组件。\nService 定义中的关键字段是 ports 和 selector。通过 selector 与 Pod 关联， port 描述 service 本身的端口，targetPort 表示流量转发的端口，也就是 Pod 的端口， 从而完成访问 service 负载均衡到后端任意一个 Pod。Kubernetes 提供了两种负载分发策略： RoundRobin 和 SessionAffinity，具体说明如下：\n RoundRobin：轮询模式，即轮询将请求转发到后端的各个 Pod 上。 SessionAffinity：基于客户端 IP 地址进行会话保持的模式， 即第 1 次将某个客户端发起的请求转发到后端的某个 Pod 上， 之后从相同的客户端发起的请求都将被转发到后端相同的 Pod 上。  在默认情况下，Kubernetes 采用 RoundRobin 模式对客户端请求进行负载分发， 但我们也可以通过设置 service.spec.sessionAffinity=ClientIP 来启用 SessionAffinity 策略。 这样，同一个客户端 IP 发来的请求就会被转发到后端固定的某个 Pod 上了。\n2.1 集群内访问集群外服务 #  到现在为止，我们己经讨论了后端是集群中运行的一个或多个 Pod 的服务。 但也存在希望通过 Kubernetes 服务特性暴露外部服务的情况。 不要让服务将连接重定向到集群中的 Pod，而是让它重定向到外部 IP 和端口。 这样做可以让你充分利用服务负载平衡和服务发现。 在集群中运行的客户端 Pod 可以像连接到内部服务一样连接到外部服务。\n首先要知道，service 和 Pod 并不是直接相连的，此二者之间还有一个对象叫 endpoint。 service 根据 selector 找到后端 Pod，用 Pod IP 和端口创建与 service 同名的 endpoint， 记录 Pod IP。当 Pod 异常被删除重建后，获得的新地址，只需要更新 endpoint 中记录的 Pod I P 即可。 因此，想要访问集群外部的服务，可手动配置 service 的 endpoint。\n2.1.1 创建没有 selector 的 Service #   创建没有 selector 的 Service  apiVersion: v1 kind: Service metadata: name: external-service spec: ports: - port: 80 为没有选择器的服务创建 Endpoint 资源  apiVersion: v1 kind: Endpoint metadata: # endpoint 的名称必须和服务的名称相匹配 name: external-service  subsets: - addresses: # 将 service 重定向到 endpoint 的地址 - ip: 11.11.11.11 - ip: 22.22.22.22 ports: # endpoint 目标端口 - port: 80 Endpoint 对象需要与服务具有相同的名称，并包含该服务的目标 IP 地址和端口列表。 服务和 Endpoint 资源都发布到服务器后，这样服务就可以像具有 Pod 选择器那样的服务正常使用。 在服务创建后创建的容器将包含服务的环境变量，并且与其 IP:Port 对的所有连接都将在服务端点之间进行负载均衡。\n2.1.2 创建 ExternalName 的 service #  除了手动配置服务的 Endpoint 来代替公开外部服务方法，有一种更简单的方法， 就是通过其完全限定域名 (FQDN) 访问外部服务。\n要创建一个具有别名的外部服务的服务时，要将创建服务资源的一个 type 字段设置为 ExternalName。 例如，设想一下在 api.somecompany.com 上有公共可用的 API 可以定义一个指向它的服务， 如下面的代码清单所示：\napiVersion: v1 kind: Service metadata: name: external-service spec: type: ExternalName externalName: someapi.somecompany.com ports: - port: 80 服务创建完成后，Pod 可以通过 external-service.default.svc.cluster.local 域名 （甚至是 external-service) 连接到外部服务，而不是使用服务的实际 FQDN。 这隐藏了实际的服务名称及其使用该服务的 Pod 的位置，允许修改服务定义， 并且在以后如果将其指向不同的服务，只需简单地修改 externalName 属性， 或者将类型重新变回 Cluster IP 并为服务创建 Endpoint ——无论是手动创建， 还是对服务上指定标签选择器使其自动创建。\nExternalName 服务仅在 DNS 级别实施——为服务创建了简单的 CNAME DNS 记录。 因此，连接到服务的客户端将直接连接到外部服务，完全绕过服务代理。 出于这个原因，这些类型的服务甚至不会获得集群 IP。\n2.2 集群外访问集群内服务 #  2.2.1 NodePort 服务 #  将服务的类型设置成 NodePort：每个集群节点都会在节点上打开一个端口，对于 NodePort 服务， 每个集群节点在节点本身（因此得名叫 NodePort) 上打开一个端口， 并将在该端口上接收到的流量重定向到基础服务。该服务仅在内部集群 IP 和端口上才可访间， 但也可通过所有节点上的专用端口访问。\napiVersion: v1 kind: Service metadata: name: kubia-nodeport spec: type: NodePort ports: - port: 80 targetPort: 8080 nodePort: 30123 selector: app: kubia 2.2.2 LoadBalancer 服务 #  在云提供商上运行的 Kubernetes 集群通常支持从云基础架构自动提供负载平衡器。 所有需要做的就是设置服务的类型为 LoadBadancer 而不是 NodePort。 负载均衡器拥有自己独一无二的可公开访问的 IP 地址，并将所有连接重定向到服务。 可以通过负载均衡器的 IP 地址访问服务。\n如果 Kubemetes 在不支持 LoadBadancer 服务的环境中运行，则不会调配负载平衡器， 但该服务仍将表现得像一个 NodePort 服务。这是因为 LoadBadancer 服务是 NodePort 服务的扩展。\napiVersion: v1 kind: Service metadata: name: kubia-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 8080 selector: app: kubia clusterIP: 10.0.171.239 loadBalancerIP: 78.11.24.19 status: loadBalancer: ingress: - ip: 146.148.47.155 3. 通过 Ingress 暴露服务 #   为什么需要 Ingress？  一个重要的原因是每个 LoadBalancer 服务都需要自己的负载均衡器，以及独有的公有 IP 地址， 而 Ingress 只需要一个公网 IP 就能为许多服务提供访问。当客户端向 Ingress 发送 HTTP 请求时， Ingress 会根据请求的主机名和路径决定请求转发到的服务。\n3.1 创建 Ingress Controller 和默认的 backend 服务 #  在定义 Ingress 策略之前，需要先部署 Ingress Controller， 以实现为所有后端 Service 都提供一个统一的入口。 Ingress Controller 需要实现基于不同 HTTP URL 向后转发的负载分发规则， 并可以灵活设置 7 层负载分发策略。如果公有云服务商能够提供该类型的 HTTP 路由 LoadBalancer， 则也可设置其为 Ingress Controller。\n在 Kubernetes 中，Ingress Controller 将以 Pod 的形式运行， 监控 API Server 的 ingress 接口后端的 backend services， 如果 Service 发生变化，则 Ingress Controller 应自动更新其转发规则。\n下面的例子使用 Nginx 来实现一个 Ingress Controller，需要实现的基本逻辑如下：\n 监听 API Server，获取全部 Ingress 的定义。 基于 Ingress 的定义，生成 Nginx 所需的配置文件 /etc/nginx/nginx.conf。 执行 nginx -s reload 命令，重新加载 nginx.conf 配置文件的内容。  为了让 Ingress Controller 正常启动，还需要为它配置一个默认的 backend， 用于在客户端访问的 URL 地址不存在时，返回一个正确的 404 应答。 这个 backend 服务用任何应用实现都可以，只要满足对根路径“/”的访问返回 404 应答， 并且提供 /healthz 路径以使 kubelet 完成对它的健康检查。 另外，由于 Nginx 通过 default-backend-service 的服务名称（Service Name）去访问它， 所以需要 DNS 服务正确运行。\n3.2 创建 Ingress 资源 #  3.2.1 转发到单个后端服务上 #  基于这种设置，客户端到 Ingress Controller 的访问请求都将被转发到后端的唯一 Service 上， 在这种情况下 Ingress 无须定义任何 rule。\n通过如下所示的设置，对 Ingress Controller 的访问请求都将被转发到“myweb:8080”这个服务上。\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec: backend: serviceName: webapp servicePort: 8080 3.2.2 将不同的服务映射到相同主机的不同路径 #  这种配置常用于一个网站通过不同的路径提供不同的服务的场景， 例如 /web 表示访问 Web 页面，/api 表示访问 API 接口，对应到后端的两个服务， 通过 Ingress 的设置很容易就能将基于 URL 路径的转发规则定义出来。\n通过如下所示的设置，对 mywebsite.com/web 的访问请求将被转发到 web-service:80 服务上； 对 mywebsite.com/api 的访问请求将被转发到 api-service:80 服务上：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec rules: - host mywebsite.com http: paths: - path: /web backend: serviceName: web-service servicePort: 80 - path: /api backend: serviceName: api-service servicePort: 80 3.2.3 不同的域名（虚拟主机名）被转发到不同的服务上 #  这种配置常用于一个网站通过不同的域名或虚拟主机名提供不同服务的场景， 例如 foo.example.com 域名由 foo 提供服务，bar.example.com 域名由 bar 提供服务。\n通过如下所示的设置，对“foo.example.com”的访问请求将被转发到“foo:80”服务上， 对“bar.example.com”的访问请求将被转发到“bar:80”服务上：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec: rules: - host: foo.example.com http: paths: - path: I backend: serviceName: foo servicePort: 80 - host: bar.example.com http: paths: - path: I backend: serviceName: bar servicePort: 80 3.2.4 不使用域名的转发规则 #  这种配置用于一个网站不使用域名直接提供服务的场景， 此时通过任意一台运行 ingress-controller 的 Node 都能访问到后端的服务。\n下面的配置为将“/demo”的访问请求转发到“webapp:8080/demo”服务上：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec: rules: - http: paths: - path: /demo backend: serviceName: webapp servicePort: 8080 注意，使用无域名的 Ingress 转发规则时，将默认禁用非安全 HTTP，强制启用 HTTPS。 可以在 Ingress 的定义中设置一个 annotation[ingress.kubernetes.io/ssl-redirect: \u0026ldquo;false\u0026rdquo;] 来关闭强制启用 HTTPS 的设置：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress annotations: ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; spec: rules: - http: paths: - path: /demo backend: serviceName: webapp servicePort: 8080 3.3 Ingress 的 TLS 安全设置 #  当客户端创建到 Ingress 控制器的 TLS 连接时，控制器将终止 TLS 连接。 客户端和控制器之间的通信是加密的，而控制器和后端 Pod 之间的通信则不是运行在 Pod 上的应用程序不需要支持 TLS。 例如，如果 Pod 运行 web 服务器，则它只能接收 HTTP 通信，并让 Ingress 控制器负责处理与 TLS 相关的所有内容。 要使控制器能够这样做，需要将证书和私钥附加到 Ingress。 这两个必需资源存储在称为 Secret 的 Kubernetes 资源中，然后在 Ingress manifest 中引用它。\n为了 Ingress 提供 HTTPS 的安全访问，可以为 Ingress 中的域名进行 TLS 安全证书的设置。设置的步骤如下。\n 创建自签名的密钥和 SSL 证书文件  openssl genrsa -out tls.key 2048 openssl req -new - x509 -key tls.key -out tls.cert -days 360 -subject/CN=kubia.example.com 将证书保存到 Kubernetes 中的一个 Secret 资源对象上  kubectl create secret tls mywebsite-tls-secret --cert=tls.cert --key=tls.key 将该 Secret 对象设置到 Ingress 中  apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: mywebsite-ingress-tls spec: tls: - hosts: - mywebsite.com secretName: mywebsite-tls-secret rules: - http: paths: - path: /demo backend: serviceName: webapp servicePort: 8080 根据提供服务的网站域名是一个还是多个，可以使用不同的操作完成前两步 SSL 证书和 Secret 对象的创建， 在只有一个域名的情况下设置相对简单。第 3 步对于这两种场景来说是相同的。\n"});index.add({'id':15,'href':'/docs/kubernetes/kube-apiserver/crd/','title':"CRD 入门和使用",'section':"kube-apisever",'content':"1. Customer Resource #  自定义资源是 Kubernetes API 的扩展，本文将讨什么时候应该向 Kubernetes 集群添加自定义资源以及何时使用独立服务。它描述了添加自定义资源的两种方法以及如何在它们之间进行选择。\n1.1 自定义资源概述 #  资源就是 Kubernetes API 集合中的某个的对象。例如，内置 pods 资源包含 Pod 对象的集合。\n自定义资源是扩展了 Kubernetes API，但在默认的 API 集合中是不可用的，因为没有对应的 controller 处理业务逻辑。使用自定义资源，不仅可以解耦大多数的自研功能，还可用使得 Kubernetes 更加模块化。\n自定义资源可以通过动态注册的方法，于正在 running 的集群中创建、删除和更新，并且与集群本身的资源是相互独立的。当自定义资源被创建，就可以通过 kubectl 创建和访问对象，和对内置资源的操作完全一致。\n1.2 是否需要自定义资源 #  在创建新的 API 时，应该考虑与 Kubernetes 的 API 聚合，还是让 API 独立运行。\n   API 聚合 API 独立     声明式 非声明式   kubectl 可读可写 不需要 kubectl 支持   接受 k8s 的 REST 限制 特定 API 路径   可限定为 cluster 或者 namespace 不适用 namespace   重用 k8s API 支持的功能 不需要这些功能    1.3 声明式 API VS 命令式 API #  在声明式 API，通过具有以下特性：\n 对象颗粒度小 基础结构的定义 读写多，更新少，操作主要为 CRUD API 表期望状态  命令式 API，通过具有以下特性：\n 同步响应 RPC 调用 大量数据存储（大对象或多对象） 高带宽访问 操作非 CRUD API 不易抽象  2. Customer Resource Definition #  Kubernetes 提供了两种向集群添加自定义资源的方法：\n 创建自定义 API server 并聚合到 API 中 CRDs  2.1 Customer Resource Definition 介绍 #   kubernetes: v1.19.0\n 通过 CRD 创建自定义资源，只要符合 CRD 结构体定义，均可以被 Kubernetes 所接收，CRD 的定义和 k8s 原生资源的定义非常类似，都是由 4 个子资源组成：TypeMeta、ObjectMeta、Spec、Status，具体代码如下：\nstaging/src/k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/types.go:354\ntype CustomResourceDefinition struct { metav1.TypeMeta metav1.ObjectMeta Spec CustomResourceDefinitionSpec Status CustomResourceDefinitionStatus } 下面主要来看下 Spec 和 Status 字段：\ntype CustomResourceDefinitionSpec struct { Group string // 定义复数、单数，简称，Kind，ListKind，Categories \tNames CustomResourceDefinitionNames // 表示集群级别或 namespace 级别 \tScope ResourceScope // 定义 CR 所有支持的版本号 \tVersions []CustomResourceDefinitionVersion // 定义不同版本之间的转换方法 \tConversion *CustomResourceConversion // 是否保留未知字段，apiVersion、kind、metadata 等总是保留 \tPreserveUnknownFields *bool } type CustomResourceDefinitionStatus struct { // 描述资源的当前状态，即 status.conditions \tConditions []CustomResourceDefinitionCondition // 与 spec.names 类似，即 status.acceptNames \tAcceptedNames CustomResourceDefinitionNames // 资源曾经存在的版本，可在 etcd 中存储，供迁移用 \tStoredVersions []string } 2.2 CRD 定义示例 #  apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # 名称必须符合下面的格式：\u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: foos.samplecontroller.io spec: # REST API 使用的组名称：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: samplecontroller.io # REST API 使用的版本号：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; versions: - name: v1alpha1 served: true storage: true schema: # 校验方法 openAPIV3Schema: type: object properties: spec: type: object properties: #类型校验 replicas: type: integer deploymentName: type: string # Namespaced 或 Cluster scope: Namespaced names: # URL 中使用的复数名称：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: foos # CLI 中使用的单数名称 singular: foo # CamelCased 格式的单数类型。在清单文件中使用 kind: Foo # 资源的 List 类型，默认是 “`kind`List” listKind: FooList # CLI 中使用的资源简称 shortNames: - fo # 分组 categories: - all 2.3 Customer Controller #  只定义资源，不实现资源控制器是没有任何意义的。自定义控制器能够完成业务逻辑，最主要是依赖 client-go 库的各个组件的交互。下图展示它们之间的关系：\n 通过图示，可以看到几个核心组件的交互流程，蓝色表示 client-go，黄色是自定义 controller，各组件作用介绍如下：\n2.3.1 client-go 组件 #   Reflector：reflector 用来 watch 特定的 k8s API 资源。具体的实现是通过 ListAndWatch 的方法，watch 可以是 k8s 内建的资源或者是自定义的资源。当 reflector 通过 watch API 接收到有关新资源实例存在的通知时，它使用相应的列表 API 获取新创建的对象，并将其放入 watchHandler 函数内的 Delta FIFO 队列中。 Informer：informer 从 Delta FIFO 队列中弹出对象。执行此操作的功能是 processLoop。base controller 的作用是保存对象以供以后检索，并调用我们的控制器将对象传递给它。 Indexer：索引器提供对象的索引功能。典型的索引用例是基于对象标签创建索引。 Indexer 可以根据多个索引函数维护索引。Indexer 使用线程安全的数据存储来存储对象及其键。 在 Store 中定义了一个名为 MetaNamespaceKeyFunc 的默认函数，该函数生成对象的键作为该对象的 namespace/name 组合。  2.3.2 自定义 controller 组件 #   Informer reference：指的是 Informer 实例的引用，定义如何使用自定义资源对象。自定义控制器代码需要创建对应的 Informer。 Indexer reference：自定义控制器对 Indexer 实例的引用。自定义控制器需要创建对应的 Indexer。 Resource Event Handlers：资源事件回调函数，当它想要将对象传递给控制器时，它将被调用。编写这些函数的典型模式是获取调度对象的 key，并将该 key 排入工作队列以进行进一步处理。 Work queue：任务队列。编写资源事件处理程序函数以提取传递的对象的 key 并将其添加到任务队列。 Process Item：处理任务队列中对象的函数，这些函数通常使用 Indexer 引用或 Listing 包装器来重试与该 key 对应的对象。  简单的说，整个处理流程大概为：Reflector 通过检测 Kubernetes API 来跟踪该扩展资源类型的变化，一旦发现有变化，就将该 Object 存储队列中，Informer 循环取出该 Object 并将其存入 Indexer 进行检索，同时触发 Callback 回调函数，并将变更的 Object Key 信息放入到工作队列中，此时自定义 Controller 里面的 Process Item 就会获取工作队列里面的 Key，并从 Indexer 中获取 Key 对应的 Object，从而进行相关的业务处理。\n3. sample-controller 使用 #  3.1 资源准备 #  3.1.1 foo.crd.yaml #  使用 2.2 节的示例。\n3.1.2 example.foo.yaml #  创建 foo 资源类型的对象：\napiVersion: samplecontroller.io/v1alpha1 kind: Foo metadata: name: example-foo spec: deploymentName: example-foo replicas: 1 3.2 sample-controller 部署 #  3.2.1 代码 #  获取示例代码：\ngit clone https://github.com/kubernetes/sample-controller.git cd sample-controller 3.2.2 编译 #  在编译二进制之前，要修改 pkg/apis/samplecontroller/register.go:21 GroupName 的定义，k8s.io 和 kubernetes.io 后缀都是官方保留字段，不建议 CRD 使用。所以将其修改为 samplecontroller.io。\ngo build -o sample-controller . 3.2.3 启动 #  使用节点上的 kubeconfig 文件启动 controller：\n./sample-controller -kubeconfig=$HOME/.kube/config 启动时，会提示 watch 不到资源，因为还没创建定义：\nI1026 23:40:56.529233 57978 controller.go:115] Setting up event handlers I1026 23:40:56.529444 57978 controller.go:156] Starting Foo controller I1026 23:40:56.529450 57978 controller.go:159] Waiting for informer caches to sync E1026 23:40:56.554564 57978 reflector.go:138] k8s.io/sample-controller/pkg/generated/informers/externalversions/factory.go:117: Failed to watch *v1alpha1.Foo: failed to list *v1alpha1.Foo: the server could not find the requested resource (get foos.samplecontroller.io) E1026 23:40:57.841194 57978 reflector.go:138] k8s.io/sample-controller/pkg/generated/informers/externalversions/factory.go:117: Failed to watch *v1alpha1.Foo: failed to list *v1alpha1.Foo: the server could not find the requested resource (get foos.samplecontroller.io) 3.2.4 创建资源 #  $ kubectl create -f foos.crd.yaml customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.io created $ kubectl create -f example.foos.yaml foo.samplecontroller.io/example-foo created 创建完成，可以使用 kubectl get crd 查看资源定义和对象：\n$ kubectl get crd NAME CREATED AT clusterversions.cfe.io 2020-10-26T14:43:27Z foos.samplecontroller.io 2020-10-26T15:41:52Z $ kubectl get foos NAME AGE example-foo 19m sample-controller 会 watch 到新对象 default/example-foo：\nI1026 23:42:03.232577 57978 controller.go:164] Starting workers I1026 23:42:03.232607 57978 controller.go:170] Started workers I1026 23:42:03.515970 57978 controller.go:228] Successfully synced \u0026#39;default/example-foo\u0026#39; I1026 23:42:03.516099 57978 event.go:291] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/example-foo\u0026#34; kind=\u0026#34;Foo\u0026#34; apiVersion=\u0026#34;samplecontroller.io/v1alpha1\u0026#34; type=\u0026#34;Normal\u0026#34; reason=\u0026#34;Synced\u0026#34; message=\u0026#34;Foo synced successfully\u0026#34; 3.2.5 校验 #  检查 controller 创建 Deployment 对象：\n$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE example-foo 1/1 1 1 3m1s $ kubectl get pod NAME READY STATUS RESTARTS AGE example-foo-54dc4db9fc-4r8pp 1/1 Running 0 3m8s shell-demo 1/1 Running 9 43d web-0 1/1 Running 16 78d web-1 1/1 Running 16 78d 4. 参考资料 #    定制资源  controller-client-go  sample-controller  "});index.add({'id':16,'href':'/menu/','title':"Menu",'section':"介绍",'content':"  Golang   数据结构   切片  哈希表    语言基础   反射    内存管理   内存模型      Kubernetes   kube-apiserver   垃圾回收  k8s watch 关键设计    kube-controller-manager   DaemonSet Controller 源码分析  Statefulset Controller 源码分析  k8s 应用滚动更新    kube-proxy   深入了解 Service    kubelet   Eviction Manager 工作机制  Topology Manager 设计方案    kube-scheduler   高级调度      "});index.add({'id':17,'href':'/docs/golang/data-structure/map/','title':"map",'section':"数据结构",'content':"1. 引言 #  粗略的讲，Go 语言中 map 采用的是哈希查找表， 由一个 key 通过哈希函数得到哈希值， 64 位系统中就生成一个 64 bit 的哈希值， 由这个哈希值将 key 对应到不同的桶（bucket）中， 当有多个哈希映射到相同的的桶中时，使用链表解决哈希冲突。\n1.1 hash 函数 #  首先要知道的就是 map 中哈希函数的作用，go 中 map 使用 hash 作查找， 就是将 key 作哈希运算，得到一个哈希值，根据哈希值确定 key-value 落在哪个 bucket 的哪个 cell。 golang 使用的 hash 算法和 CPU 有关，如果 CPU 支持 aes，那么使用 aes hash，否则使用 memhash。\n1.2 数据结构 #  hmap 可以理解为 header of map 的缩写，即 map 数据结构的入口。\ntype hmap struct { // map 中的元素个数，必须放在 struct 的第一个位置，因为 内置的 len 函数会从这里读取 \tcount int // map 状态标识，比如是否在被写或者迁移等，因为 map 不是线程安全的所以操作时需要判断 flags \tflags uint8 // log_2 of buckets （最多可以放 loadFactor * 2^B 个元素即 6.5*2^B，再多就要 hashGrow 了） \tB uint8 // overflow 的 bucket 的近似数 \tnoverflow uint16 // hash seed，随机哈希种子可以防止哈希碰撞攻击 \thash0 uint32 // 存储数据的 buckets 数组的指针， 大小 2^B，如果 count == 0 的话，可能是 nil \tbuckets unsafe.Pointer // 一半大小的之前的 bucket 数组，只有在 growing 过程中是非 nil \toldbuckets unsafe.Pointer // 扩容进度标志，小于此地址的 buckets 已迁移完成。 \tnevacuate uintptr // 可以减少 GC 扫描，当 key 和 value 都可以 inline 的时候，就会用这个字段 \textra *mapextra // optional fields } 用 mapextra 来存储 key 和 value 都不是指针类型的 map，并且大小都小于 128 字节，这样可以避免 GC 扫描整个 map。\ntype mapextra struct { // 如果 key 和 value 都不包含指针，并且可以被 inline(\u0026lt;=128 字节）  // 使用 extra 来存储 overflow bucket，这样可以避免 GC 扫描整个 map  // 然而 bmap.overflow 也是个指针。这时候我们只能把这些 overflow 的指针  // 都放在 hmap.extra.overflow 和 hmap.extra.oldoverflow 中了  // overflow 包含的是 hmap.buckets 的 overflow 的 bucket  // oldoverflow 包含扩容时的 hmap.oldbuckets 的 overflow 的 bucket  overflow *[]*bmap oldoverflow *[]*bmap // 指向空闲的 overflow bucket 的指针  nextOverflow *bmap } bmap 可以理解为 buckets of map 的缩写，它就是 map 中 bucket 的本体，即存 key 和 value 数据的“桶”。\ntype bmap struct { // tophash 是 hash 值的高 8 位  tophash [bucketCnt]uint8 // 以下字段没有显示定义在 bmap，但是编译时编译器会自动添加  // keys // 每个桶最多可以装 8 个 key  // values // 8 个 key 分别有 8 个 value 一一对应  // overflow pointer // 发生哈希碰撞之后创建的 overflow bucket } 根据哈希函数将 key 生成一个哈希值，其中低位哈希用来判断桶位置，高位哈希用来确定在桶中哪个 cell。 低位哈希就是哈希值的低 B 位，hmap 结构体中的 B，比如 B 为 5，2^5=32， 即该 map 有 32 个桶，只需要取哈希值的低 5 位就可以确定当前 key-value 落在哪个桶 (bucket) 中； 高位哈希即 tophash，是指哈希值的高 8 bits，根据 tophash 来确定 key 在桶中的位置。 每个桶可以存储 8 对 key-value，存储结构不是 key/value/key/value\u0026hellip;，而是 key/key..value/value， 这样可以避免字节对齐时的 padding，节省内存空间。\n当不同的 key 根据哈希得到的 tophash 和低位 hash 都一样，发生哈希碰撞，这个时候就体现 overflow pointer 字段的作用了。 桶溢出时，就需要把 key-value 对存储在 overflow bucket（溢出桶），overflow pointer 就是指向 overflow bucket 的指针。 如果 overflow bucket 也溢出了呢？那就再给 overflow bucket 新建一个 overflow bucket，用指针串起来就形成了链式结构， map 本身有 2^B 个 bucket，只有当发生哈希碰撞后才会在 bucket 后链式增加 overflow bucket。\n2. map 内存布局 #   2.1 扩容 #    装填因子是否大于 6.5\n装填因子 = 元素个数/桶个数，大于 6.5 时，说明桶快要装满，需要扩容\n  overflow bucket 是否太多\n​当 bucket 的数量 \u0026lt; 2^15，但 overflow bucket 的数量大于桶数量 ​当 bucket 的数量 \u0026gt;= 2^15，但 overflow bucket 的数量大于 2^15\n  双倍扩容：装载因子多大，直接翻倍，B+1；扩容也不是申请一块内存，立马开始拷贝，每一次访问旧的 buckets 时，就迁移一部分，直到完成，旧 bucket 被 GC 回收。\n等量扩容：重新排列，极端情况下，重新排列也解决不了，map 成了链表，性能大大降低，此时哈希种子 hash0 的设置，可以降低此类极端场景的发生。\n2.2 查找 #   根据 key 计算出哈希值 根据哈希值低位确定所在 bucket 根据哈希值高 8 位确定在 bucket 中的存储位置 当前 bucket 未找到则查找对应的 overflow bucket。 对应位置有数据则对比完整的哈希值，确定是否是要查找的数据 如果当前处于 map 进行了扩容，处于数据搬移状态，则优先从 oldbuckets 查找。  2.3 插入 #   根据 key 计算出哈希值 根据哈希值低位确定所在 bucket 根据哈希值高 8 位确定在 bucket 中的存储位置 查找该 key 是否存在，已存在则更新，不存在则插入  2.4 map 无序 #  map 的本质是散列表，而 map 的增长扩容会导致重新进行散列，这就可能使 map 的遍历结果在扩容前后变得不可靠， Go 设计者为了让大家不依赖遍历的顺序，故意在实现 map 遍历时加入了随机数， 让每次遍历的起点\u0026ndash;即起始 bucket 的位置不一样，即不让遍历都从 bucket0 开始， 所以即使未扩容时我们遍历出来的 map 也总是无序的。\n3. 参考资料 #    Golang map 底层实现  Go 语言之 map：map 的用法到 map 底层实现分析  Go 语言 map 底层浅析  哈希表  "});index.add({'id':18,'href':'/docs/golang/language-basics/reflect/','title':"reflect",'section':"语言基础",'content':"1. 引言 #  计算机中提到的反射一般是指，程序借助某种手段检查自己结构的一种能力，通常就是借助编程语言中定义的类型（types）。因此，反射是建立在类型系统上的。\ngo 是静态类型化，每个变量都有一个静态类型，也就是说，在编译时，变量的类型就已经确定。不显示地去做强制类型转换，不同类型之间是无法相互赋值的。\n有一种特殊的类型叫做接口（interface），一个接口表示的是一组方法集合。一个接口变量能存储任何具体的值，只要这个值实现了这个接口的方法集合。比如 io 包中的 Reader 和 Writer，io.Reader 接口变量能够保存任意实现了 Read() 方法的类型所定义的值。\n一个特殊接口就是空接口 interface{}，任何值都可以说实现了空接口，因为空接口中没有定义方法，所以空接口可以保存任何值。\n一个接口类型变量存储了一对值：赋值给这个接口变量的具体值 + 这个值的类型描述符。更进一步的讲，这个“值”是实现了这个接口的底层具体数据项（underlying concrete data item)，而这个“类型”是描述了具体数据项（item）的全类型（full type）。\n所以反射是干嘛的呢？反射是一种检查存储在接口变量中的（类型/值）对的机制。reflect 包中提供的 2 个类型 Type 和 Value，提供了访问接口值的 reflect.Type 和 reflect.Value 部分。\n2. 三大法则 #  2.1. Reflection goes from interface value to reflecton object #  从 interface{} 变量可以反射出反射对象\ntype MyInt int32 func main() { var x MyInt = 7 v := reflect.ValueOf(x) t := reflect.TypeOf(x) fmt.Println(\u0026#34;type:\u0026#34;, t) // type: main.MyInt  fmt.Println(\u0026#34;value:\u0026#34;, v) // value: 7  fmt.Println(\u0026#34;kind:\u0026#34;, v.Kind()) // kind: int32  fmt.Println(\u0026#34;type:\u0026#34;, v.Type()) // type: main.MyInt  x = MyInt(int32(v.Int)) // v.Int returns a int64 } reflect.Value 的 Type 返回的是静态类型 MyInt，而 kind() 方法返回的是底层类型 int32；为了保持 API 简单，value 的 Setter 和 Getter 类型方法操作，是包含某个值的最大类型，v.Int() 返回的是 int64，必要时转化成实际类型。\n2.2 Reflection goes from reflection object to interface value #  从反射对象可以获取 interface{} 变量；\ntype MyInt int32 func main() { var x MyInt = 7 v := reflect.ValueOf(x) y := v.Interface().(int32) fmt.Println(y) // 7 } 对于一个 reflect.Value，可以用 Interface() 方法恢复成一个接口值，效果就是包类型和值打包成接口，并返回结果。\n2.3 To modify a reflection object, the value must be settable #  要修改反射对象，其值必须可设置；\nfunc main() { var x float64 = 3.4 v := reflect.ValueOf(x) // panic: reflect: reflect.flag.mustBeAssignable using addressable value  v.SetFloat(7.1) p := reflect.ValueOf(\u0026amp;X) // panic: reflect: reflect.flag.mustBeAssignable using addressable value  p.SetFloat(7.1) e := reflect.ValueOf(\u0026amp;X).Elem() // OK  e.SetFloat(7.1) } 如果我们想通过反射来修改变量 x，我们必须把我们想要修改的值的指针传给一个反射库。Go 语言的函数调用都是值传递的，所以我们只能先获取指针对应的 reflect.Value，再通过 reflect.Value.Elem 方法迂回的方式得到可以被设置的变量。我们通过如下所示的代码理解这个过程：\nfunc main() { i := 1 v := \u0026amp;i *v = 10 } 如果不能直接操作 i 变量修改其持有的值，我们就只能获取 i 变量所在地址并使用 *v 修改所在地址中存储的整数。\n3. 相关资料 #    Go 语言中反射包的实现原理  4.3 反射  "});index.add({'id':19,'href':'/docs/golang/data-structure/slice/','title':"slice",'section':"数据结构",'content':"1. 数据结构 #  type slice struct { array unsafe.Pointer len int cap int }  slice 的底层数据结构中的 array 是一个指针，指向的是一个 Array len 代表这个 slice 的元素个数 cap 表示 slice 指向的底层数组容量  对 slice 的赋值，以值作为函数参数时，只拷贝 1 个指针和 2 个 int 值。\n2. 操作 #  2.1 创建 #   var []T 或 []T{} func make([]T,len,cap) []T  2.2 nil 切片和空切片 #   nil 切片被用在很多标准库和内置函数中，描述一个不存在的切片的时候，就需要用到 nil 切片。比如函数在发生异常的时候，返回的切片就是 nil 切片。nil 切片的指针指向 nil. 空切片一般会用来表示一个空集合。比如数据库查询，一条结果也没有查到，那么就可以返回一个空切片。  2.3 扩容 #  2.3.1 计算策略 #   若 Slice cap 大于 doublecap，则扩容后容量大小为 新 Slice 的容量（超了基准值，我就只给你需要的容量大小） 若 Slice len 小于 1024 个，在扩容时，增长因子为 1（也就是 3 个变 6 个） 若 Slice len 大于 1024 个，在扩容时，增长因子为 0.25（原本容量的四分之一）  2.3.2 内存策略 #   翻新扩展：当前元素为 kindNoPointers，也就是非指针类型，将在老 Slice cap 的地址后继续申请空间用于扩容 举家搬迁：重新申请一块内存地址，整体迁移并扩容  2.4 拷贝 #  slicecopy() 方法会把源切片值（即 from Slice ) 中的元素复制到目标切片（即 to Slice ) 中，并返回被复制的元素个数，copy 的两个类型必须一致。slicecopy() 方法最终的复制结果取决于较短的那个切片，当较短的切片复制完成，整个复制过程就全部完成了。\n3. 特性 #  slice 的 array 存储在连续内存上，因此具有以下特点：\n 随机访问很快，适合下标访问，缓存命中率很高； 动态扩容会涉及内存拷贝和开辟新内存，会带来 gc 压力，内存碎片化； 如果可预估使用空间，提前分配 cap 的大小是极好的； 新、老 slice 共用底层数组，对底层数组的更改都会影响到彼此； append 可以掰断新老 slice 共用底层数组的关系；  4. 参考资料 #    深入解析 Go 中 Slice 底层实现  深入解析 Go Slice  "});index.add({'id':20,'href':'/docs/golang/memory-manage/memory-model/','title':"内存模型",'section':"内存管理",'content':"1. 引言 #  Go 语言的内存模型规定了一个 goroutine 可以看到另外一个 goroutine 修改同一个变量的值的条件，这类似 java 内存模型中内存可见性问题。当多个 goroutine 并发同时存取同一个数据时候必须把并发的存取的操作顺序化，在 go 中可以实现操作顺序化的工具有高级的通道（channel）通信和同步原语比如 sync 包中的互斥锁（Mutex）、读写锁（RWMutex）或者和 sync/atomic 中的原子操作。\n2. 设计原则 #  2.1 happens-before #  假设 A 和 B 表示一个多线程的程序执行的两个操作。如果 A happens-before B，那么 A 操作对内存的影响将对执行 B 的线程（且执行 B 之前）可见。\n单一 goroutine 中当满足下面条件时候，对一个变量的写操作 w1 对读操作 r1 可见：\n 读操作 r1 不是发生在写操作 w1 前 在读操作 r1 之前，写操作 w1 之后没有其他的写操作 w2 对变量进行了修改  多 goroutine 下则需要满足下面条件才能保证写操作 w1 对读操作 r1 可见：\n 写操作 w1 先于读操作 r1 任何对变量的写操作 w2 要先于写操作 w1 或者晚于读操作 r1  关于 channel 的 happens-before 在 Go 的内存模型中提到了三种情况：\n 带缓冲的 channel 的发送操作 happens-before 相应 channel 的接收操作完成 不带缓冲的 channel 的接收操作 happens-before 相应 channel 的发送操作完成 关闭一个 channel happens-before 从该 channel 接收到最后的返回值 0  2.2 Synchronization #  2.2.1 初始化 #  如果在一个 goroutine 所在的源码包 p 里面通过 import 命令导入了包 q，那么 q 包里面 go 文件的初始化方法的执行会 happens before 于包 p 里面的初始化方法执行。\n2.2.2 创建 goroutine #  go 语句启动一个新的 goroutine 的动作 happen before 该新 goroutine 的运行。\n2.2.3 销毁 goroutine #  一个 goroutine 的销毁操作并不能确保 happen before 程序中的任何事件。\n2.2.4 通道通信 #  在 go 中通道是用来解决多个 goroutines 之间进行同步的主要措施，在多个 goroutines 中，每个对通道进行写操作的 goroutine 都对应着一个从通道读操作的 goroutine。\n 在有缓冲的通道时候向通道写入一个数据总是 happen before 这个数据被从通道中读取完成。 对应无缓冲的通道来说从通道接受（获取叫做读取）元素 happen before 向通道发送（写入）数据完成。 从容量为 C 的通道接受第 K 个元素 happen before 向通道第 k+C 次写入完成，比如从容量为 1 的通道接受第 3 个元素 happen before 向通道第 3+1 次写入完成。  2.3 Locks #   对应任何 sync.Mutex 或 sync.RWMutex 类型的变量 I 来说，调用 n 次 l.Unlock() 操作 happen before 调用 m 次 l.Lock() 操作返回，其中 n\u0026lt;m。 对任何一个 sync.RWMutex 类型的变量 l 来说，存在一个次数 n，调用 l.RLock()（读锁）操作 happens after 调用 n 次 l.Unlock()（释放写锁）并且相应的 l.RUnlock()（释放读锁） happen before 调用 n+1 次 l.Lock()（写锁）。  2.4 Once #  多 goroutine 下同时调用 once.Do(f) 时，真正执行 f() 函数的 goroutine， happen before 任何其他由于调用 once.Do(f) 而被阻塞的 goroutine 返回。\n3. 参考资料 #    Golang 内存模型  The Go Memory Model  "});})();