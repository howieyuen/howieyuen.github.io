<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIG-Node on 学习笔记</title>
    <link>https://howieyuen.github.io/docs/kubernetes/sig-node/</link>
    <description>Recent content in SIG-Node on 学习笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://howieyuen.github.io/docs/kubernetes/sig-node/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>kubelet 拓扑管理器</title>
      <link>https://howieyuen.github.io/docs/kubernetes/sig-node/topology-manager/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://howieyuen.github.io/docs/kubernetes/sig-node/topology-manager/</guid>
      <description>注：本文翻译自Node Topology Manager
 1. 概要 越来越多的系统将CPU和硬件加速器组合使用，以支撑高延迟和搞吞吐量的并行计算。包括电信、科学计算、机器学习、金融服务和数据分析等领域的工作。这种的混血儿构成了一个高性能环境。
为了达到最优性能，需要对CPU隔离、内存和设备的物理位置进行优化。然而，在kubernetes中，这些优化没有一个统一的组件管理。
本次建议提供一个新机制，可以协同kubernetes各个组件，对硬件资源的分配可以有不同的细粒度。
2. 启发 当前，kubelet中多个组件决定系统拓扑相关的分配：
 CPU 管理器  CPU管理器限制容器可以使用的CPU。该能力，在1.8只实现了一种策略——静态分配。该策略不支持在容器的生命周期内，动态上下线CPU。   设备管理器  设备管理器将某个具体的设备分配给有该设备需求的容器。设备通常是在外围互连线上。如果设备管理器和CPU管理器策略不一致，那么CPU和设备之间的所有通信都可能导致处理器互连结构上的额外跳转。   容器运行时（CNI）  网络接口控制器，包括SR-IOV虚拟功能（VF）和套接字有亲和关系，socket不同，性能不同。    相关问题：
 节点层级的硬件拓扑感知（包括NUMA） 发现节点的NUMA架构 绑定特定CPU支持虚拟函数 提议：CPU亲和与NUMA拓扑感知  注意，以上所有的关注点都只适用于多套接字系统。内核能从底层硬件接收精确的拓扑信息（通常是通过SLIT表），是正确操作的前提。更多信息请参考ACPI规范的5.2.16和5.2.17节。
2.1 目标  根据CPU管理器和设备管理器的输入，给容器选择最优的NUMA亲和节点。 集成kubelet中其他支持拓扑感知的组件，提供一个统一的内部接口。  2.2 非目标  设备间连接：根据直接设备互连来决定设备分配。此问题不同于套接字局部性。设备间的拓扑关系，可以都在设备管理器中考虑，可以做到套接字的亲和性。实现这一策略，可以从逐渐支持任何设备之间的拓扑关系。 大页：本次提议有2个前提，一是集群中的节点已经预分配了大页；二是操作系统能给容器做好本地页分配（只需要本地内存节点上有空闲的大页即可） 容器网络接口：本次提议不包含修改CNI。但是，如果CNI后续支持拓扑管理，此次提出的方案应该具有良好的扩展性，以适配网络接口的局部性。对于特殊的网络需求，可以使用设备插件API作为临时方案，以减少网络接口的局限性。  2.3 用户故事 故事1: 快速虚拟化的网络功能
要求在一个首选的NUMA节点上，既要“网络快”，又能自动完成各个组件（大页，cpu集，网络设备）的协同。在大多数场景下，只有极少数的NUMA节点才能满足。
故事2: 加速神经网络训练
NUMA节点中的已分配的CPU和设备，可满足神经网络训练的加速器和独占的CPU的需求，以达到性能最优。
3. 提议 主要思想：两相拓扑一致性协议 拓扑亲和性在容器级别，与设备和CPU的亲和类似。在Pod准入期间，一个新组件可以从设备管理器和CPU管理器收集pod中每个容器的配置，这个组件名为拓扑管理器。当这些组件进行资源分配时，拓扑管理器扮演本地对齐的预分配角色。我们希望各个组件能利用pod中隐含的QoS类型进行优先级排序，以满足局部重要性最优。
3.1 提议修改点 3.1.1 新概念：拓扑管理器 这个提议主要关注kubelet中一个新组件，叫做拓扑管理器。拓扑管理器实现了Pod的Admit()接口，并参与kubelet的对pod的准入。当Admit()方法被调用，拓扑管理器根据kubelet标志，逐个pod或逐个容器收集kubelet其他组件的的拓扑信息。
如果提示不兼容，拓扑管理器可以选择拒绝pod，这是由kubelet配置的拓扑策略所决定的。拓扑管理器支持4中策略：none（默认）、best-erffort、restricted和single-numa-node。</description>
    </item>
    
    <item>
      <title>kubelet 驱逐管理器</title>
      <link>https://howieyuen.github.io/docs/kubernetes/sig-node/kubelet-eviction-manager/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://howieyuen.github.io/docs/kubernetes/sig-node/kubelet-eviction-manager/</guid>
      <description>1、概述 在可用计算资源较少时，kubelet为保证节点稳定性，会主动地结束一个或多个pod以回收短缺地资源，这在处理内存和磁盘这种不可压缩资源时，驱逐pod回收资源的策略，显得尤为重要。下面来具体研究下Kubelet Eviction Policy的工作机制。
 kubelet预先监控本节点的资源使用，防止资源被耗尽，保证节点稳定性。 kubelet会预先Fail N(&amp;gt;=1)个Pod，以回收出现紧缺的资源。 kubelet在Fail一个pod时，kill掉pod内所有container，并设置pod.status.phase = Failed。 kubelet按照事先设定好的Eviction Threshold来触发驱逐动作，实现资源回收。  1.1 驱逐信号 在源码pkg/kubelet/eviction/api/types.go中定义了以下及几种Eviction Signals：
   Eviction Signal Description     memory.available := node.status.capacity[memory] - node.stats.memory.workingSet   nodefs.available := node.stats.fs.available   nodefs.inodesFree := node.stats.fs.inodesFree   imagefs.available := node.stats.runtime.imagefs.available   imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree   allocatableMemory.available := pod.allocatable - pod.workingSet   pid.available := node.MaxPID - node.NumOfRunningProcesses    上表主要涉及三个方面，memory、file system和pid。其中kubelet值支持2种文件系统分区：</description>
    </item>
    
  </channel>
</rss>
