<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sig-node on 袁昊的学习笔记</title>
    <link>https://howieyuen.github.io/docs/kubernetes/sig-node/</link>
    <description>Recent content in sig-node on 袁昊的学习笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://howieyuen.github.io/docs/kubernetes/sig-node/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>kubelet topology manager</title>
      <link>https://howieyuen.github.io/docs/kubernetes/sig-node/topology-manager/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://howieyuen.github.io/docs/kubernetes/sig-node/topology-manager/</guid>
      <description>注：本文翻译自 Node Topology Manager
 1. 概要 #  越来越多的系统将 CPU 和硬件加速器组合使用，以支撑高延迟和搞吞吐量的并行计算。 包括电信、科学计算、机器学习、金融服务和数据分析等领域的工作。这种的混血儿构成了一个高性能环境。
为了达到最优性能，需要对 CPU 隔离、内存和设备的物理位置进行优化。 然而，在 kubernetes 中，这些优化没有一个统一的组件管理。
本次建议提供一个新机制，可以协同kubernetes各个组件，对硬件资源的分配可以有不同的细粒度。
2. 启发 #  当前，kubelet 中多个组件决定系统拓扑相关的分配：
 CPU 管理器  CPU 管理器限制容器可以使用的 CPU。该能力，在 1.8 只实现了一种策略——静态分配。 该策略不支持在容器的生命周期内，动态上下线 CPU。   设备管理器  设备管理器将某个具体的设备分配给有该设备需求的容器。设备通常是在外围互连线上。 如果设备管理器和 CPU 管理器策略不一致，那么CPU和设备之间的所有通信都可能导致处理器互连结构上的额外跳转。   容器运行时（CNI）  网络接口控制器，包括 SR-IOV 虚拟功能（VF）和套接字有亲和关系，socket 不同，性能不同。    相关问题：
  节点层级的硬件拓扑感知（包括 NUMA）  发现节点的 NUMA 架构  绑定特定 CPU 支持虚拟函数  提议：CPU 亲和与 NUMA 拓扑感知  注意，以上所有的关注点都只适用于多套接字系统。 内核能从底层硬件接收精确的拓扑信息（通常是通过 SLIT 表），是正确操作的前提。 更多信息请参考ACPI规范的 5.</description>
    </item>
    
    <item>
      <title>kubelet eviction manager</title>
      <link>https://howieyuen.github.io/docs/kubernetes/sig-node/kubelet-eviction-manager/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://howieyuen.github.io/docs/kubernetes/sig-node/kubelet-eviction-manager/</guid>
      <description>1、概述 #  在可用计算资源较少时，kubelet 为保证节点稳定性，会主动地结束一个或多个 pod 以回收短缺地资源， 这在处理内存和磁盘这种不可压缩资源时，驱逐 pod 回收资源的策略，显得尤为重要。 下面来具体研究下 Kubelet Eviction Policy 的工作机制。
 kubelet 预先监控本节点的资源使用，防止资源被耗尽，保证节点稳定性。 kubelet 会预先 Fail N(&amp;gt;=1)个Pod，以回收出现紧缺的资源。 kubelet 在 Fail一个 pod 时，kill掉pod内所有 container，并设置 pod.status.phase = Failed。 kubelet 按照事先设定好的 Eviction Threshold 来触发驱逐动作，实现资源回收。  1.1 驱逐信号 #  在源码 pkg/kubelet/eviction/api/types.go 中定义了以下及几种 Eviction Signals：
   Eviction Signal Description     memory.available := node.status.capacity[memory] - node.stats.memory.workingSet   nodefs.available := node.stats.fs.available   nodefs.inodesFree := node.</description>
    </item>
    
  </channel>
</rss>
