[{"id":0,"href":"/docs/translation/XaC/how-to-manage-gitops-secrets-a-detailed-guide/","title":"如何管理 GitOps Secret：详细指南","section":"X as Code","content":" 原文链接： How to Manage GitOps Secrets: A Detailed Guide\nGitOps 正变得越来越流行。越来越多的公司开始使用 Git 作为其基础设施和应用程序配置的真实来源。然而，伴随其优势而来的是挑战。例如，如果你的所有配置都存储在 Git 中，你如何管理 Secret？你不能简单地将密码和令牌以明文形式提交到 Git 存储库，即使该存储库是私有的并且只有少数人可以访问它。在这篇文章中，你将学习如何安全地管理 GitOps Secret 。敬请关注。\nGitOps vs Secret # 如果你以前从未使用过 GitOps，这里有一个简短的介绍。GitOps 是一种纯粹通过 Git 存储库以声明方式管理基础设施和应用程序配置的方法。它的工作原理如下：你将所有配置存储在 Git 中，然后在某处安装一个 GitOps 工具，该工具会持续监控对该 Git 存储库的更改，并在检测到存储库中发生更改时应用基础架构和应用程序更改。GitOps 的全部意义在于，你的所有基础架构和应用程序配置都拥有一个集中的、单一的事实点。GitOps 最常与 Kubernetes 一起使用。\n但正如本文开头所述，使用 GitOps 时存在一些挑战。而最大的一个是 Secret 管理。你的基础设施将需要许多 Secret。你的应用程序配置可能也充满了 Secret。不用说，以纯文本形式将 Secret 存储在 Git 存储库中是一个安全漏洞。即使该存储库是私有的也是如此。你需要一个不同的解决方案，但理想情况下仍然以 GitOps 方式工作的东西。这意味着最好不要有一个单独的过程来定义 Secret。我会告诉你如何做到这一点。\nGitOps 方式的 Secret # 有两种流行的方法可以解决这个问题。它们的工作方式完全不同，但都实现了相同的结果：将 Secret 或其引用存储在 Git 存储库中的能力。你选择哪一种将取决于你的需求。我们来聊聊此二者。\nSealedSecrets # 我们已经确定你不能在 Git 存储库中以纯文本形式存储 Secret 。但是如何将它们存储在非纯文本版本中呢？这正是 SealedSecrets 工具所做的。它允许你加密你的 Secret，并且只将它们的加密版本存储在你的 Git 存储库中。就那么简单。\n你问 SealedSecrets 是如何工作的？你在 Kubernetes 集群上安装 SealedSecrets Controller，在本地机器上安装 kubeseal 二进制文件。SealedSecrets 将生成用于加密 Secret 的私钥和公钥。在将密钥提交到 Git 存储库之前，你将使用 kubeseal 二进制文件对其进行加密。然后，以加密形式将其存储在存储库中是完全安全的，只有运行在 Kubernetes 集群中的 SealedSecrets Controller 才能解密。\n如何使用 SealedSecrets # 首先，按照此处的 SealedSecrets 安装说明进行操作。 启动并运行它后，你可以尝试使用 kubeseal 密封你的第一个 Secret。让我们创建一个简单的 Kubernetes Secret 定义 YAML 文件并使用 kubeseal 对其进行密封。\napiVersion: v1 kind: Secret metadata: name: example-secret type: Opaque data: username: my-username password: super-secret-password 获得文件后，你可以将其内容传给 kubeseal：\ncat secret.yaml| kubeseal --controller-name=sealed-secrets-controller --format yaml \u0026gt; sealed-secret.yaml 如果你现在查看创建的 seal-secret.yaml 文件，你会看到实际的用户名和密码已加密。\napiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: example-secret namespace: default spec: encryptedData: password: AgC7jlVk(...)eb+XOk5/99fKHk= username: AgAHbCU7(...)hIgv5D6LDYopF4n template: data: null metadata: creationTimestamp: null name: example-secret namespace: default type: Opaque 该文件现在可以安全地存储在 Git 存储库中，因为只有用于加密该文件的 SealedSecrets Controller 才能解密它。\n但是你如何在你的集群中使用这个 Secret 呢？这很简单。你可以直接将该密封文件应用到你的集群，运行在其上的 SealedSecrets 控制器将自动解封它并从中创建一个标准的 Kubernetes Secret 资源。\n$ kubectl apply -f sealed-secret.yaml sealedsecret.bitnami.com/example-secret created $ kubectl get secret NAME TYPE DATA AGE example-secret Opaque 2 7s 从现在开始，你可以像使用任何标准 Kubernetes 密钥一样使用 example-secret。\nExternalSecrets # 为你的 GitOps 需求存储 Secret 的另一种方法是使用 ExternalSecrets。它的工作方式与 SealedSecrets 不同，但也解决了在 Git 存储库中存储纯文本 Secret 的问题。ExternalSecrets 通过消除将实际 Secret 存储在存储库中的需要来做到这一点。相反，你的 Secret 可以安全地存储在 Secret 保险库中，你只需要将对 Secret 的引用存储在你的存储库中。\n因此，例如，在 Git 中的文件中没有实际的用户名和密码，而是有一个文件，上面写着“这个用户名是密码存储在此 key 下的 Secret 保险库中”。然后，External Secret Operator 的工作就是在你需要时为你获取实际值。\n使用 ESO（External Secret Operator） # 可以像使用 Helm 的任何其他工具一样安装 External Secret Operator。你可以按照此处的安装和初始配置步骤进行操作。启动并运行 ExternalSecrets 后，使用它就非常简单。你首先需要将你的 Secret 添加到要使用的 Secret 保险库中，然后创建一个 ExternalSecrets 引用文件。此文件将替代你的典型 Kubernetes Secret 定义文件。\n如前所述，使用 ESO 意味着从外部 Secret 保险库中引用实际 Secret 。因此，你创建了一个外部 Secret 资源，ESO 将从后台的外部保险库中获取实际 Secret，并为你创建一个实际的 Kubernetes Secret。这是一个例子：\napiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: database-externalsecret spec: refreshInterval: 3h secretStoreRef: name: azure-keyvault kind: SecretStore target: name: database-secret creationPolicy: Owner data: - secretKey: database-secret-dev remoteRef: key: database-secret-dev 这是一个 ExternalSecrets 定义文件，它告诉 ESO 从 Azure Key Vault 获取 database-secret-dev 密钥的值，并从中创建一个名为 database-secret 的 Kubernetes 密钥。如你所见，我们在此文件中没有实际的 Secret 值，因此将其存储在 Git 存储库中非常好。\n在使用 Secret 方面也是如此。你只需将该 ExternalSecrets 定义文件应用到你的集群，ESO 就会自动从定义的 Secret 保险库中获取 Secret 并从中创建一个实际的 Kubernetes Secret 。\n$ kubectl apply -f external-secret.yaml externalsecret.external-secrets.io/database-externalsecret created $ kubectl get secret NAME TYPE DATA AGE example-secret Opaque 2 12m database-secret Opaque 2 4s 总结 # 如你所见，GitOps Secret 问题可以得到解决。这甚至没有那么困难。但是，它确实包含一些额外的步骤和工具。但是一旦完成初始设置，在 GitOps 实践中安全地管理 Secret 就不需要其他的日常工作。\n如果你想了解更多有关 Secret 或 GitOps 的信息，可以在 我们的博客上找到更多内容。\n"},{"id":1,"href":"/docs/translation/protobuf/overview-of-protocal-buffers/","title":"概述","section":"Protocol Buffers","content":" 原文链接： Overview | Protocol Buffers | Google Developers\n协议缓冲区（Protocol Buffers，protobuf）提供了一种语言中立、平台中立、可扩展的机制，用来序列化结构化数据，并且支持向前/向后兼容。 它类似于 JSON，只是它更小更快，并且能生成本地语言绑定。\nprotobuf 包含以下模块：\n定义语言（在 .proto 文件中创建） 连接数据的代码（ proto 编译器生成） 特定语言的运行时库 序列化格式的数据（写入文件或者通过网络传输） protobuf 解决了哪些问题？ # protobuf 为大小高达几 MB 的类型化、结构化数据包提供了一种序列化格式。 该格式既适用于临时网络流量，又适用于长期数据存储。 可以使用新信息扩展 protobuf ，而无需废弃当前数据或更新代码。\nprotobuf 是 Google 最常用的数据格式。 它们广泛用于服务器间通信以及磁盘上数据的归档存储。 Protocol buffer 的 message 和 service 由工程师编写的 .proto 文件描述。 下面是一个 message 示例：\nmessage Person { optional string name = 1; optional int32 id = 2; optional string email = 3; } proto 编译器在构建 .proto 文件时，生成各种编程语言的代码 （在后文的跨语言兼容性中有说明）来操作相应的 protobuf 。 每个生成的类都包含每个字段和方法的简单访问器，用于序列化和解析整个结构与原始字节之间的关系。 下面向你展示了一个使用这些生成方法的示例：\nPerson john = Person.newBuilder() .setId(1234) .setName(\u0026#34;John Doe\u0026#34;) .setEmail(\u0026#34;jdoe@example.com\u0026#34;) .build(); output = new FileOutputStream(args[0]); john.writeTo(output); 由于 protobuf 在 Google 的各种服务中广泛使用，并且其中的数据可能会保留一段时间，因此保持向后兼容性至关重要。 protobuf 允许无缝支持对任何 protobuf 的更改，包括添加新字段和删除现有字段，而不会破坏现有服务。 有关此话题的更多信息，请参阅后文的在不更新代码的情况下更新原型定义。\nprotobuf 带来的好处是什么？ # protobuf 非常适合需要以语言中立、平台中立并且可扩展的方式去序列化结构化、类似记录、类型化数据的情况。 它们最常用于定义通信协议（与 gRPC 一起）和数据存储。\n使用 protobuf 的优势包括：\n紧凑的数据存储 快速解析 多编程语言的可用性 通过自动生成类的优化功能 跨语言兼容性 # 以任何受支持的编程语言编写的代码都可以读取相同的消息。 你可以让一个平台上的 Java 程序从一个软件系统捕获数据，根据 .proto 定义对其进行序列化， 然后在另一个平台上运行的单独 Python 应用程序中从序列化数据中提取特定值。\nprotobuf 编译器（protoc）直接支持以下语言：\nC++ C# Java Kotlin Objective-C PHP Python Ruby Google 支持以下语言，但项目的源代码位于 GitHub 存储库中。 protoc 编译器使用这些语言的插件：\nDart Go 其他语言不直接由 Google 支持，而是由其他 GitHub 项目支持。 这些语言包含在 Protocol Buffers 的第三方插件中。\n跨项目支持 # 你可以通过在驻留在特定项目代码库之外的 .proto 文件中定义 message 类型来跨项目使用 protobuf 。 如果你正在定义预计在你的直接团队之外广泛使用的 message 类型或枚举，你可以将它们放在自己的文件中，而无需依赖。\n在 Google 中广泛使用的几个原型定义示例是 timestamp.proto 和 status.proto。\n在不更新代码的情况下更新原型定义 # 软件产品向后兼容是标准，但向前兼容却不太常见。 只要你在更新 .proto 定义时遵循一些 简单的做法， 旧代码将毫无问题地读取新消息，而忽略任何新添加的字段。 对于旧代码，已删除的字段将具有默认值，已删除的重复字段将为空。 有关什么是“重复”字段的信息，请后续的 protobuf 定义语法小节。\n新代码也将透明地读取旧消息。旧消息中不会出现新字段； 在这些情况下， protobuf 提供了一个合理的默认值。\n什么时候 protobuf 不适合？ # protobuf 并不适合所有数据。尤其：\nprotobuf 倾向于假设整个消息可以一次加载到内存中并且不大于对象图。 对于超过几兆字节的数据，考虑不同的解决方案； 在处理较大的数据时，由于序列化副本，你可能会有效地获得多个数据副本， 这可能会导致内存使用量出现惊人的峰值。 当 protobuf 被序列化时，相同的数据可以有许多不同的二进制序列化。 如果不完全解析它们，就无法比较两条消息是否等同。 消息未压缩。虽然消息可以像任何其他文件一样被压缩或 gzip 压缩， 但 JPEG 和 PNG 使用的专用压缩算法将为适当类型的数据生成更小的文件。 对于许多涉及大型多维浮点数数组的科学和工程用途， protobuf 消息在大小和速度方面都没有达到最大效率。 对于这些应用程序， FITS 和类似格式的开销较小。 protobuf 在科学计算中流行的非面向对象语言（例如 Fortran 和 IDL）中没有得到很好的支持。 protobuf 消息本身并不自我描述其数据，但它们具有完全反射的模式，你可以使用它来实现自我描述。 也就是说，如果不访问其相应的 .proto 文件，你将无法完全解释它。 protobuf 不是任何组织的正式标准。这使得它们不适合在具有法律或其他要求以建立在标准之上的环境中使用。 谁使用 Protocol Buffers？ # 许多外部可用的项目使用 protobuf ，包括以下内容：\ngRPC Google Cloud Envoy Proxy protobuf 如何工作？ # 下图显示了如何使用 protobuf 来处理数据：\nprotobuf 生成的代码提供了实用方法来从文件和流中检索数据、从数据中提取单个值、检查数据是否存在、将数据序列化回文件或流以及其他有用的功能。\n以下代码示例向你展示了此流程在 Java 中的示例。如前所示，这是一个 .proto 定义：\nmessage Person { optional string name = 1; optional int32 id = 2; optional string email = 3; } 编译此 .proto 文件会创建一个 Builder 类，你可以使用它来创建新实例，如以下 Java 代码所示：\nPerson john = Person.newBuilder() .setId(1234) .setName(\u0026#34;John Doe\u0026#34;) .setEmail(\u0026#34;jdoe@example.com\u0026#34;) .build(); output = new FileOutputStream(args[0]); john.writeTo(output); 然后，你可以使用 protobuf 在其他语言（如 C++）中创建的方法反序列化数据：\nPerson john; fstream input(argv[1], ios::in | ios::binary); john.ParseFromIstream(\u0026amp;input); int id = john.id(); std::string name = john.name(); std::string email = john.email(); protobuf 定义语法 # 定义 .proto 文件时，你可以指定字段是 optional 或 repeated（proto2 和 proto3）或 singular（proto3）。 （proto3 中不存在将字段设置为 required 的情况，并且在 proto2 中强烈反对。 有关此的更多信息，请参阅 指定字段规则 中的 \u0026ldquo;Required is Forever\u0026rdquo;。）\n在设置字段的可选性/可重复性后，你指定数据类型。 protobuf 支持通常的原始数据类型，例如整数、布尔值和浮点数。 有关完整列表，请参阅 标量值类型。\n一个字段也可以是：\nmessage 类型，你可以嵌套部分定义，例如用于重复数据集。 enum 类型，你可以指定一组值以供选择。 oneof 类型，当消息有多个可选字段且最多同时设置一个字段时，可以使用该类型。 map 类型，用于将键值定义。 在 proto2 中，消息可以允许扩展定义消息本身之外的字段。 例如，protobuf 库的内部消息模式允许扩展自定义的、特定于使用的选项。\n有关可用选项的更多信息，请参阅 proto2 或 proto3 的语言指南。\n设置可选属性和字段类型后，你应该分配一个字段编号。 字段编号不能改变或重复。 如果你删除一个字段，你应该保留其字段编号，以防止有人意外重复该编号。\n额外的数据类型支持 # protobuf 支持许多标量值类型，包括使用可变长度编码和固定大小的整数。 你还可以通过定义消息来创建自己的复合数据类型，这些消息本身就是可以分配给字段的数据类型。 除了简单和复合值类型之外，还发布了几种常见类型。\n常见类型 # Duration 是有符号的、固定长度的时间跨度，例如 42s。 Timestamp 是独立于任何时区或日历的时间点，例如 2017-01-15T01:30:15.01Z。 Interval 是独立于时区或日历的时间间隔，例如 2017-01-15T01:30:15.01Z - 2017-01-16T02:30:15.01Z。 Date 是一个完整的日历日期，例如 2025-09-19。 DayOfWeek 是一周中的某一天，例如 Monday。 TimeOfDay 是一天中的某个时间，例如 10:42:23。 LatLng 是一个纬度/经度对，例如 37.386051 纬度和 -122.083855 经度。 Money 是具有货币类型的货币数量，例如 42 USD。 PostalAddress 是一个邮政地址，例如 1600 Amphitheatre Parkway Mountain View, CA 94043 USA。 Color 是 RGBA 颜色空间中的一种颜色。 Month 是一年中的某个月，例如 April（四月）。 protobuf 开源哲学 # protobuf 于 2008 年开源，旨在为 Google 以外的开发人员提供与我们在内部从中获得的相同好处的一种方式。 我们通过定期更新语言来支持开源社区，因为我们进行这些更改以支持我们的内部需求。 虽然我们接受来自外部开发人员的精选拉取请求，但我们不能总是优先考虑不符合 Google 特定需求的功能请求和错误修复。\n"},{"id":2,"href":"/docs/translation/protobuf/language-guideproto3/","title":"语言指南（proto3）","section":"Protocol Buffers","content":" 原文链接： Language Guide(proto3) | Protocol Buffers | Google Developers\n本指南描述了如何使用 protobuf 语言结构化你的协议缓冲区数据， 包括 .proto 文件语法和如何从你的 .proto 文件生成数据访问的类（Class）。 它覆盖了 protobuf 语言的 proto3 版本：对于 proto2 语言的信息， 请查阅 Proto2 Language Guide。\n这是一篇参考指南——有关使用本文档中描述的许多功能的分步示例， 请参阅你选择的语言的 教程（目前仅有 proto2；更多 proto3 文档即将推出）。\n定义消息类型 # 首先让我们看一个非常简单的例子。假设你要定义搜索请求消息格式， 其中每个搜索请求都有一个查询字符串、你感兴趣的特定结果页面以及每页的结果数量。 这是用于定义消息类型的 .proto 文件。\nsyntax = \u0026#34;proto3\u0026#34;; message SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; } 该文件的第一行指定你使用的是 proto3 语法：如果你不这样做，protocol buffer 编译器将假定你使用的是 proto2。 这必须是文件的第一个非空、非注释行。 SearchRequest 消息定义指定了三个字段（名称/值对），每个字段代表你希望此消息中包含的数据。 每个字段都有一个名称和一个类型。 指定字段类型 # 在上面的例子中，所有字段都是 标量类型： 2 个整型（page_number 和 result_per_page）和 1 个字符串类型（query）。 然而，你也可以给你的字段指定复杂的类型，包括 枚举 和其他消息类型。\n分配字段编号 # 如你所见，消息定义中的每个字段都有一个唯一编号。 这些字段编号用于以 消息二进制格式 标识字段，编号一旦使用，你的消息类型就不应更改。 请注意，1 到 15 范围内的字段编号需要一个字节进行编码， 包括字段编号和字段类型（你可以在 protobuf 编码 中找到更多相关信息）。 16 到 2047 范围内的字段编号占用两个字节。 因此，你应该为非常频繁出现的消息元素保留数字 1 到 15。 请记住为将来可能添加的频繁出现的元素留出一些空间。\n你可以指定的最小字段编号是 1，最大的是 2^29 - 1，即 536,870,911。你也不能使用数字 19000 到 19999 （FieldDescriptor::kFirstReservedNumber 到 FieldDescriptor::kLastReservedNumber）， 因为它们是为 protobuf 实现保留的 - 如果你在 .proto 中使用这些保留数字之一，protobuf 编译器会报错。 同样，你不能使用任何以前 保留 的字段编号。\n指定字段规则 # Message 字段可以是以下其一：\n单数：格式良好的消息可以有零个或一个此字段（但不能超过一个）。这是 proto3 语法的默认字段规则。 复数（repeated）：此字段可以在格式良好的消息中重复任意次数（包括零次）。重复值的顺序将被保留。 在 proto3 中，标量数字类型的重复字段默认使用打包编码。 你可以在 Protocol Buffer Encoding 中找到有关打包编码的更多信息。\n添加更多的消息类型 # 可以在单个 .proto 文件中定义多种消息类型。如果你要定义多个相关消息，这很有用——例如，如果你你定义与你的 SearchResponse 消息类型相对应的回复消息格式，你可以将其添加到同一个 .proto 中：\nmessage SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; } message SearchResponse { ... } 添加注释 # 要向 .proto 文件中添加注释，请使用 C/C++ 风格的 // 和 /* ... */ 语法：\n/* SearchRequest represents a search query, with pagination options to * indicate which results to include in the response. */ message SearchRequest { string query = 1; int32 page_number = 2; // Which page number do we want? int32 result_per_page = 3; // Number of results to return per page. } 保留字段 # 如果你通过完全删除某个字段或将其注释掉来更新消息类型，未来的用户可以在对类型进行自己的更新时重新使用该字段编号。 如果他们稍后加载相同 .proto 的旧版本，这可能会导致严重的问题，包括数据损坏、隐私错误等。 确保不会发生这种情况的一种方法是指定保留已删除字段的字段编号（和/或名称，这也可能导致 JSON 序列化问题）。 如果将来有任何用户尝试使用这些字段标识符，protobuf 编译器会报错。\nmessage Foo { reserved 2, 15, 9 to 11; reserved \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;; } 请注意，你不能在同一保留语句里，混合字段名和字段编号。\n你的 .proto 文件生成了什么？ # 当你在 .proto 上运行 protobuf 编译器时，编译器会以你选择的语言生成代码， 你需要使用文件中描述的消息类型，包括获取和设置字段值，将消息序列化为输出流，并从输入流中解析你的消息。\n对于 C++，编译器从每个 .proto 生成一个 .h 和 .cc 文件，并为文件中描述的每种消息类型提供一个类。 对于 Java，编译器会生成一个 .java 文件，其中包含每个消息类型的类，以及用于创建消息类实例的特殊 Builder 类。 对于 Kotlin，除了 Java 生成的代码之外，编译器还会为每种消息类型生成一个 .kt 文件，其中包含可用于简化创建消息实例的 DSL。 Python 有点不同——Python 编译器会生成一个模块，其中包含 .proto 中每种消息类型的静态描述符，然后将其与元类一起用于在运行时创建必要的 Python 数据访问类。 对于 Go，编译器会生成一个 .pb.go 文件，其中包含文件中每种消息类型的类型。 对于 Ruby，编译器会生成一个 .rb 文件，其中包含一个包含消息类型的 Ruby 模块。 对于 Objective-C，编译器从每个 .proto 生成一个 pbobjc.h 和 pbobjc.m 文件，并为文件中描述的每种消息类型提供一个类。 对于 C#，编译器从每个 .proto 生成一个 .cs 文件，其中包含文件中描述的每种消息类型的类。 对于 Dart，编译器会生成一个 .pb.dart 文件，其中包含文件中每种消息类型的类。 你可以按照所选语言的教程（即将推出 proto3 版本）了解有关使用每种语言的 API 的更多信息。 有关更多 API 详细信息，请参阅相关 API 参考（proto3 版本也即将推出）。\n标量值类型 # 标量消息字段可以具有以下类型之一。该表显示了 .proto 文件中指定的类型，以及自动生成的类中的相应类型：\n.proto C++ Java/Kotlin[1] Python[3] Go Ruby C# PHP Dart double double double float float64 Float double float double float float float float float32 Float float float double int32 int32 int int int32 Fixnum or Bignum (as required) int integer int int64 int64 long int/long[4] int64 Bignum long integer/string[6] Int64 uint32 uint32 int[2] int/long[4] uint32 Fixnum or Bignum (as required) uint integer int uint64 uint64 long[2] int/long[4] uint64 Bignum ulong integer/string[6] Int64 sint32 int32 int int int32 Fixnum or Bignum (as required) int integer int sint64 int64 long int/long[4] int64 Bignum long integer/string[6] Int64 fixed32 uint32 int[2] int/long[4] uint32 Fixnum or Bignum (as required) uint integer int fixed64 uint64 long[2] int/long[4] uint64 Bignum ulong integer/string[6] Int64 sfixed32 int32 int int int32 Fixnum or Bignum (as required) int integer int sfixed64 int64 long int/long[4] int64 Bignum long integer/string[6] Int64 bool bool boolean bool bool TrueClass/FalseClass bool boolean bool string string String str/unicode[5] string String (UTF-8) string string String bytes string ByteString str (Python 2) bytes (Python 3) []byte String (ASCII-8BIT) ByteString string 注意：\nint32：使用可变长度编码。对负数进行编码效率低下。如果你的字段可能有负值，请改用 sint32。 int64：使用可变长度编码。对负数进行编码效率低下。如果你的字段可能有负值，请改用 sint64。 uint32：使用可变长度编码。 uint64：使用可变长度编码。 sint32：使用可变长度编码。带符号的 int 值。这些比常规 int32 更有效地编码负数。 sint64：使用可变长度编码。带符号的 int 值。这些比常规 int64 更有效地编码负数。 fixed32：总是四个字节。如果值通常大于 2^28，则比 uint32 更有效。 fixed64：总是八个字节。如果值通常大于 2^56，则比 uint64 更有效。 sfixed32：总是四个字节。 sfixed64：总是八个字节。 string：字符串必须始终包含 UTF-8 编码或 7 位 ASCII 文本，并且不能超过 2^32。 bytes：可以包含不超过 2^32 的任意字节序列。 当你在 protobuf 编码 中序列化你的消息时，你可以了解有关这些类型如何编码的更多信息。\n[1] Kotlin 使用 Java 中的相应类型，甚至是无符号类型，以确保在混合 Java/Kotlin 代码库中的兼容性。\n[2] 在 Java 中，无符号 32 位和 64 位整数使用它们的有符号对应物表示，最高位简单地存储在符号位中。\n[3] 在所有情况下，为字段设置值将执行类型检查以确保其有效。\n[4] 64 位或无符号 32 位整数在解码时始终表示为 long，但如果在设置字段时给出 int，则可以是 int。在所有情况下，该值必须适合设置时表示的类型。见[2]。\n[5] Python 字符串在解码时表示为 unicode，但如果给出 ASCII 字符串，则可以是 str（这可能会发生变化）。\n[6] 整数用于 64 位机器，字符串用于 32 位机器。\n默认值 # 解析消息时，如果编码的消息不包含特定的单个元素，则解析对象中的相应字段将设置为该字段的默认值。这些默认值是特定于类型的：\n对于字符串，默认值为空字符串。 对于字节，默认值为空字节。 对于布尔值，默认值为 false。 对于数字类型，默认值为零。 对于枚举，默认值是首个定义的枚举值，必须为 0。 对于消息字段，未设置该字段。它的确切值取决于语言。有关详细信息，请参阅生成代码指南。 重复字段的默认值为空（通常是相应语言的空列表）。\n请注意，对于标量消息字段，一旦解析了消息，就无法判断一个字段是显式设置为默认值（例如布尔值是否设置为 false）还是根本没有设置：你应该在定义消息类型时请注意。 例如，如果你不希望在默认情况下也发生该行为，则不要在设置为 false 时打开某些行为的布尔值。 另请注意，如果标量消息字段设置为其默认值，则该值将不会在线上序列化。\n有关默认值如何在生成的代码中工作的更多详细信息，请参阅你选择的语言的生成代码指南。\n枚举 # 在定义消息类型时，你可能希望其字段之一仅具有预定义的值列表之一。 例如，假设你要为每个 SearchRequest 添加一个 corpus 字段，其中 corpus 可以是 UNIVERSAL、WEB、IMAGES、LOCAL、NEWS、PRODUCTS 或 VIDEO。 你可以通过在消息定义中添加一个 enum，即可非常简单地做到这一点，每个可能的值都有一个常量。\n在以下示例中，我们添加了一个名为 Corpus 的 enum，其中包含所有可能的值，以及一个 Corpus 类型的字段：\nmessage SearchRequest { string query = 1; int32 page_number = 2; int32 result_per_page = 3; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } Corpus corpus = 4; } 如你所见，Corpus 枚举的第一个常量映射到零：每个枚举定义必须包含一个映射到零的常量作为其第一个元素。这是因为：\n必须有一个零值，以便我们可以使用 0 作为数字默认值。 零值必须是第一个元素，以便与第一个枚举值始终为默认值的 proto2 语义兼容。 你可以通过将相同的值分配给不同的枚举常量来定义别名。为此，你需要将 allow_alias 选项设置为 true，否则协议编译器将在找到别名时生成错误消息。\nmessage MyMessage1 { enum EnumAllowingAlias { option allow_alias = true; UNKNOWN = 0; STARTED = 1; RUNNING = 1; } } message MyMessage2 { enum EnumNotAllowingAlias { UNKNOWN = 0; STARTED = 1; // RUNNING = 1; // 取消注释此行将导致 Google 内部出现编译错误，外部出现警告消息。 } } 枚举器常量必须在 32 位整数范围内。由于 enum 在线路上使用 varint 编码， 因此负数效率低下，因此不推荐使用。你可以在消息定义中定义 enum，如上例所示，也可以在外部定义 enum，这些 enum 可以在 .proto 文件中的任何消息定义中复用。 你还可以使用在一条消息中声明的 enum 类型作为另一条消息中字段的类型，使用语法 _MessageType_._EnumType_。\n当你在使用 enum 的 .proto 上运行协议缓冲区编译器时，生成的代码将具有对应于 Java、Kotlin 或 C++ 的 enum， 或用于 Python 的特殊 EnumDescriptor 类，用于创建一组带整数的符号常量运行时生成的类中的值。\n注意：生成的代码可能会受到特定于语言的枚举数限制（一种语言的低至数千）。请查看你计划使用的语言的限制。 在反序列化期间，无法识别的枚举值将保留在消息中，尽管在反序列化消息时如何表示这取决于语言。 在支持具有超出指定符号范围的值的开放枚举类型的语言中，例如 C++ 和 Go，未知的枚举值简单地存储为其底层整数表示。 在 Java 等具有封闭枚举类型的语言中，枚举中的 case 用于表示无法识别的值，并且可以使用特殊的访问器访问底层整数。 在任何一种情况下，如果消息被序列化，则无法识别的值仍将与消息一起序列化。\n有关如何在应用程序中使用消息 enum 的更多信息，请参阅所选语言的生成代码指南。\n保留值 # 如果你通过完全删除枚举条目或将其注释掉来更新枚举类型，将来的用户可以在对类型进行自己的更新时重用该数值。 如果他们稍后加载相同 .proto 的旧版本，这可能会导致严重的问题，包括数据损坏、隐私错误等。 确保不会发生这种情况的一种方法是指定保留已删除条目的数值（和/或名称，这也可能导致 JSON 序列化问题）。 如果将来有任何用户尝试使用这些标识符，protocol buffer 编译器会抱怨。你可以使用 max 关键字指定保留的数值范围达到最大可能值。\nenum Foo { reserved 2, 15, 9 to 11, 40 to max; reserved \u0026#34;FOO\u0026#34;, \u0026#34;BAR\u0026#34;; } 请注意，你不能在同一保留语句中混合字段名称和数值。\n使用其他消息类型 # 你可以使用其他消息类型作为字段类型。例如，假设你想在每个 SearchResponse 消息中包含 Result 消息。 为此，你可以在同一个 .proto 中定义一个 Result 消息类型，然后在 SearchResponse 中指定一个 Result 类型的字段：\nmessage SearchResponse { repeated Result results = 1; } message Result { string url = 1; string title = 2; repeated string snippets = 3; } 引入定义 # 默认情况下，你只能使用直接导入的 .proto 文件中的定义。但是，有时你可能需要将 .proto 文件移动到新位置。 你可以在旧位置放置一个占位符 .proto 文件，以使用 import public 概念将所有导入转发到新位置，而不是直接移动 .proto 文件并在一次更改中更新所有调用站点。\n请注意，公共导入功能在 Java 中不可用。\n任何导入包含 import public 语句的 proto 的代码都可以传递依赖 import public 依赖项。例如：\n// new.proto // 所有定义被移动到这里 // old.proto // 这是所有客户端正在导入的原型。 import public \u0026#34;new.proto\u0026#34;; import \u0026#34;other.proto\u0026#34;; // client.proto import \u0026#34;old.proto\u0026#34;; // 你使用 old.proto 和 new.proto 中的定义，而不是 other.proto 协议编译器使用 -I/--proto_path 标志在协议编译器命令行上指定的一组目录中搜索导入的文件。 如果没有给出标志，它会在调用编译器的目录中查找。通常，你应该将 --proto_path 标志设置为项目的根目录，并为所有导入使用完全限定名称。\n使用 proto2 消息类型 # 可以导入 proto2 消息类型并在你的 proto3 消息中使用它们，反之亦然。 但是，proto2 枚举不能直接在 proto3 语法中使用（如果导入的 proto2 消息使用它们也没关系）。\n内嵌类型 # 你可以在其他消息类型中定义和使用消息类型，如下例所示，这里 Result 消息在 SearchResponse 消息中定义：\nmessage SearchResponse { message Result { string url = 1; string title = 2; repeated string snippets = 3; } repeated Result results = 1; } 如果你想在其父消息类型之外重用此消息类型，则将其称为 _Parent_._Type_：\nmessage SomeOtherMessage { SearchResponse.Result result = 1; } 你可以随意嵌套消息：\nmessage Outer { // Level 0 message MiddleAA { // Level 1 message Inner { // Level 2 int64 ival = 1; bool booly = 2; } } message MiddleBB { // Level 1 message Inner { // Level 2 int32 ival = 1; bool booly = 2; } } } 更新消息类型 # 如果现有的消息类型不再满足你的所有需求。例如，你希望消息格式有一个额外的字段，但你仍然希望使用使用旧格式创建的代码，请不要担心！ 在不破坏任何现有代码的情况下更新消息类型非常简单。只需记住以下规则：\n不要更改任何现有字段的字段编号。 如果你添加新字段，则使用“旧”消息格式的代码序列化的任何消息仍然可以由新生成的代码解析。 你应该记住这些元素的默认值，以便新代码可以正确地与旧代码生成的消息交互。 类似地，新代码创建的消息可以由旧代码解析：旧二进制文件在解析时会忽略新字段。有关详细信息，请参阅未知字段部分。 只要在更新的消息类型中不再使用字段编号，就可以删除字段。 你可能想要重命名该字段，可能添加前缀“OBSOLETE_”，或保留字段编号，以便你的 .proto 的未来用户不会意外重用该编号。 int32、uint32、int64、uint64 和 bool 都是兼容的——这意味着你可以将字段从其中一种类型更改为另一种类型，而不会破坏前向或后向兼容性。 如果从不适合相应类型的线路中解析出一个数字，你将获得与在 C++ 中将该数字强制转换为该类型相同的效果（例如，如果一个 64 位数字被读取为 int32，它将被截断为 32 位）。 sint32 和 sint64 相互兼容，但与其他整数类型不兼容。 只要字节是有效的 UTF-8，string 和 byte 就兼容。 如果字节包含消息的编码版本，则嵌入消息与 byte 兼容。 fixed32 与 sfixed32 兼容，fixed64 与 sfixed64 兼容。 对于 string、byte 和 消息字段，optional 与 repeated 兼容。给定一个重复字段的序列化数据作为输入， 如果它是一个原始类型字段，那么期望这个字段是 optional 的客户端将采用最后一个输入值， 或者如果它是一个消息类型字段，则合并所有输入元素。 请注意，这对于数字类型（包括布尔值和枚举）通常不安全。 数字类型的重复字段可以以打包格式序列化，当需要 optional 字段时，将无法正确解析。 enum 在有线格式方面与 int32、uint32、int64 和 uint64 兼容（请注意，如果值不合适，将被截断）。 但是请注意，当消息被反序列化时，客户端代码可能会以不同的方式对待它们： 例如，无法识别的 proto3 enum 类型将保留在消息中，但是当消息被反序列化时如何表示则取决于语言。Int 字段总是只保留它们的值。 将单个值更改为新 oneof 的成员是安全且二进制兼容的。 如果你确定没有代码一次设置多个字段，则将多个字段移动到新的 oneof 中可能是安全的。 将任何字段移动到现有的 oneof 中是不安全的。 未知字段 # 未知字段是格式良好的协议缓冲区序列化数据，表示解析器无法识别的字段。 例如，当旧二进制文件用新字段解析新二进制文件发送的数据时，这些新字段将成为旧二进制文件中的未知字段。\n最初，proto3 消息在解析过程中总是丢弃未知字段，但在 3.5 版本中，我们重新引入了保留未知字段以匹配 proto2 行为。 在 3.5 及更高版本中，未知字段在解析期间保留并包含在序列化输出中。\nAny # Any 消息类型允许你将消息用作嵌入类型，而无需定义它们的 .proto。 Any 包含作为 byte 的任意序列化消息，以及充当全局唯一标识符并解析为该消息类型的 URL。 要使用 Any 类型，你需要导入 google/protobuf/any.proto。\nimport \u0026#34;google/protobuf/any.proto\u0026#34;; message ErrorStatus { string message = 1; repeated google.protobuf.Any details = 2; } 给定消息类型的默认类型 URL 是 type.googleapis.com/_packagename_._messagename_。\n不同的语言实现将支持运行时库助手以类型安全的方式打包和解包 Any 值。 例如，在 Java 中，Any 类型将具有特殊的 pack() 和 unpack() 访问器，而在 C++ 中则有 PackFrom() 和 UnpackTo() 方法：\n// 在 Any 中存储任意类型消息 NetworkErrorDetails details = ...; ErrorStatus status; status.add_details()-\u0026gt;PackFrom(details); // 从 Any 中读取任意消息 ErrorStatus status = ...; for (const Any\u0026amp; detail : status.details()) { if (detail.Is\u0026lt;NetworkErrorDetails\u0026gt;()) { NetworkErrorDetails network_error; detail.UnpackTo(\u0026amp;network_error); ... processing network_error ... } } 目前，用于处理 Any 类型的运行时库正在开发中。\n如果你已经熟悉 proto2 语法， Any 可以保存任意 proto3 消息，类似于可以允许扩展的 proto2 消息。\nOneof # 如果你有一条包含多个字段的消息，并且最多同时设置一个字段，你可以强制执行此行为并使用 oneof 功能节省内存。\noneof 字段与常规字段一样，除了一个 oneof 共享内存中的所有字段外，最多可以同时设置一个字段。 设置 oneof 的任何成员会自动清除所有其他成员。你可以使用特殊的 case() 或 WhichOneof() 方法检查 oneof 中设置的值（如果有），具体取决于你选择的语言。\n使用 oneof # 要在 .proto 中定义 oneof，请使用 oneof 关键字，后跟 oneof 名称，在本例中为 test_oneof：\nmessage SampleMessage { oneof test_oneof { string name = 4; SubMessage sub_message = 9; } } 然后，你将 oneof 字段添加到 oneof 定义中。你可以添加任何类型的字段，map 字段和 repeated 字段除外。\n在你生成的代码中，oneof 字段具有与常规字段相同的 getter 和 setter。 你还可以获得一种特殊的方法来检查 oneof 中设置了哪个值（如果有）。 你可以在相关 API 参考中找到有关所选语言的 oneof API 的更多信息。\noneof 功能 # 设置 oneof 字段将自动清除 oneof 的所有其他成员。 因此，如果你设置了多个 oneof 字段，则只有你设置的最后一个字段仍有值。 SampleMessage message; message.set_name(\u0026#34;name\u0026#34;); CHECK(message.has_name()); message.mutable_sub_message(); // 将会清除字段名 CHECK(!message.has_name()); 如果解析器在线路上遇到同一个成员的多个成员，则在解析的消息中只使用最后一个看到的成员。 oneof 字段不能是 repeated。 反射 API 适用于 oneof 字段。 如果将 oneof 字段设置为默认值（例如将 int32 oneof 字段设置为 0），则会设置该 oneof 字段的“大小写”，并且该值将在线上序列化。 如果你使用 C++，请确保你的代码不会导致内存崩溃。 以下示例代码将崩溃，因为 sub_message 已通过调用 set_name() 方法删除。 SampleMessage message; SubMessage* sub_message = message.mutable_sub_message(); message.set_name(\u0026#34;name\u0026#34;); // 将会删除 sub_message sub_message-\u0026gt;set_... // 在此处崩溃 同样在 C++ 中，如果你 Swap() 两个 oneof 消息，则每条消息都会以另一个的 oneof 情况结束。 在下面的示例中，msg1 将有一个 sub_message，而 msg2 将有一个名称。 SampleMessage msg1; msg1.set_name(\u0026#34;name\u0026#34;); SampleMessage msg2; msg2.mutable_sub_message(); msg1.swap(\u0026amp;msg2); CHECK(msg1.has_sub_message()); CHECK(msg2.has_name()); 向后兼容性问题 # 添加或删除其中一个字段时要小心。如果检查 oneof 的值返回 None/NOT_SET，则可能意味着 oneof 尚未设置或已设置为 oneof 不同版本中的字段。 没有办法区分，因为无法知道线路上的未知字段是否是 oneof 的成员。\n标签重用问题\n将字段移入或移出 oneof：在消息被序列化和解析后，你可能会丢失一些信息（某些字段将被清除）。 但是，你可以安全地将单个字段移动到新的 oneof 中，并且如果知道只设置了一个字段，则可以移动多个字段。 删除 oneof 字段并重新添加：这可能会在消息被序列化和解析后清除你当前设置的 oneof 字段。 拆分或合并其中一个：这与移动常规字段有类似的问题。 Map # 如果你想创建关联映射作为数据定义的一部分，protocol buffers 提供了一种方便的快捷语法：\nmap\u0026lt;key_type, value_type\u0026gt; map_field = N; \u0026hellip;其中 key_type 可以是任何整数或字符串类型（因此，除浮点类型和字节之外的任何标量类型）。 请注意，枚举不是有效的 key_type。value_type 可以是除另一个映射之外的任何类型。\n因此，例如，如果你想创建一个项目映射，其中每个项目消息都与一个字符串键相关联，你可以这样定义它：\nmap\u0026lt;string, Project\u0026gt; projects = 3; map 字段不能 repeated。 map 值的线格式排序和 map 迭代排序是未定义的，因此你不能依赖 map 项处于特定顺序。 为 .proto 生成文本格式时，map 按键排序。 数字键按数字排序。 从连线解析或合并时，如果有重复的映射键，则使用最后看到的键。从文本格式解析 map 时，如果有重复的键，则解析可能会失败。 如果你为映射字段提供键但没有值，则该字段被序列化时的行为取决于语言。在 C++、Java、Kotlin 和 Python 中，类型的默认值是序列化的，而在其他语言中则没有序列化。 生成的地图 API 目前可用于所有 proto3 支持的语言。你可以在相关 API 参考中找到有关所选语言的 map API 的更多信息。\n向后兼容 # map 语法在网络上等同于以下内容，因此不支持 map 的协议缓冲区实现仍然可以处理你的数据：\nmessage MapFieldEntry { key_type key = 1; value_type value = 2; } repeated MapFieldEntry map_field = N; 任何支持映射的协议缓冲区实现都必须生成和接受上述定义可以接受的数据。\n包 # 你可以将可选的 package 说明符添加到 .proto 文件中，以防止协议消息类型之间的名称冲突。\npackage foo.bar; message Open { ... } 然后，你可以在定义消息类型的字段时使用包说明符：\nmessage Foo { ... foo.bar.Open open = 1; ... } 包说明符影响生成代码的方式取决于你选择的语言：\n在 C++ 中，生成的类被包装在 C++ 命名空间中。例如，Open 将位于命名空间 foo::bar 中。 在 Java 和 Kotlin 中，包用作 Java 包，除非你在 .proto 文件中明确提供选项 java_package 。 在 Python 中，package 指令被忽略，因为 Python 模块是根据它们在文件系统中的位置来组织的。 在 Go 中，包用作 Go 包名称，除非你在 .proto 文件中明确提供选项 go_package。 在 Ruby 中，生成的类封装在嵌套的 Ruby 命名空间中，转换为所需的 Ruby 大写样式（第一个字母大写；如果第一个字符不是字母，则 PB_ 被前置）。 例如，Open 将位于命名空间 Foo::Bar 中。 在 C# 中，包在转换为 PascalCase 后用作命名空间，除非你在 .proto 文件中明确提供选项 csharp_namespace。 例如，Open 将在命名空间 Foo.Bar 中。 包和名称解析 # protobuf 中的类型名称解析与 C++ 类似：首先搜索最内部的范围，然后搜索下一个最内部的范围，依此类推，每个包都被认为是其父包的“内部”。 一个前置的 \u0026lsquo;.\u0026rsquo; （例如，.foo.bar.Baz）表示从最外面的范围开始。\n协议缓冲区编译器通过解析导入的 .proto 文件来解析所有类型名称。 每种语言的代码生成器都知道如何引用该语言中的每种类型，即使它有不同的范围规则。\n定义服务 # 如果你想在 RPC（远程过程调用）系统中使用你的消息类型，你可以在 .proto 文件中定义一个 RPC 服务接口，并且协议缓冲区编译器将以你选择的语言生成服务接口代码和存根。 因此，例如，如果你想使用获取 SearchRequest 并返回 SearchResponse 的方法定义 RPC 服务，你可以在 .proto 文件中定义它，如下所示：\nservice SearchService { rpc Search(SearchRequest) returns (SearchResponse); } 与协议缓冲区一起使用的最直接的 RPC 系统是 gRPC：由 Google 开发的一种语言和平台中立的开源 RPC 系统。 gRPC 特别适用于协议缓冲区，并允许你使用特殊的协议缓冲区编译器插件直接从 .proto 文件生成相关的 RPC 代码。\n如果你不想使用 gRPC，也可以将协议缓冲区与你自己的 RPC 实现一起使用。 你可以在 Proto2 语言指南中找到更多相关信息。\n还有一些正在进行的第三方项目为 Protocol Buffers 开发 RPC 实现。 有关我们了解的项目的链接列表，请参阅第三方附加组件 wiki 页面。\nJSON 映射 # Proto3 支持 JSON 中的规范编码，从而更容易在系统之间共享数据。下表中按类型描述了编码。\n如果 JSON 编码的数据中缺少某个值，或者它的值为 null，则在解析到协议缓冲区时，它将被解释为适当的默认值。 如果某个字段在协议缓冲区中具有默认值，则在 JSON 编码的数据中默认将其省略以节省空间。 实现可以提供选项以在 JSON 编码的输出中发出具有默认值的字段。\nproto3 JSON JSON example message object {\u0026ldquo;fooBar\u0026rdquo;: v, \u0026ldquo;g\u0026rdquo;: null, …} enum string \u0026ldquo;FOO_BAR\u0026rdquo; map\u0026lt;K,V\u0026gt; object {\u0026ldquo;k\u0026rdquo;: v, …} repeated V array [v, …] bool true, false true, false string string \u0026ldquo;Hello World!\u0026rdquo; bytes base64 string \u0026ldquo;YWJjMTIzIT8kKiYoKSctPUB+\u0026rdquo; int32, fixed32, uint32 number 1, -10, 0 int64, fixed64, uint64 string \u0026ldquo;1\u0026rdquo;, \u0026ldquo;-10\u0026rdquo; float, double number 1.1, -10.0, 0, \u0026ldquo;NaN\u0026rdquo;, \u0026ldquo;Infinity\u0026rdquo; Any object {\u0026quot;@type\u0026quot;: \u0026ldquo;url\u0026rdquo;, \u0026ldquo;f\u0026rdquo;: v, … } Timestamp string \u0026ldquo;1972-01-01T10:00:20.021Z\u0026rdquo; Duration string \u0026ldquo;1.000340012s\u0026rdquo;, \u0026ldquo;1s\u0026rdquo; Struct object { … } Wrapper types various types 2, \u0026ldquo;2\u0026rdquo;, \u0026ldquo;foo\u0026rdquo;, true, \u0026ldquo;true\u0026rdquo;, null, 0, … FieldMask string \u0026ldquo;f.fooBar,h\u0026rdquo; ListValue array [foo, bar, …] Value value NullValue null Empty object {} JSON 选项 # proto3 JSON 实现可以提供以下选项：\n发送具有默认值的字段：在 proto3 JSON 输出中默认省略具有默认值的字段。实现可以提供一个选项来覆盖此行为并使用其默认值输出字段。 忽略未知字段：Proto3 JSON 解析器默认应拒绝未知字段，但可能会提供在解析中忽略未知字段的选项。 使用 proto 字段名称而不是 lowerCamelCase 名称：默认情况下，proto3 JSON 打印器应将字段名称转换为首字符小写的驼峰式并将其用作 JSON 名称。 实现可能会提供一个选项来使用 proto 字段名称作为 JSON 名称。proto3 JSON 解析器需要接受转换后的首字符小写的驼峰式名称和 proto 字段名称。 将枚举值作为整数而不是字符串发送：默认情况下，在 JSON 输出中使用枚举值的名称。可以提供一个选项来代替使用枚举值的数值。 选项 # .proto 文件中的单个声明可以使用多个选项进行注释。选项不会改变声明的整体含义，但可能会影响它在特定上下文中的处理方式。 可用选项的完整列表在 google/protobuf/descriptor.proto 中定义。\n一些选项是文件级选项，这意味着它们应该在顶级范围内编写，而不是在任何消息、枚举或服务定义中。 一些选项是消息级别的选项，这意味着它们应该写在消息定义中。 一些选项是字段级选项，这意味着它们应该写在字段定义中。 选项也可以写在枚举类型、枚举值、oneof 字段、服务类型和服务方法上；但是，目前不存在任何有用的选项。\n以下是一些最常用的选项：\njava_package（文件选项）：要用于生成的 Java/Kotlin 类的包。 如果 .proto 文件中没有明确给出 java_package 选项，则默认使用 proto 包（使用 .proto 文件中的 “package” 关键字指定）。 但是，proto 包通常不能制作好的 Java 包，因为不期望 proto 包以反向域名开头。如果不生成 Java 或 Kotlin 代码，则此选项无效。\noption java_package = \u0026#34;com.example.foo\u0026#34;; java_outer_classname（文件选项）：要生成的包装 Java 类的类名（以及文件名）。 如果 .proto 文件中没有明确指定 java_outer_classname，则将通过将 .proto 文件名转换为驼峰式来构造类名（因此 foo_bar.proto 变为 FooBar.java）。 如果 java_multiple_files 选项被禁用，那么所有其他 class/enum 等为 .proto 文件生成的文件将在这个外部包装 Java 类中生成为嵌套 class/enum 等。 如果不生成 Java 代码，则此选项无效。\noption java_outer_classname = \u0026#34;Ponycopter\u0026#34;; java_multiple_files（文件选项）：如果为 false，则只会为此 .proto 文件以及所有 Java class/enum 等生成一个 .java 文件。 为顶级消息、服务和枚举生成的将嵌套在外部类中（请参阅 java_outer_classname）。 如果为 true，将为每个 Java class/enum 等生成单独的 .java 文件。 为顶级消息、服务和枚举生成，并且为此 .proto 文件生成的包装 Java 类将不包含任何嵌套 class/enum 等。 这是一个布尔选项，默认为 false。如果不生成 Java 代码，则此选项无效。\noption java_multiple_files = true; optimize_for（文件选项）：可以设置为 SPEED、CODE_SIZE 或 LITE_RUNTIME。这会通过以下方式影响 C++ 和 Java 代码生成器（可能还有第三方生成器）：\nSPEED（默认）：protocol buffer 编译器将生成用于序列化、解析和对消息类型执行其他常见操作的代码。此代码经过高度优化。 CODE_SIZE：protocol buffer 编译器将生成最少的类，并将依赖共享的、基于反射的代码来实现序列化、解析和各种其他操作。因此，生成的代码将比使用 SPEED 小得多，但操作会更慢。 类仍将实现与在 SPEED 模式下完全相同的公共 API。此模式在包含大量 .proto 文件且不需要所有文件都非常快的应用程序中最有用。 LITE_RUNTIME：协议缓冲区编译器将生成仅依赖于“lite”运行时库（libprotobuf-lite 而不是 libprotobuf）的类。 lite 运行时比完整库小得多（大约小一个数量级），但省略了描述符和反射等某些功能。这对于在手机等受限平台上运行的应用程序特别有用。 编译器仍将生成所有方法的快速实现，就像它在 SPEED 模式下一样。生成的类只会在每种语言中实现 MessageLite 接口，它只提供完整 Message 接口的方法的子集。 option optimize_for = CODE_SIZE; cc_enable_arenas（文件选项）：为 C++ 生成的代码启用 arens 分配。\nobjc_class_prefix（文件选项）：设置 Objective-C 类前缀，该前缀添加到所有 Objective-C 生成的类和来自此 .proto 的枚举中。 没有默认值。你应该按照 Apple 的建议使用介于 3-5 个大写字符之间的前缀。请注意，所有 2 个字母前缀均由 Apple 保留。\ndeprecated（字段选项）：如果设置为 true，则表示该字段已弃用，不应被新代码使用。在大多数语言中，这没有实际效果。 在 Java 中，这成为 @Deprecated 注释。将来，其他特定于语言的代码生成器可能会在字段的访问器上生成弃用注释，这反过来会导致在编译尝试使用该字段的代码时发出警告。 如果该字段未被任何人使用并且你希望阻止新用户使用它，请考虑将字段声明替换为保留语句。\nint32 old_field = 6 [deprecated = true]; 自定义选项 # Protocol Buffers 还允许你定义和使用自己的选项。这是大多数人不需要的高级功能。 如果你确实认为需要创建自己的选项，请参阅 Proto2 语言指南了解详细信息。 请注意，创建自定义选项使用扩展，仅允许用于 proto3 中的自定义选项。\n生成你的类 # 要生成 Java、Kotlin、Python、C++、Go、Ruby、Objective-C 或 C# 代码， 你需要使用 .proto 文件中定义的消息类型，你需要在 .proto 上运行协议缓冲区编译器协议。 如果你尚未安装编译器，请下载软件包并按照 README 中的说明进行操作。 对于 Go，你还需要为编译器安装一个特殊的代码生成器插件：你可以在 GitHub 上的 golang/protobuf 存储库中找到此插件和安装说明。\n协议编译器调用如下：\nprotoc --proto_path=IMPORT_PATH --cpp_out=DST_DIR --java_out=DST_DIR --python_out=DST_DIR --go_out=DST_DIR --ruby_out=DST_DIR --objc_out=DST_DIR --csharp_out=DST_DIR path/to/file.proto IMPORT_PATH 指定解析导入指令时要在其中查找 .proto 文件的目录。 如果省略，则使用当前目录。通过多次传递 \u0026ndash;proto_path 选项可以指定多个导入目录；他们将被按顺序搜索。 -I=_IMPORT_PATH_ 可以用作 --proto_path 的缩写形式。 你可以提供一个或多个输出指令： --cpp_out 在 DST_DIR 中生成 C++ 代码。 有关更多信息，请参阅 C++ 生成的代码参考。 --java_out 在 DST_DIR 中生成 Java 代码。 有关更多信息，请参阅 Java 生成的代码参考。 --kotlin_out 在 DST_DIR 中生成额外的 Kotlin 代码。 有关更多信息，请参阅 Kotlin 生成的代码参考。 --python_out 在 DST_DIR 中生成 Python 代码。 有关更多信息，请参阅 Python 生成的代码参考。 --go_out 在 DST_DIR 中生成 Go 代码。 有关更多信息，请参阅 Go 生成的代码参考。 --ruby_out 在 DST_DIR 中生成 Ruby 代码。 有关更多信息，请参阅 Ruby 生成的代码参考。 --objc_out 在 DST_DIR 中生成 Objective-C 代码。 有关更多信息，请参阅 Objective-C 生成的代码参考。 --csharp_out 在 DST_DIR 中生成 C# 代码。 有关更多信息，请参阅 C# 生成的代码参考。 --php_out 在 DST_DIR 中生成 PHP 代码。 有关更多信息，请参阅 PHP 生成的代码参考。 作为额外的便利，如果 DST_DIR 以 .zip 或 .jar 结尾，编译器会将输出写入具有给定名称的单个 ZIP 格式存档文件。 .jar 输出也将按照 Java JAR 规范的要求提供一个清单文件。请注意，如果输出存档已经存在，它将被覆盖；编译器不够聪明，无法将文件添加到现有存档中。 你必须提供一个或多个 .proto 文件作为输入。可以一次指定多个 .proto 文件。 尽管文件是相对于当前目录命名的，但每个文件必须驻留在 IMPORT_PATH 之一中，以便编译器可以确定其规范名称。 "},{"id":3,"href":"/posts/devops-vs-gitops/","title":"DevOps VS GitOps","section":"Blogs","content":"GitOps 和 DevOps 正迅速成为开发团队的黄金标准方法。DevOps 文化代表了从传统软件和技术开发的转变，传统的软件和技术开发涉及从构思和概念到发布的线性路径，并鼓励协作和快速反馈，而不是孤立地工作。在本文中，我们将仔细研究 GitOps 与 DevOps，以帮助确定 GitOps 是否适用于您的软件开发项目。\nDevOps # 从字面上来看，\u0026ldquo;DevOps\u0026rdquo; 一词是由英文 Development（开发）和 Operations（运维）组合而成，但它所代表的理念和实践要比这广阔的多。DevOps 涵盖了安全、协作方式、数据分析等许多方面。但它是什么呢？ DevOps 强调通过一系列手段来实现既快又稳的工作流程，使每个想法（比如一个新的软件功能，一个功能增强请求或者一个 bug 修复）在从开发到生产环境部署的整个流程中，都能不断地为用户带来价值。这种方式需要开发团队和运维团队密切交流、高效协作并且彼此体谅。此外，DevOps 还要能够方便扩展，灵活部署。有了 DevOps，需求最迫切的工作就能通过自助服务和自动化得到解决；通常在标准开发环境编写代码的开发人员也可与运维人员紧密合作，加速软件的构建、测试和发布，同时保障开发成果的稳定可靠。 一句话总结：DevOps 是文化、开发、运营和工具的结合体，可提高组织高速生产应用和服务的能力。\n工作流 # 图中描述了 Dev 和 Ops 的角色任务是一个循环往复的过程：\nDev 开始研发迭代（Plan），开始编写代码（Code），完成开始编译（Build）和测试（Test），最后发布（Release） Ops 拿到发布打包开始部署（Deploy），并运维（Operate）和监控（Monitor），并收集新的功能需求，特性增强以及 bug 修复等，形成下一个迭代安排（Plan） GitOps # GitOps 是一套使用 Git 来管理基础设施和应用配置的实践，而 Git 指的是一个开源版控制系统。GitOps 在运行过程中以 Git 为声明性基础设施和应用的单一事实来源。 GitOps 使用 Git PR 来自动管理基础设施的置备和部署。Git Repo 包含系统的全部状态，因此系统状态的修改痕迹既可查看也可审计。 GitOps 围绕开发者经验而构建，可帮助团队使用与软件开发相同的工具和流程来管理基础架构。除了 Git 以外，GitOps 还可以按照自己的需求选择工具。 一句话总结：GitOps 就是使用 Git 拉取请求来验证和自动部署系统基础设施变更的实践。\n工作流 # 我们可以认为 GitOps 是基础架构即代码（IaC）的一个演变，以 Git 为基础架构配置的版本控制系统。IaC 通常采用声明式方法来管理基础架构，定义系统的期望状态，并跟踪系统的实际状态。 与 IaC 一样，GitOps 也要求声明式描述系统的期望状态。通过使用声明式工具，所有的配置文件和源代码都可以在 Git 中进行版本控制。\nCI/CD 流程通常由外部事件触发，比如代码被推送到了存储库中。在 GitOps 工作流中，要进行变更，需要通过 PR 来修改 Git 仓库的状态。 一旦这些 PR 被批准和合并，它们将自动应用于实时基础设施。开发人员可以使用他们的标准工作流以及 CI/CD 流水线。 同时搭配使用 Kubernetes 与 GitOps 时，Operator 通常为 Kubernetes Operator。 Operator 会将 Repo 中的期望状态与所部署基础设施的实际状态进行比较。每当注意到实际状态与存储库中的期望状态存在差异时，Operator 便会更新基础设施。Operator 还可以监控容器镜像仓库，并以同样的方式进行更新，从而部署新的镜像。 GitOps 中还有一个重要概念——可观察性，指可观察的任何系统状态。GitOps 的可观察性有助于确保观察到的状态（或实际状态）与理想状态一致。 使用 PR 和 Git 之类的版本控制系统让部署过程清晰可见。这样一来，便可查看和跟踪对系统进行的任何变更，提供审计跟踪，并在出现问题时进行回滚变更。 GitOps 工作流可以提高生产率，加快开发和部署速度，同时提高系统的稳定性和可靠性。\n部署策略 # GitOps 的部署策略有两种实现方式：基于推送（Push）和基于拉取（Pull）的部署。两种部署类型之间的区别在于如何确保部署环境实际上类似于所需的基础架构。在可能的情况下，应该首选基于 Pull 的方法，因为它被认为更安全，因此是实施 GitOps 的更好实践。\n基于 Push # 基于 Push 的部署策略由流行的 CI/CD 工具实现，例如 Jenkins、CircleCI 或 Travis CI。应用程序的源代码与部署应用程序所需的配置一起位于应用程序代码库中。每当更新应用程序代码时，都会触发构建管道，构建容器镜像，最后使用新的部署描述符更新环境配置仓库。\n基于 Pull # 基于 Pull 的部署策略使用与基于 Push 的基本相同的概念，但在部署流水线的工作方式上有所不同。传统的 CI/CD 管道由外部事件触发，例如当新代码被推送到应用程序存储库时。使用基于 Pull 的部署方法，引入了 Operator。它通过不断地将环境存储库中的所需状态与已部署基础架构中的实际状态进行比较，接管 Pipeline 的角色。每当发现差异时，Operator 都会更新基础架构以匹配环境存储库。此外，可以监控镜像仓库以查找要部署的镜像版本。\n小结 # DevOps GitOps 定位 文化，专注于 CI/CD 的文化 技术，使用 Git 管理基础设施配置和软件部署的技术 依赖 CI/CD Pipeline Git 协作 云配置即代码和供应链管理 Kubernetes、IaC 和各种 CI/CD Pipeline 目标 降低错误率，消除团队中的孤岛，减少成本工作等 速度、准确性、清洁代码、提高生产力 正确性 不太关注代码的精度 高度关注准确和简洁的代码 灵活性 不那么严格，更开放 更严格，更少开放 参考资料 # DevOps in a Nutshell – What Is DevOps, Really? Understanding DevOps What is GitOps? Guide To GitOps GitOps vs DevOps: Differences and Why They are Better Together GitOps vs DevOps – What’s the difference? "},{"id":4,"href":"/posts/notes-of-circle-ci/","title":"Circle CI 学习笔记","section":"Blogs","content":"CircleCI 是基于云的 CI/CD 工具，可自动执行软件构建和交付过程。它提供快速的配置和维护，没有任何复杂性。 由于它是基于云的 CI/CD 工具，因此消除了专用服务器的困扰，并降低了维护本地服务器的成本。 此外，基于云的服务器是可扩展的，健壮的，并有助于更快地部署应用程序。\n基本概念 # 首先介绍下 Circle CI 的 基本概念，帮助大家理解 Circle CI 是如何管理 CI/CD 流程。\n管控层 # Project # Circle CI 项目在你的 VCS（GitHub 或者 Bitbucket）中 ，共享代码库的名称。在 Circle CI 的主页中点击 Projects 可以自由添加项目进入仪表盘，以追踪代码变更。\nConfiguration # Circle CI 遵循配置即代码（Configuration as Code）原则，整个 CI/CD 的流程都是通过 config.yml 文件，该文件位于项目根目录下的 .circleci 文件夹中。以下按照术语和依赖性顺序，列举了 Circle CI 最常见的组件：\nPipeline：代码表整个配置，仅用于 Circle CI 云 Workflow：负责编排多个 Job Job：负责运行执行命令的一系列 Step Step：运行命令（例如安装依赖项或运行测试）和 shell 脚本 User Type # 大多数都是从 VCS 帐号中继承的权限，主要包括以下四种：\n组织管理员（Organization Administrator）：从 VCS 继承的权限级别。 GitHub：Owner 权限，并且至少关注了一个在 Circle CI 上的构建的项目。 Bitbucket：Admin 权限，并且至少关注了一个在 Circle CI 上的构建的项目。 项目管理员（Project Administrator）：是将 GitHub 或 Bitbucket 的 Repo 作为项目添加到 Circle CI 的用户。 用户（User）：是组织内的个人用户，从 VCS 继承。 Circle CI 用户（Circle CI User）：可以使用用户名和密码登录 Circle CI 平台的任何人。 配置层 # Pipeline # 管道是触发项目工作时运行的全套流程，它包含了 Workflow，Workflow 会协调 Job，这些都应在项目的配置文件中。Pipeline 在 Circle CI 2.X 版本不可用。\nOrb # Orb 是可重复使用的代码片段，有助于自动化重复流程、加快项目设置并使其易于与第三方工具集成。\nversion: 2.1 orbs: maven: circleci/maven@0.0.12 workflow: maven_test: jobs: - maven/test Job # 作业是配置的基础，也是 Step 的集合，根据需要运行命令/脚本。每个作业必须声明 Executor，它可以是：\ndocker：必须指定镜像 machine：存在默认镜像 windows：必须使用 Window Orb macos：必须执行 XCode 版本 Executor 和 images # 每个作业都可以在唯一的执行器中运行，可以是 Docker 容器，也可以是 Linux/Windows 的虚拟机。Circle CI 2.X 不支持 MacOS。\nversion: 2.1 jobs: build1: # job 名称 docker: # 使用 docker 环境 - image: buildpack-deps:trusty # 主镜像 auth: username: mydockerhub-user password: $DOCKERHUB_PASSWORD # context/project UI 环境变量引用 - image: postgres:9.4.1 # 次镜像 auth: username: mydockerhub-user password: $DOCKERHUB_PASSWORD # context/project UI 环境变量引用 # 次容器可以通过 localhost 访问主容器 environment: # 指定环境变量 POSTGRES_USER: root build2: machine: # 使用 Linux 环境 image: ubuntu-2004:202010-01 # 指定 Linux VM 版本 build3: macos: # 使用 macOS 环境 xcode: \u0026#34;11.3.0\u0026#34; # 指定 Xcode 版本 Step # 步骤通常是一组可执行命令，例如内置命令 checkout 通过 SSH 检查源码；run 可以自定义命令。命令可以定义成全局，供多次使用。\njobs: build: docker: - image: \u0026lt;image-name-tag\u0026gt; auth: username: mydockerhub-user password: $DOCKERHUB_PASSWORD # context/project UI 环境变量引用 steps: - checkout # 内置 step，checkout 代码 - run: # 内置 step，用于执行命令 name: Running tests # step 名称 command: make test # 默认情况下，可执行命令在非登录 shell 中使用 /bin/bash -eo pipefail 选项运行。 Image # 镜像就是一个打包的系统，在 .circleci/config.yml 中定义的主镜像，这是 Docker 或者其他执行器执行 Job 命令的地方。\nWorkflow # 工作流定义了 Job 列表及其运行顺序。可以并行、串行、按计划或使用 approval 的来运行 Job。\n#... workflows: build_and_test: # workflow 名称 jobs: - build - test: # test 依赖 build 执行成功 requires: - build - hold: # hold 依赖 build 和 test 执行成功，并且需要 approve type: approval requires: - build - test - deploy: # deploy 依赖 hold 执行成功 requires: - hold Cache/Workspace/Artifact # Cache 在对象存储中保存的是文件或文件目录，例如依赖项或源代码。每个作业都可能包含特殊步骤，例如使用先前作业缓存的依赖项来加速构建。 version: 2.1 jobs: build1: docker: - image: \u0026lt;image-name-tag\u0026gt; auth: username: mydockerhub-user password: $DOCKERHUB_PASSWORD # context/project UI 环境变量引用 steps: - checkout - save_cache: # 内置 step，cache 依赖环境变量提供的 key key: v1-repo-{{ .Environment.CIRCLE_SHA1 }} paths: - ~/circleci-demo-workflows # build2 使用 build1 的缓存 build2: docker: - image: \u0026lt;image-name-tag\u0026gt; auth: username: mydockerhub-user password: $DOCKERHUB_PASSWORD # context/project UI 环境变量引用 steps: - restore_cache: # 内置 step，恢复缓存依赖项 key: v1-repo-{{ .Environment.CIRCLE_SHA1 }} Workspace 是一种感知 Workflow 的存储机制。Workspace 保存 Job 独有的数据，下游 Job 可能需要这些数据。每个 Workflow 都有一个与之关联的临时 Workspace。Workspace 可用于将同一 Workflow 下某个 Job 在执行期间构建的特殊数据传递给其他 Job。 Artifact 在 Workflow 完成后保留数据，可用于构建并输出长期存储。 version: 2.1 jobs: build1: #... steps: - persist_to_workspace: # 内置 step # 将指定路径(workspace/echo-output) 保留到 Workspace 中以供下游 Job 使用 # 路径必须是绝对路径，或来自 working_directory 的相对路径。 # 这是容器里的目录，是工作空间的根目录。 root: workspace # 必须是根目录的相对路径 paths: - echo-output build2: #... steps: - attach_workspace: # 内置 step # 路径必须是绝对路径，或来自 working_directory 的相对路径。 at: /tmp/workspace build3: #... steps: - store_artifacts: # 内置 step path: /tmp/artifact-1 destination: artifact-file 小结 # 走到这里，应该对 Circle CI 有了粗浅的认识，但想要上手甚至是玩转，还需要更多指导和实践。这是初次尝试 Circle CI 时写 config.yml。当然 Circle CI 提供了丰富的配置项，详情可以移步这里： Circle CI 配置参考。\nOrb 简介 # Circle CI Orb 是可共享的配置包，可以看做是 lib 库，选择合适的 Orb ，会让 Circle CI 配置编写更容易。Orb 中可配置元素主要包括：Command、Executor 和 Job。\nCommand 示例 # version: 2.1 orbs: aws-s3: circleci/aws-s3@x.y.z jobs: build: docker: - image: \u0026#39;cimg/python:3.6\u0026#39; auth: username: mydockerhub-user password: $DOCKERHUB_PASSWORD # context/project UI 环境变量引用 steps: - checkout - run: mkdir bucket \u0026amp;\u0026amp; echo \u0026#34;lorem ipsum\u0026#34; \u0026gt; bucket/build_asset.txt # 使用orb：aws-s3 的 copy 命令 - aws-s3/copy: from: bucket/build_asset.txt to: \u0026#39;s3://my-s3-bucket-name\u0026#39; Executor 示例 # description: \u0026gt; Select the version of NodeJS to use. Uses CircleCI\u0026#39;s highly cached convenience images built for CI. docker: - image: \u0026#39;cimg/node:\u0026lt;\u0026lt;parameters.tag\u0026gt;\u0026gt;\u0026#39; auth: username: mydockerhub-user password: $DOCKERHUB_PASSWORD # context/project UI 环境变量引用 parameters: tag: default: \u0026#39;13.11\u0026#39; description: \u0026gt; Pick a specific cimg/node image version tag: https://hub.docker.com/r/cimg/node type: string Job 示例 # version: 2.1 orbs: \u0026lt;orb\u0026gt;: \u0026lt;namespace\u0026gt;/\u0026lt;orb\u0026gt;@x.y #orb 版本 workflows: use-orb-job: jobs: - \u0026lt;orb\u0026gt;/\u0026lt;job-name\u0026gt; Orb 开发实践 # Orb 实践，简单来说包括这些阶段：取名、分类、说明、context 限制、发布、升级。在实践之前，先安装 Orb 开发套件： circleci。\n取名 # 一个好的 Orb 命名，是由 Namespace + 名称组成，由正斜杠分隔。Namespace 代表拥有和维护 Orb 的个人、公司或组织，而 Orb 名称本身应描述提供的产品、服务或操作。取名之前，先注册 Namespace，这里需要关联 Circle CI 帐号，所以需要获取 Access Token（注意保存），然后就可以使用 注册 Namespace 命令 circleci namespace create \u0026lt;name\u0026gt; \u0026lt;vcs-type\u0026gt; \u0026lt;org-name\u0026gt; [flags]，详细如下：\n➜ ~ circleci namespace create howieyuen-orb github howieyuen --token 5b8b950571bbe70ad5fa15e22ec052b52702f241 You are creating a namespace called \u0026#34;howieyuen-orb\u0026#34;. This is the only namespace permitted for your github organization, howieyuen. To change the namespace, you will have to contact CircleCI customer support. ? Are you sure you wish to create the namespace: `howieyuen-orb` Yes Namespace `howieyuen-orb` created. Please note that any orbs you publish in this namespace are open orbs and are world-readable. 初始化 # circleci 工具提供了 初始化 Orb 命令：circleci orb init。它包含了分类、说明等操作。但有几个前置操作需要完成：\n设置 Context：Organization Settings \u0026gt; Contexts 启用 Uncertified Orbs：Organization Settings \u0026gt; Security 新建 GitHub Repo： https://github.com/howieyuen/hello-circleci-orb 到此，就可以执行初始化命令 circleci orb init \u0026lt;path\u0026gt; [flags]，详细如下：\n➜ circleci orb init hello-circleci-orb --token 5b8b950571bbe70ad5fa15e22ec052b52702f241 Note: This command is in preview. Please report any bugs! https://github.com/CircleCI-Public/circleci-cli/issues/new/choose ? Would you like to perform an automated setup of this orb? Yes, walk me through the process. Downloading Orb Project Template into hello-circleci-orb A few questions to get you up and running. ? Are you using GitHub or Bitbucket? GitHub ? Enter your github username or organization howieyuen ? Enter the namespace to use for this orb howieyuen-orb Saving namespace howieyuen-orb as default ? Orb name hello-circleci-orb ? What categories will this orb belong to? Testing ? Automatically set up a publishing context for your orb? Yes, set up a publishing context with my API key. `orb-publishing` context already exists, continuing on ? Would you like to set up your git project? Yes ? Enter your primary git branch. main ? Enter the remote git repository git@github.com:howieyuen/hello-circleci-orb.git Thank you! Setting up your orb... An initial commit has been created - please run \u0026#39;git push origin main\u0026#39; to publish your first commit! ? I have pushed to my git repository using the above command Yes Your orb project is building here: https://circleci.com/gh/howieyuen/hello-circleci-orb You are now working in the alpha branch. Once the first public version is published, you\u0026#39;ll be able to see it here: https://circleci.com/developer/orbs/orb/howieyuen-orb/hello-circleci-orb View orb publishing doc: https://circleci.com/docs/2.0/orb-author 该步骤完成后，项目默认位于 alpha 分支，该分支代码是从模板克隆，模板详见 链接。\n发布 # 在发布之前，可以将 alpha 分支，推送到远端，触发 test-pack workflow，执行基本验证、lint 和单元测试。\n目前为止，Orb 只是在 dev 阶段，其他用户无法从 Orb 市场检索到。正式发布的流程也很简单，是根据 Orb 仓库的 Commit Message 的前缀字段 [semver:\u0026lt;increment\u0026gt;]，来确定是否需要发布，以及发布的版本号。Orb 发布也遵循 语义化版本 控制：\nIncrement 描述 major 发布 1.0.0 增量版本 minor 发布 x.1.0 增量版本 patch 发布 xx1 增量版本 skip 不要发布版本 因此，发布版本，只需要修改 alpha 分支的 commit message，将 “skip” 改成 increment 任何一个值即可，Circle CI 就会触发 integration_test_deploy 工作流，待完成后，即可在 Orb 市场检索到刚才发布的 Orb： howieyuen-orb/hello-circleci-orb@1.0.0。\n参考资料 # Circle CI 基本概念 Circle CI 配置样例 Circle CI 配置参考 Circle CI 可重用配置指南 Orb 介绍 Orb 概念 CircleCI 本地 CLI "},{"id":5,"href":"/docs/golang/tools/pprof/","title":"pprof","section":"golang 工具","content":" 1. 什么是 pprof # Profiling 是指在程序执行过程中，收集能够反映程序执行状态的数据。 在软件工程中，性能分析（performance analysis，也称为 profiling）， 是以收集程序运行时信息为手段研究程序行为的分析方法，是一种动态程序分析的方法。\nGo 语言自带的 pprof 库就可以分析程序的运行情况，并且提供可视化的功能。它包含两个相关的库：\nruntime/pprof 对于只跑一次的程序，例如每天只跑一次的离线预处理程序，调用 pprof 包提供的函数，手动开启性能数据采集。 net/http/pprof 对于在线服务，对于一个 HTTP Server，访问 pprof 提供的 HTTP 接口，获得性能数据。 当然，实际上这里底层也是调用的 runtime/pprof 提供的函数，封装成接口对外提供网络访问。 2. pprof 的作用 # 下表来自 golang pprof 实战\n类型 描述 备注 allocs 内存分配情况的采样信息 可以用浏览器打开，但可读性不高 blocks 阻塞操作情况的采样信息 可以用浏览器打开，但可读性不高 cmdline 当前程序的命令行调用 可以用浏览器打开，显示编译文件的临时目录 goroutine 当前所有协程的堆栈信息 可以用浏览器打开，但可读性不高 heap 堆上内存使用情况的采样信息 可以用浏览器打开，但可读性不高 mutex 锁争用情况的采样信息 可以用浏览器打开，但可读性不高 profile CPU 占用情况的采样信息 浏览器打开会下载文件 threadcreate 系统线程创建情况的采样信息 可以用浏览器打开，但可读性不高 trace 程序运行跟踪信息 浏览器打开会下载文件，本文不涉及，可另行参阅 深入浅出 Go trace allocs 和 heap 采样的信息一致，不过前者是所有对象的内存分配，而 heap 则是活跃对象的内存分配。\n3. pprof 如何使用 # 我们可以通过报告生成、Web 可视化界面、交互式终端三种方式来使用 pprof。\n3.1 runtime/pprof # 拿 CPU profiling 举例，增加两行代码，调用 pprof.StartCPUProfile 启动 cpu profiling， 调用 pprof.StopCPUProfile() 将数据刷到文件里：\npackage main import \u0026#34;runtime/pprof\u0026#34; var cpuprofile = flag.String(\u0026#34;cpuprofile\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;write cpu profile to file\u0026#34;) func main() { // … pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() // … } 3.2 net/http/pprof # 启动一个端口（和正常提供业务服务的端口不同）监听 pprof 请求：\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;regexp\u0026#34; ) func handler(wr http.ResponseWriter, r *http.Request) { var pattern = regexp.MustCompile(`^(\\w+)@foo.bar$`) account := r.URL.Path[1:] res := pattern.FindSubmatch([]byte(account)) if len(res) \u0026gt; 1 { wr.Write(res[1]) } else { wr.Write([]byte(\u0026#34;None\u0026#34;)) } } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) err := http.ListenAndServe(\u0026#34;:8888\u0026#34;, nil) if err != nil { log.Fatal(\u0026#34;ListenAndServe:\u0026#34;, err) } } pprof 包会自动注册 handler， 处理相关的请求：\n// src/net/http/pprof/pprof.go:71 func init() { http.Handle(\u0026#34;/debug/pprof/\u0026#34;, http.HandlerFunc(Index)) http.Handle(\u0026#34;/debug/pprof/cmdline\u0026#34;, http.HandlerFunc(Cmdline)) http.Handle(\u0026#34;/debug/pprof/profile\u0026#34;, http.HandlerFunc(Profile)) http.Handle(\u0026#34;/debug/pprof/symbol\u0026#34;, http.HandlerFunc(Symbol)) http.Handle(\u0026#34;/debug/pprof/trace\u0026#34;, http.HandlerFunc(Trace)) } 启动服务后，直接在浏览器访问：http://localhost:8888/debug/pprof/，得到如下页面：\n关于 goroutine 的信息有两个链接，goroutine 和 full goroutine stack dump， 前者是一个汇总的消息，可以查看 goroutines 的总体情况， 后者则可以看到每一个 goroutine 的状态。 页面具体内容的解读可以参考大彬的文章 实战 Go 内存泄露。\n4. 采样分析 # 开始采样之前，需要模拟服务访问，这里用到的是压力测试管径 wrk。\n# 启动 500 个 HTTP 连接，30 个线程，持续访问 1 分钟 wrk -c500 -t30 -d1m http://localhost:8888/zzz@foo.bar 4.1 报告生成 # 报告生成有 2 种方式：\nruntime/pprof 写入本地文件 内置页面下载采样文件 下面以内置页面为例： 在 debug/pprof 页面点击 profile，会在后台进行默认 30s 的数据采样，采样完成后，返回给浏览器一个采样文件，其他命令类似。 然后使用以下命令：\ngo tool pprof -http=:8080 profile 会在本地 8080 端口启动 http 服务，提供可视化界面：\n图中的连线代表对方法的调用，连线上的标签代表指定的方法调用的采样值（例如时间、内存分配大小等）， 方框的大小与方法运行的采样值的大小有关。 每个方框由两个标签组成：在 cpu profile 中，一个是方法运行的时间占比， 一个是它在采样的堆栈中出现的时间占比（前者是 flat 时间，后者则是 cumulate 时间占比）； 框越大，代表耗时越多或是内存分配越多\n点击左上角 VIEW 可以选择不同图形化展示，最常见的火焰图如下：\n它和一般的火焰图相比刚好倒过来了，调用关系的展现是从上到下。形状越长，表示执行时间越长。\n4.2 交互式命令 # 除了使用内置的页面采样分析，也直接使用如下命令，不需要通过浏览器直接进入~\n# step1：压测工具访问服务 wrk -c500 -t30 -d1m http://localhost:8888/zzz@foo.bar # step2：采集数据，默认 30s go tool pprof http://localhost:8888/debug/pprof/profile 其他类似的采集数据命令还有：\n# 自定义等待时间 120s go tool pprof http://localhost:8888/debug/pprof/profile?second=120 # 下载 heap profile go tool pprof http://localhost:8888/debug/pprof/heap # 下载 goroutine profile go tool pprof http://localhost:8888/debug/pprof/goroutine # 下载 block profile go tool pprof http://localhost:8888/debug/pprof/block # 下载 mutex profile go tool pprof http://localhost:8888/debug/pprof/mutex 下面开始进入命令行交互式模式，使用 top、list、web 等命令：\n(pprof) top Showing nodes accounting for 86.85s, 91.15% of 95.28s total Dropped 397 nodes (cum \u0026lt;= 0.48s) Showing top 10 nodes out of 93 flat flat% sum% cum cum% 75.37s 79.10% 79.10% 75.60s 79.35% syscall.syscall 3.79s 3.98% 83.08% 3.81s 4.00% runtime.kevent 1.52s 1.60% 84.68% 1.53s 1.61% runtime.usleep 1.21s 1.27% 85.95% 1.21s 1.27% runtime.pthread_cond_wait 1.20s 1.26% 87.21% 1.20s 1.26% runtime.pthread_kill 0.94s 0.99% 88.19% 0.94s 0.99% runtime.nanotime1 0.89s 0.93% 89.13% 0.89s 0.93% runtime.procyield 0.84s 0.88% 90.01% 0.84s 0.88% indexbytebody 0.59s 0.62% 90.63% 0.59s 0.62% runtime.pthread_cond_signal 0.50s 0.52% 91.15% 0.85s 0.89% runtime.scanobject 得到 5 列数据：\n字段 描述 flat 本函数的执行耗时 flat% flat 占 CPU 总时间的比例。程序总耗时 95.28s，syscall.syscall 的 75.60s 占了 79.35% sum% 前面每一行的 flat 占比总和 cum 累计量。指该函数加上该函数调用的函数总耗时 cum% cum 占 CPU 总时间的比例 上面的结果没啥有价值的信息，都是 go 的底层调用。使用 list 正则匹配业务代码：\n(pprof) list handler Total: 1.65mins ROUTINE ======================== main.handler in /Users/yuanhao/Study/GoProjects/testProject/pprof/main.go 30ms 2.63s (flat, cum) 2.65% of Total . . 6:\t_ \u0026#34;net/http/pprof\u0026#34; . . 7:\t\u0026#34;regexp\u0026#34; . . 8:) . . 9: . . 10:func handler(wr http.ResponseWriter, r *http.Request) { . 2.38s 11:\tvar pattern = regexp.MustCompile(`^(\\w+)@foo.bar$`) 20ms 20ms 12:\taccount := r.URL.Path[1:] . 160ms 13:\tres := pattern.FindSubmatch([]byte(account)) . . 14:\tif len(res) \u0026gt; 1 { 10ms 70ms 15:\twr.Write(res[1]) . . 16:\t} else { . . 17:\twr.Write([]byte(\u0026#34;None\u0026#34;)) . . 18:\t} . . 19:} . . 20: ROUTINE ======================== net/http.(*ServeMux).handler in /usr/local/Cellar/go/1.15.1/libexec/src/net/http/server.go 10ms 10ms (flat, cum) 0.01% of Total . . 2384:\treturn mux.handler(host, r.URL.Path) . . 2385:} . . 2386: . . 2387:// handler is the main implementation of Handler. . . 2388:// The path is known to be in canonical form, except for CONNECT methods. 10ms 10ms 2389:func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { . . 2390:\tmux.mu.RLock() . . 2391:\tdefer mux.mu.RUnlock() . . 2392: . . 2393:\t// Host-specific pattern takes precedence over generic ones . . 2394:\tif mux.hosts { 可以看出，main.handler 总共用时 2.63s，其中 regexp.MustComplie 耗时 2.38ms。 罪魁祸首在这里：服务器每次收到请求，都要重新解析正则表达式。\n进入下一步，使用 web 命令查看调用链，需要提前安装 graphviz 工具，自动调用浏览器展示下图：\n图示与之前看到的调用链图基本一致，聚焦业务代码 main.handler， 可以发现regexp.Compile 耗时明显，因为每次请求过来，都要重新编译正则表达式。\n5. 参考资料 # 深度解密 Go 语言之 pprof 曹大 pprof Russ Cox 优化过程，并附上代码 google pprof 使用 pprof 和火焰图调试 golang 应用 资源合集 Profiling your Golang app in 3 steps 案例，压测 Golang remote profiling and flamegraphs 煎鱼 pprof 鸟窝 pprof 关于 Go 的 7 种性能分析方法 pprof 比较全 通过实例来讲解分析、优化过程 wolfogre 非常精彩的实战文章 实战案例 大彬 实战内存泄露 查找内存泄露 "},{"id":6,"href":"/docs/kubernetes/kube-apiserver/authorization/","title":"鉴权机制","section":"kube-apisever","content":" 1. 鉴权概述 # 在客户端请求通过认证后，会进入鉴权阶段，kube-apiserver 同样支持多种鉴权机制，并支持同时开启多个鉴权模块。 如果开启多个鉴权模块，则按照顺序执行鉴权模块，排在前面的鉴权模块有较高的优先级来允许或者拒绝请求。 只要有一个鉴权模块通过，则鉴权成功。\nkube-apiserver 目前提供了 6 种鉴权机制：\nAlwaysAllow：总是允许 AlwaysDeny：总是拒绝 ABAC：基于属性的访问控制（Attribute-Based Access Control） Node：节点鉴权，专门鉴权给 kubelet 发出的 API 请求 RBAC：基于角色额的访问控制（Role-Based Access Control） Webhook：基于 webhook 的一种 HTTP 回调机制，可以进行远程鉴权管理 在学习鉴权之前，有三个概念需要补齐：\nDecision：决策状态 Authorizer：鉴权接口 RuleResolver：规则解析器 1.1 Decision：决策状态 # Decision 决策状态类似于认证中的 true 和 false，用于决定是否鉴权成功。 鉴权支持三种 Decision 决策状态，例如鉴权成功，则返回 DecisionAllow，代码如下：\n// staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go type Decision int const ( DecisionDeny Decision = iota DecisionAllow DecisionNoOpinion ) DecisionDeny：拒绝该操作 DecisionAllow：允许该操作 DecisionNoOpinion：表示无明显意见允许或拒绝，继续执行下一个鉴权模块 1.2 Authorizer：鉴权接口 # 每个鉴权模块都要实现该接口的方法，代码如下：\n// staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go type Authorizer interface { Authorize(ctx context.Context, a Attributes) (authorized Decision, reason string, err error) } Authorizer 接口定义了 Authorize 方法，该方法接收一个 Attribute 参数。 Attributes 是决定鉴权模块从 HTTP 请求中获取鉴权信息方法的参数，它是一个方法集合的接口， 例如 GetUser、GetVerb、GetNamespace、GetResource 等鉴权信息方法。 如果鉴权成功，Decision 状态变成 DecisionAllow， 如果鉴权失败，Decision 状态变成 DecisionDeny，并返回失败的原因。\nAttributes 定义如下：\n// staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go type Attributes interface { GetUser() user.Info GetVerb() string IsReadOnly() bool GetNamespace() string GetResource() string GetSubresource() string GetName() string GetAPIGroup() string GetAPIVersion() string IsResourceRequest() bool GetPath() string } 1.3 RuleResolver：规则解析器 # 鉴权模块通过 RuleResolver 解析规则，定义如下：\n// staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go type RuleResolver interface { RulesFor(user user.Info, namespace string) ([]ResourceRuleInfo, []NonResourceRuleInfo, bool, error) } RuleResolver 接口定义的 RulesFor 方法，所有鉴权模块都要实现。 RulesFor 方法接受 user 用户信息和 namespace 命名空间参数，解析出规则列表并返回。 规则列表分成两种：\nResourceRuleInfo：资源类型的规则列表，例如 /api/v1/pods 的资源接口 NonResourceRuleInfo：非资源类型的规则列表，例如 /healthz 的非资源接口 以 ResourceRuleInfo 资源类型为例，定义如下：\n// staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go type DefaultResourceRuleInfo struct { Verbs []string APIGroups []string Resources []string ResourceNames []string } Pod 资源规则列表示例如下，其中通配符（*）表示匹配所有，该规则表示用户对所有资源版本的 Pod 拥有所有操作权限， 即：get、list、watch、create、update、patch、delete、deletecollection。\nresourcesRules: []authorizer.ResourceRuleInfo { \u0026amp;authorizer.DefaultResourceRuleInfo { Verbs: []string{\u0026#34;*\u0026#34;} APIGroups: []string{\u0026#34;*\u0026#34;} Resources: []string{\u0026#34;pods\u0026#34;} } } 每一种鉴权机制实例化后，成为一个鉴权模块，被封装在 http.Handler 函数中，他们接受组件或者客户端的请求并鉴权。 假设 kube-apiserver 启用了 Node 鉴权模块 和 RBAC 鉴权模块。 请求会进入 Authorization Handler 函数，该函数会遍历已经启用的鉴权模块列表，按照顺序执行每个鉴权模块， 例如在 Node 鉴权模块返回 DecisionNoOpinion 时，会继续执行 RBAC 鉴权模块。代码如下：\n// staging/src/k8s.io/apiserver/pkg/authorization/union/union.go func (authzHandler unionAuthzHandler) Authorize(ctx context.Context, a authorizer.Attributes) (authorizer.Decision, string, error) { var ( errlist []error reasonlist []string ) for _, currAuthzHandler := range authzHandler { decision, reason, err := currAuthzHandler.Authorize(ctx, a) if err != nil { errlist = append(errlist, err) } if len(reason) != 0 { reasonlist = append(reasonlist, reason) } switch decision { case authorizer.DecisionAllow, authorizer.DecisionDeny: return decision, reason, err case authorizer.DecisionNoOpinion: // continue to the next authorizer } } return authorizer.DecisionNoOpinion, strings.Join(reasonlist, \u0026#34;\\n\u0026#34;), utilerrors.NewAggregate(errlist) } 2. ABAC 鉴权 # 2.1 概述 # ABAC 授权器是基于属性的访问控制（Attributed-Based Access Control，ABAC）定义了访问控制范例， 其中通过属性组合在一起的策略来向用户授予操作权限。\n2.2 启用 # --authorization-mode=ABAC：启用 ABAC 授权器 --authorization-policy-file：基于 ABAC 模式，指定策略文件， 该文件使用 JSON Lines 格式描述，每行都是一个策略对象。 ABAC 模式策略文件定义：\n{\u0026#34;apiVersion\u0026#34;: \u0026#34;abac.authorization.kubernetes.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;Deck\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;apiGroup\u0026#34;: \u0026#34;*\u0026#34;}} 上面的策略，Deck 用户可以对所有资源做任何操作。\n2.3 实现 # // pkg/auth/authorizer/abac/abac.go func (pl PolicyList) Authorize(ctx context.Context, a authorizer.Attributes) (authorizer.Decision, string, error) { for _, p := range pl { if matches(*p, a) { return authorizer.DecisionAllow, \u0026#34;\u0026#34;, nil } } return authorizer.DecisionNoOpinion, \u0026#34;No policy matched.\u0026#34;, nil } 进行 ABAC 策略授权时，遍历所有策略，通过 matches 函数判断是否匹配，有一个策略满足，返回 DecisionAllow 决策状态。\n此外，ABAC 的规则解析器会根据每个策略，将资源类型的规则列表（ResoureceRuleInfo）和非资源类型的规则列表（NonResourceInfo） 都设置为该用户有权限操作的资源版本、资源和资源操作方法。 代码如下：\n// pkg/auth/authorizer/abac/abac.go func (pl PolicyList) RulesFor(user user.Info, namespace string) ([]authorizer.ResourceRuleInfo, []authorizer.NonResourceRuleInfo, bool, error) { var ( resourceRules []authorizer.ResourceRuleInfo nonResourceRules []authorizer.NonResourceRuleInfo ) for _, p := range pl { if subjectMatches(*p, user) { if p.Spec.Namespace == \u0026#34;*\u0026#34; || p.Spec.Namespace == namespace { if len(p.Spec.Resource) \u0026gt; 0 { r := authorizer.DefaultResourceRuleInfo{ Verbs: getVerbs(p.Spec.Readonly), APIGroups: []string{p.Spec.APIGroup}, Resources: []string{p.Spec.Resource}, } var resourceRule authorizer.ResourceRuleInfo = \u0026amp;r resourceRules = append(resourceRules, resourceRule) } if len(p.Spec.NonResourcePath) \u0026gt; 0 { r := authorizer.DefaultNonResourceRuleInfo{ Verbs: getVerbs(p.Spec.Readonly), NonResourceURLs: []string{p.Spec.NonResourcePath}, } var nonResourceRule authorizer.NonResourceRuleInfo = \u0026amp;r nonResourceRules = append(nonResourceRules, nonResourceRule) } } } } return resourceRules, nonResourceRules, false, nil } 3. RBAC 鉴权 # 3.1 概述 # RBAC 是基于角色的访问控制（Role-Based Access Control），是根据组织中用户的角色来控制对计算机或者网络资源访问的方法。 也是目前使用最为广泛的鉴权模型。在 RBAC 鉴权模块中，权限与角色相关联，形成了用户——角色——权限的鉴权模型。 用户通过加入某些角色从而的得到这些角色的操作权限，极大地简化了权限管理。RBAC 的鉴权图如下所示：\ngraph LR; A[用户] --\u003e B[角色] B --\u003e C[权限] API 对象\nRBAC API 声明了 4 种 Kubernetes 对象：Role、ClusterRole、RoleBinding、ClusterRoleBinding。\nRole 和 ClusterRole\nRole 和 ClusterRole 中包含一组代表相关权限的规则，这些权限是单纯的累加关系，不存在角色某种操作的规则。 这 2 种资源类型不同，是因为 kubernetes 的对象要么是命名空间作用域（Namespace Scope），要么是集群作用域（Cluster Scope）。\n数据结构如下： classDiagram class Role{ +metav1.TypeMeta +metav1.ObjectMeta +Rules []PolicyRule } class ClusterRole{ +metav1.TypeMeta +metav1.ObjectMeta +Rules []PolicyRule } class PolicyRule{ +Verbs []string +APIGroups []string +Resources []string +ResourceNames []string } Role \"1\"--\u003e\"n\" PolicyRule ClusterRole \"1\"--\u003e\"n\" PolicyRule Role：总是用来设置某个命名空间里的发个文权限。 ClusterRole：是集群作用域的资源。 Rule：规则相当于操作权限，控制资源的操作方法（即 Verbs） RoleBinding 和 ClusterRoleBinding # 数据结构如下： classDiagram class RoleBinding{ +metav1.TypeMeta +metav1.ObjectMeta +Subjects []Subeject +RoleRef RoleRef } class ClusterRoleBinding{ +metav1.TypeMeta +metav1.ObjectMeta +Subjects []Subeject +RoleRef RoleRef } class Subject{ +Kind string +APIGroup string +Name string +Namespace string } class RoleRef{ +APIGroup string +Kind string +Name string } RoleBinding \"1\" --\u003e\"n\" Subject RoleBinding \"1\"--\u003e\"1\" RoleRef ClusterRoleBinding \"1\"--\u003e\"n\" Subject ClusterRoleBinding \"1\"--\u003e\"1\" RoleRef RoleRef：被授予权限的角色的引用 Subject：主体可以是用户、组或者服务账户 RoleBinding：将角色中定义的权限授予一个或者一组用户，只能用于某一个命名空间的权限 clusterRoleBinding：将集群角色的中定义的权限首页一个或者一组用户没，可用于集群范围的权限 3.2 启用 # kube-apiserver 通过指定以下参数启用 Node 鉴权\n--authorization-mode=RBAC 3.3 实现 # kube-apiserver 通过表示用户、操作、角色、角色绑定来描述 RBAC 的关系。模型图如下：\n+--------+ +-------------+ +--------+ +-------------+ | User | | RoleBinding | | Role | | Operation | +--------+ +-------------+ +--------+ +-------------+ | User-A |\u0026lt;-------| |--------\u0026gt;| |--------\u0026gt;| Operation-A | +--------+ | Binding-A | | Role-A | +-------------+ | User-B |\u0026lt;-------| |--------\u0026gt;| |--------\u0026gt;| Operation-B | +--------+ +-------------+ +--------+ +-------------+ | User-C |\u0026lt;-------| |--------\u0026gt;| |--------\u0026gt;| Operation-C | +--------+ | Binding-B | | Role-B | +-------------+ | User-D |\u0026lt;-------| |--------\u0026gt;| |--------\u0026gt;| Operation-D | +--------+ +-------------+ +--------+ +-------------+ Role-A 角色拥有 Operation-A 和 Operation-B 的权限，Binding-A 将用户 User-A 与角色 Role-A 绑定， 因此 User-A 就有了 Operation-A 和 Operation-B 的权限，但没有 Operation-C 和 Operation-D 的权限。\n// plugin/pkg/auth/authorizer/rbac/rbac.go func (r *RBACAuthorizer) Authorize(ctx context.Context, requestAttributes authorizer.Attributes) (authorizer.Decision, string, error) { ruleCheckingVisitor := \u0026amp;authorizingVisitor{requestAttributes: requestAttributes} // ruleCheckingVisitor.visit -\u0026gt; RulesAllow -\u0026gt; 各种 match 函数验证 r.authorizationRuleResolver.VisitRulesFor(requestAttributes.GetUser(), requestAttributes.GetNamespace(), ruleCheckingVisitor.visit) if ruleCheckingVisitor.allowed { return authorizer.DecisionAllow, ruleCheckingVisitor.reason, nil } ... return authorizer.DecisionNoOpinion, reason, nil } 进行 RBAC 鉴权时，首先调用 ruleCheckingVisitor.visit 验证授权，该函数返回的 allowed 字段为 true，表示授权成功。 ruleCheckingVisitor.visit 会调用 RBAC 的 RulesAllows 函数，该函数是实际验证函数的合集。 鉴权的原理如下所示：\ngraph LR A{IsResourceRequest} --\u003e |Yes| B[VerbMacthes] B --\u003e C[APIGroupMacthes] C --\u003e D[ResourceMacthes] D --\u003e E[ResourceNameMacthes] A --\u003e|No| F[VerbMatchs] F --\u003e G[NonResourceMacthes] 3.4 内置 ClusterRole # 介绍内置角色之前，先了解下 kube-apiserver 的内置权限，内置的角色会引用内置权限，代码示例如下：\n// plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go var ( Write = []string{\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;deletecollection\u0026#34;} ReadWrite = []string{\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;deletecollection\u0026#34;} Read = []string{\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;} ReadUpdate = []string{\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;} ) Write：只写 ReadWrite：读写 Read：只读 ReadUpdate：只读和更新 kube-apiserver 在启动时，会默认创建内置角色，例如：cluster-admin，它拥有最高权限；定义如下：\n// plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go { // a \u0026#34;root\u0026#34; role which can do absolutely anything ObjectMeta: metav1.ObjectMeta{Name: \u0026#34;cluster-admin\u0026#34;}, Rules: []rbacv1.PolicyRule{ rbacv1helpers.NewRule(\u0026#34;*\u0026#34;).Groups(\u0026#34;*\u0026#34;).Resources(\u0026#34;*\u0026#34;).RuleOrDie(), rbacv1helpers.NewRule(\u0026#34;*\u0026#34;).URLs(\u0026#34;*\u0026#34;).RuleOrDie(), }, }, cluster-admin 的定义中，将资源类型和非资源类型都设置为通配符（*），匹配所有版本和资源，与集群角色 system:master 绑定。 在 plugin/pkg/auth/authorizer/rbac/bootstrappolicy 目录下，有关所有内置的角色和权限配置信息，具体可以分为三类： API 发现角色、面向用户角色和面向组件角色。\nDiscover Roles\n无论是经过身份验证的还是未经过身份验证的用户，默认的角色绑定都授权他们读取被认为是可安全地公开访问的 API（包括 CustomResourceDefinitions）。 如果要禁用匿名的未经过身份验证的用户访问，请在 API 服务器配置中中添加 \u0026ndash;anonymous-auth=false 的配置选项。\nClusterRole ClusterRoleBinding 说明 system:basic-user system:authenticated 允许用户以只读的方式去访问他们自己的基本信息 system:discovery system:authenticated 允许以只读方式访问 API discovery endpoints system:public-info-viewer system:authenticated 和 system:unauthenticated 允许对集群的非敏感信息进行只读访问 User-facing Roles\n面向用户的角色，包含超级用户角色（cluster-admin），即在集群范围内授权的角色， 以及那些使用 使用 RoleBinding（admin、edit、view）在特定命名空间空间中授予的角色。\nClusterRole ClusterRoleBinding 说明 cluster-admin system:masters 超级用户权限，允许对任何资源执行任何操作 admin None 允许管理员访问权限，旨在使用 RoleBinding 在命名空间内执行授权。如果在 RoleBinding 中使用，则可授予对命名空间中的大多数资源的读/写权限，包括创建角色和角色绑定的能力。但是它不允许对资源配额或者命名空间本身进行写操作。 edit None 允许对命名空间的大多数对象进行读/写操作。它不允许查看或者修改角色或者角色绑定。不过，此角色可以访问 Secret，以命名空间中任何 ServiceAccount 的身份运行 Pods，所以可以用来了解命名空间内所有服务账号的 API 访问级别。 view None 允许对某个命名空间内大部分的对象进行只读访问，但不允许查看 Role 或者 RoleBinding。由于可扩展行等原因，不允许查看 Secret 资源 Component Roles\n核心组件角色： ClusterRole ClusterRoleBinding 说明 system:kube-scheduler system:kube-scheduler 允许访问 scheduler 组件所需要的资源 system:volume-scheduler system:kube-scheduler 允许访问 kube-scheduler 组件所需要的卷资源 system:kube-controller-manager system:kube-controller-manager 允许访问控制器管理器组件所需要的资源。 system:node None 允许访问 kubelet 所需要的资源，包括对所有 Secret 的读操作和对所有 Pod 状态对象的写操作 system:node-proxier system:kube-proxy 允许访问 kube-proxy 组件所需要的资源 其他组件角色： ClusterRole ClusterRoleBinding 说明 system:auth-delegator 无 委托身份认证和鉴权检查。这种角色通常用在插件式 API 服务器上，以实现统一的身份认证和鉴权。 system:heapster 无 为 Heapster 组件（已弃用）定义的角色 system:kube-aggregator 无 为 kube-aggregator 组件定义的角色 system:kube-dns kube-dns 为 kube-dns 组件定义的角色 system:kubelet-api-admin 无 允许 kubelet API 的完全访问权限 system:node-bootstrapper 无 允许访问执行 kubelet TLS 启动引导 所需要的资源 system:node-problem-detector 无 为 node-problem-detector 组件定义的角色 system:persistent-volume-provisioner 无 允许访问大部分动态卷驱动（dynamic volume driver）所需要的资源 Controller Roles\nKubernetes 控制器管理器 运行内建于 Kubernetes 控制面的控制器。 当使用 --use-service-account-credentials 参数启动时， kube-controller-manager 使用单独的服务账号来启动每个控制器。 每个内置控制器都有相应的、前缀为 system:controller: 的角色。 如果控制管理器启动时未设置 --use-service-account-credentials， 它使用自己的身份凭据来运行所有的控制器，该身份必须被授予所有相关的角色。 这些角色包括：\nsystem:controller:attachdetach-controller system:controller:certificate-controller system:controller:clusterrole-aggregation-controller system:controller:cronjob-controller system:controller:daemon-set-controller system:controller:deployment-controller system:controller:disruption-controller system:controller:endpoint-controller system:controller:expand-controller system:controller:generic-garbage-collector system:controller:horizontal-pod-autoscaler system:controller:job-controller system:controller:namespace-controller system:controller:node-controller system:controller:persistent-volume-binder system:controller:pod-garbage-collector system:controller:pv-protection-controller system:controller:pvc-protection-controller system:controller:replicaset-controller system:controller:replication-controller system:controller:resourcequota-controller system:controller:root-ca-cert-publisher system:controller:route-controller system:controller:service-account-controller system:controller:service-controller system:controller:statefulset-controller system:controller:ttl-controller 4. Node 鉴权 # 4.1 概述 # Node 鉴权，也成为节点鉴权，是一种专门针对 kubelet 发出的请求进行鉴权。 Node 鉴权机制基于 RBAC 授权机制实现，对 kubelet 组件进行基于 system:node 内置角色的权限控制。 system:node 内置角色的权限定义在 NodeRules 函数中，具体可以移步：\n// plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go // NodeRules returns node policy rules, it is slice of rbacv1.PolicyRule. func NodeRules() []rbacv1.PolicyRule { nodePolicyRules := []rbacv1.PolicyRule{ // Needed to check API access. These creates are non-mutating rbacv1helpers.NewRule(\u0026#34;create\u0026#34;).Groups(authenticationGroup).Resources(\u0026#34;tokenreviews\u0026#34;).RuleOrDie(), rbacv1helpers.NewRule(\u0026#34;create\u0026#34;).Groups(authorizationGroup).Resources(\u0026#34;subjectaccessreviews\u0026#34;, \u0026#34;localsubjectaccessreviews\u0026#34;).RuleOrDie(), ... } } 在上面的代码中，允许 kubelet 执行以下操作： 读取操作：\nservices endpoints nodes pods secrets、configmaps、pvc 以及绑定到 kubelet 节点的与 pod 相关的持久卷 写入操作：\n节点和节点状态（启用 NodeRestriction 准入插件以限制 kubelet 只能修改自己的节点） pod 和 pod 状态（启用 NodeRestriction 准入插件以限制 kubelet 只能修改绑定到自身的 Pod） 事件 鉴权相关：\n对于基于 TLS 的启动引导过程时使用的 certificationsigningrequests API 的读写权限 为委派的身份验证/鉴权检查创建 tokenreviews 和 subjectaccessreviews 的能力 4.2 启用 # kube-apiserver 通过指定以下参数启用 Node 鉴权\n--authorization-mode = Node,RBAC 4.3 实现 # // plugin/pkg/auth/authorizer/node/node_authorizer.go func (r *NodeAuthorizer) Authorize(ctx context.Context, attrs authorizer.Attributes) (authorizer.Decision, string, error) { // 获取 node 角色信息 nodeName, isNode := r.identifier.NodeIdentity(attrs.GetUser()) if !isNode { // reject requests from non-nodes return authorizer.DecisionNoOpinion, \u0026#34;\u0026#34;, nil } if len(nodeName) == 0 { // reject requests from unidentifiable nodes klog.V(2).Infof(\u0026#34;NODE DENY: unknown node for user %q\u0026#34;, attrs.GetUser().GetName()) return authorizer.DecisionNoOpinion, fmt.Sprintf(\u0026#34;unknown node for user %q\u0026#34;, attrs.GetUser().GetName()), nil } // 特定资源的请求走这里 if attrs.IsResourceRequest() { requestResource := schema.GroupResource{Group: attrs.GetAPIGroup(), Resource: attrs.GetResource()} switch requestResource { case secretResource: return r.authorizeReadNamespacedObject(nodeName, secretVertexType, attrs) case configMapResource: return r.authorizeReadNamespacedObject(nodeName, configMapVertexType, attrs) case pvcResource: if r.features.Enabled(features.ExpandPersistentVolumes) { if attrs.GetSubresource() == \u0026#34;status\u0026#34; { return r.authorizeStatusUpdate(nodeName, pvcVertexType, attrs) } } return r.authorizeGet(nodeName, pvcVertexType, attrs) case pvResource: return r.authorizeGet(nodeName, pvVertexType, attrs) case vaResource: return r.authorizeGet(nodeName, vaVertexType, attrs) case svcAcctResource: return r.authorizeCreateToken(nodeName, serviceAccountVertexType, attrs) case leaseResource: return r.authorizeLease(nodeName, attrs) case csiNodeResource: if r.features.Enabled(features.CSINodeInfo) { return r.authorizeCSINode(nodeName, attrs) } return authorizer.DecisionNoOpinion, fmt.Sprintf(\u0026#34;disabled by feature gates %s\u0026#34;, features.CSINodeInfo), nil } } // 非资源请求走这里 if rbac.RulesAllow(attrs, r.nodeRules...) { return authorizer.DecisionAllow, \u0026#34;\u0026#34;, nil } return authorizer.DecisionNoOpinion, \u0026#34;\u0026#34;, nil } Node 鉴权时，通过 r.identifier.NodeIdentity 获取角色信息， 验证其是否为 system:node 内置角色， nodeName 的格式为 system:node\u0026lt;nodeName\u0026gt; 。 资源类型请求，取出资源对象，内置了允许操作资源和操作方法。 非资源类型请求，走 rbac.RulesAllow 进行 RBAC 授权。\n5. Webhook # 5.1 概述 # webhook 是一种 HTTP 回调：当用户鉴权时，kube-apiserver 组件会查询外部的 webhook 服务。 该过程与 webhookTokenAuth 认证相似，但其中确认用户身份的机制不一样。 当客户端发送的认证请求到达 kube-apiserver 时，kube-apiserver 回调钩子方法， 将鉴权信息发给远程的 webhook 服务器进行认证，根据 webhook 服务返回的状态判断是否授权成功。\n5.2 启用 # --authorization-mode=Webhook：启用 webhook 授权器 --authorization-webhook-config-file：使用 kubeconfig 格式的 webhook 配置文件。 webhook 的配置文件如下：\n# Kubernetes API 版本 apiVersion: v1 # API 对象种类 kind: Config # clusters 代表远程服务 clusters: - name: name-of-remote-authz-service cluster: # 对远程服务进行身份认证的 CA certificate-authority: /path/to/ca.pem # 远程服务的查询 URL。必须使用 \u0026#39;https\u0026#39; server: https://authz.example.com/authorize # users 代表 API 服务器的 webhook 配置 users: - name: name-of-api-server user: client-certificate: /path/to/cert.pem # webhook plugin 使用 cert client-key: /path/to/key.pem # cert 所对应的 key # kubeconfig 文件必须有 context，需要提供一个给 API 服务器 current-context: webhook contexts: - context: cluster: name-of-remote-authz-service user: name-of-api-server name: webhook 如上所示，users 指的是 kube-apiserver 本身，cluster 指的是远程 webhook 服务。\n5.3 实现 # // staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go func (w *WebhookAuthorizer) Authorize(ctx context.Context, attr authorizer.Attributes) (decision authorizer.Decision, reason string, err error) { ... // 尝试从缓存中查找该请求 if entry, ok := w.responseCache.Get(string(key)); ok { r.Status = entry.(authorizationv1.SubjectAccessReviewStatus) } else { var ( result *authorizationv1.SubjectAccessReview err error ) webhook.WithExponentialBackoff(ctx, w.retryBackoff, func() error { // 首次请求 webhook result, err = w.subjectAccessReview.Create(ctx, r, metav1.CreateOptions{}) return err }, webhook.DefaultShouldRetry) ... r.Status = result.Status // 写缓存 if shouldCache(attr) { if r.Status.Allowed { w.responseCache.Add(string(key), r.Status, w.authorizedTTL) } else { w.responseCache.Add(string(key), r.Status, w.unauthorizedTTL) } } } switch { case r.Status.Denied \u0026amp;\u0026amp; r.Status.Allowed: return authorizer.DecisionDeny, r.Status.Reason, fmt.Errorf(\u0026#34;webhook subject access review returned both allow and deny response\u0026#34;) case r.Status.Denied: return authorizer.DecisionDeny, r.Status.Reason, nil case r.Status.Allowed: return authorizer.DecisionAllow, r.Status.Reason, nil default: return authorizer.DecisionNoOpinion, r.Status.Reason, nil } } 上面的代码可以看出，鉴权时，首先查询缓存，是否已经解析过此请求的鉴权。 如果有，直接使用该状态（Status），如果没有，create 一个 SubjectAccessView， 从远程的 webhook 服务器获取鉴权结果。w.subjectAccessReview.Create 是一个 POST 请求， body 体中携带鉴权信息，r.Status.Allowed 为 true，表示鉴权成功。\n此外，webhook 不支持规则列表解析，因为规则是由 webhook 服务器授权的。 所以，webhook 的规则解析器的资源类型列表（ResourceRulesInfo）和非资源类型规则列表（NonResourceRulesInfo）都设置为空。\nfunc (w *WebhookAuthorizer) RulesFor(user user.Info, namespace string) ([]authorizer.ResourceRuleInfo, []authorizer.NonResourceRuleInfo, bool, error) { var ( resourceRules []authorizer.ResourceRuleInfo nonResourceRules []authorizer.NonResourceRuleInfo ) incomplete := true return resourceRules, nonResourceRules, incomplete, fmt.Errorf(\u0026#34;webhook authorizer does not support user rule resolution\u0026#34;) } 6. 参考资料 # 鉴权概述 ABAC 鉴权 RBAC 鉴权 Node 鉴权 webhook 模式 "},{"id":7,"href":"/docs/golang/language-basics/unsafe/","title":"unsafe","section":"语言基础","content":" 本文转自： Go 里面的 unsafe 包详解\nThe unsafe Package in Golang # Golang 的 unsafe 包是一个很特殊的包。为什么这样说呢？本文将详细解释。\n来自 go 语言官方文档的警告 # unsafe 包的文档是这么说的：\n导入 unsafe 的软件包可能不可移植，并且不受 Go 1 兼容性指南的保护。 Go 1 兼容性指南这么说：\n导入 unsafe 软件包可能取决于 Go 实现的内部属性。我们保留对可能导致程序崩溃的实现进行更改的权利。 当然包名称暗示 unsafe 包是不安全的。但这个包有多危险呢？让我们先看看 unsafe 包的作用。\nUnsafe 包的作用 # 直到现在（Go1.7），unsafe 包含以下资源：\n三个函数： func Alignof(variable ArbitraryType) uintptr func Offsetof(selector ArbitraryType) uintptr func Sizeof(variable ArbitraryType) uintptr 和一种类型： 类型 Pointer *ArbitraryType 这里，ArbitraryType 不是一个真正的类型，它只是一个占位符。\n与 Golang 中的大多数函数不同，上述三个函数的调用将始终在编译时求值，而不是运行时。这意味着它们的返回结果可以分配给常量。\nunsafe 包中的函数中非唯一调用将在编译时求值。 当传递给 len 和 cap 的参数是一个数组值时，内置函数和 cap 函数的调用也可以在编译时被求值。 除了这三个函数和一个类型外，指针在 unsafe 包也为编译器服务。\n出于安全原因，Golang 不允许以下之间的直接转换：\n两个不同指针类型的值，例如 int64 和 float64。 指针类型和 uintptr 的值。 但是借助 unsafe.Pointer，我们可以打破 Go 类型和内存安全性，并使上面的转换成为可能。 这怎么可能发生？让我们阅读 unsafe 包文档中列出的规则：\n任何类型的指针值都可以转换为 unsafe.Pointer。 unsafe.Pointer 可以转换为任何类型的指针值。 uintptr 可以转换为 unsafe.Pointer。 unsafe.Pointer 可以转换为 uintptr。 这些规则与 Go 规范一致：\n底层类型 uintptr 的任何指针或值都可以转换为指针类型，反之亦然。 规则表明 unsafe.Pointer 类似于 c 语言中的 void* 。当然，void* 在 C 语言里是危险的！\n在上述规则下，对于两种不同类型 T1 和 T2，可以使 T1 值与 unsafe.Pointer 值一致， 然后将 unsafe.Pointer 值转换为 T2 值（或 uintptr 值）。 通过这种方式可以绕过 Go 类型系统和内存安全性。当然，滥用这种方式是很危险的。\n举个例子：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { var n int64 = 5 var pn = \u0026amp;n var pf = (*float64)(unsafe.Pointer(pn)) // now, pn and pf are pointing at the same memory address fmt.Println(*pf) // 2.5e-323 *pf = 3.14159 fmt.Println(n) // 4614256650576692846 } 在这个例子中的转换可能是无意义的，但它是安全和合法的（为什么它是安全的？）。\n因此，资源在 unsafe 包中的作用是为 Go 编译器服务，unsafe.Pointer 类型的作用是绕过 Go 类型系统和内存安全。\nunsafe.Pointer 和 uintptr # 这里有一些关于 unsafe.Pointer 和 uintptr 的事实：\nuintptr 是一个整数类型。 即使 uintptr 变量仍然有效，由 uintptr 变量表示的地址处的数据也可能被 GC 回收。 unsafe.Pointer是一个指针类型。 但是 unsafe.Pointer 值不能被取消引用。 如果 unsafe.Pointer 变量仍然有效，则由 unsafe.Pointer 变量表示的地址处的数据不会被 GC 回收。 unsafe.Pointer 是一个通用的指针类型，就像 *int 等。 由于 uintptr 是一个整数类型，uintptr 值可以进行算术运算。 所以通过使用 uintptr 和 unsafe.Pointer，我们可以绕过限制，*T 值不能在 Golang 中计算偏移量：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { a := [4]int{0, 1, 2, 3} p1 := unsafe.Pointer(\u0026amp;a[1]) p3 := unsafe.Pointer(uintptr(p1) + 2 * unsafe.Sizeof(a[0])) *(*int)(p3) = 6 fmt.Println(\u0026#34;a =\u0026#34;, a) // a = [0 1 2 6] // ... type Person struct { name string age int gender bool } who := Person{\u0026#34;John\u0026#34;, 30, true} pp := unsafe.Pointer(\u0026amp;who) pname := (*string)(unsafe.Pointer(uintptr(pp) + unsafe.Offsetof(who.name))) page := (*int)(unsafe.Pointer(uintptr(pp) + unsafe.Offsetof(who.age))) pgender := (*bool)(unsafe.Pointer(uintptr(pp) + unsafe.Offsetof(who.gender))) *pname = \u0026#34;Alice\u0026#34; *page = 28 *pgender = false fmt.Println(who) // {Alice 28 false} } unsafe 包有多危险 # 关于 unsafe 包，Ian，Go 团队的核心成员之一，已经确认：\n在 unsafe 包中的函数的签名将不会在以后的 Go 版本中更改， 并且 unsafe.Pointer 类型将在以后的 Go 版本中始终存在。 所以，unsafe 包中的三个函数看起来不危险。 go team leader 甚至想把它们放在别的地方。 unsafe 包中这几个函数唯一不安全的是它们调用结果可能在后来的版本中返回不同的值。 很难说这种不安全是一种危险。\n看起来所有的 unsafe 包的危险都与使用 unsafe.Pointer 有关。 unsafe 包 docs 列出了一些使用 unsafe.Pointer 合法或非法的情况。 这里只列出部分非法使用案例：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) // case A: conversions between unsafe.Pointer and uintptr // don\u0026#39;t appear in the same expression func illegalUseA() { fmt.Println(\u0026#34;===================== illegalUseA\u0026#34;) pa := new([4]int) // split the legal use // p1 := unsafe.Pointer(uintptr(unsafe.Pointer(pa)) + unsafe.Sizeof(pa[0])) // into two expressions (illegal use): ptr := uintptr(unsafe.Pointer(pa)) p1 := unsafe.Pointer(ptr + unsafe.Sizeof(pa[0])) // \u0026#34;go vet\u0026#34; will make a warning for the above line: // possible misuse of unsafe.Pointer // the unsafe package docs, https://golang.org/pkg/unsafe/#Pointer, // thinks above splitting is illegal. // but the current Go compiler and runtime (1.7.3) can\u0026#39;t detect // this illegal use. // however, to make your program run well for later Go versions, // it is best to comply with the unsafe package docs. *(*int)(p1) = 123 fmt.Println(\u0026#34;*(*int)(p1) :\u0026#34;, *(*int)(p1)) // } // case B: pointers are pointing at unknown addresses func illegalUseB() { fmt.Println(\u0026#34;===================== illegalUseB\u0026#34;) a := [4]int{0, 1, 2, 3} p := unsafe.Pointer(\u0026amp;a) p = unsafe.Pointer(uintptr(p) + uintptr(len(a)) * unsafe.Sizeof(a[0])) // now p is pointing at the end of the memory occupied by value a. // up to now, although p is invalid, it is no problem. // but it is illegal if we modify the value pointed by p *(*int)(p) = 123 fmt.Println(\u0026#34;*(*int)(p) :\u0026#34;, *(*int)(p)) // 123 or not 123 // the current Go compiler/runtime (1.7.3) and \u0026#34;go vet\u0026#34; // will not detect the illegal use here. // however, the current Go runtime (1.7.3) will // detect the illegal use and panic for the below code. p = unsafe.Pointer(\u0026amp;a) for i := 0; i \u0026lt;= len(a); i++ { *(*int)(p) = 123 // Go runtime (1.7.3) never panic here in the tests fmt.Println(i, \u0026#34;:\u0026#34;, *(*int)(p)) // panic at the above line for the last iteration, when i==4. // runtime error: invalid memory address or nil pointer dereference p = unsafe.Pointer(uintptr(p) + unsafe.Sizeof(a[0])) } } func main() { illegalUseA() illegalUseB() } 编译器很难检测 Go 程序中非法的 unsafe.Pointer 使用。 运行 go vet 可以帮助找到一些潜在的错误，但不是所有的都能找到。 同样是 Go 运行时，也不能检测所有的非法使用。 非法 unsafe.Pointer 使用可能会使程序崩溃或表现得怪异（有时是正常的，有时是异常的）。 这就是为什么使用不安全的包是危险的。\n转换 T1 为 T2 # 对于将 T1 转换为 unsafe.Pointer，然后转换为 T2，unsafe 包 docs 说：\n如果 T2 比 T1 大，并且两者共享等效内存布局，则该转换允许将一种类型的数据重新解释为另一类型的数据。 这种“等效内存布局”的定义是有一些模糊的。看起来 Go 团队故意如此。这使得使用 unsafe 包更危险。\n由于 Go 团队不愿意在这里做出准确的定义，本文也不尝试这样做 这里，列出了已确认的合法用例的一小部分。\n合法用例 1：在 []T 和 []MyT 之间转换 # 在这个例子里，我们用 int 作为 T：\ntype MyInt int 在 Golang 中，[]int 和 []MyInt 是两种不同的类型，它们的底层类型是自身。 因此，[]int 的值不能转换为 []MyInt，反之亦然。 但是在 unsafe.Pointer 的帮助下，转换是可能的：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) func main() { type MyInt int a := []MyInt{0, 1, 2} // b := ([]int)(a) // error: cannot convert a (type []MyInt) to type []int b := *(*[]int)(unsafe.Pointer(\u0026amp;a)) b[0]= 3 fmt.Println(\u0026#34;a =\u0026#34;, a) // a = [3 1 2] fmt.Println(\u0026#34;b =\u0026#34;, b) // b = [3 1 2] a[2] = 9 fmt.Println(\u0026#34;a =\u0026#34;, a) // a = [3 1 9] fmt.Println(\u0026#34;b =\u0026#34;, b) // b = [3 1 9] } 合法用例 2：调用 sync/atomic 包中指针相关的函数 # sync/atomic 包中的以下函数的大多数参数和结果类型都是 unsafe.Pointer 或 *unsafe.Pointer：\nfunc CompareAndSwapPointer(addr *unsafe.Pointer, old，new unsafe.Pointer) (swapped bool) func LoadPointer(addr *unsafe.Pointer) (val unsafe.Pointer) func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer) 要使用这些功能，必须导入 unsafe 包。\n注意： unsafe.Pointer 是一般类型，因此 unsafe.Pointer 的值可以转换为 unsafe.Pointer，反之亦然。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;unsafe\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;math/rand\u0026#34; ) var data *string // get data atomically func Data() string { p := (*string)(atomic.LoadPointer( (*unsafe.Pointer)(unsafe.Pointer(\u0026amp;data)), )) if p == nil { return \u0026#34;\u0026#34; } else { return *p } } // set data atomically func SetData(d string) { atomic.StorePointer( (*unsafe.Pointer)(unsafe.Pointer(\u0026amp;data)), unsafe.Pointer(\u0026amp;d), ) } func main() { var wg sync.WaitGroup wg.Add(200) for range [100]struct{}{} { go func() { time.Sleep(time.Second * time.Duration(rand.Intn(1000)) / 1000) log.Println(Data()) wg.Done() }() } for i := range [100]struct{}{} { go func(i int) { time.Sleep(time.Second * time.Duration(rand.Intn(1000)) / 1000) s := fmt.Sprint(\u0026#34;#\u0026#34;, i) log.Println(\u0026#34;====\u0026#34;, s) SetData(s) wg.Done() }(i) } wg.Wait() fmt.Println(\u0026#34;final data = \u0026#34;, *data) } 结论 # unsafe 包用于 Go 编译器，而不是 Go 运行时。 使用 unsafe 作为程序包名称只是让你在使用此包是更加小心。 使用 unsafe.Pointer 并不总是一个坏主意，有时我们必须使用它。 Golang 的类型系统是为了安全和效率而设计的。 但是在 Go 类型系统中，安全性比效率更重要。 通常 Go 是高效的，但有时安全真的会导致 Go 程序效率低下。 unsafe 包用于有经验的程序员通过安全地绕过 Go 类型系统的安全性来消除这些低效。 unsafe 包可能被滥用并且是危险的。 "},{"id":8,"href":"/docs/golang/data-structure/chan/","title":"chan","section":"数据结构","content":" 1. 引言 # Go 语言中最常见的、也是经常被人提及的设计模式就是 —— 不要通过共享内存的方式进行通信，而是应该通过通信的方式共享内存。\n先入先出\n目前的 Channel 收发操作均遵循了先入先出（FIFO）的设计，具体规则如下：\n先从 Channel 读取数据的 Goroutine 会先接收到数据； 先向 Channel 发送数据的 Goroutine 会得到先发送数据的权利； 2. 数据结构 # type hchan struct { qcount uint // 当前队列中剩余元素个数 dataqsiz uint // 环形队列长度，即可以存放的元素个数 buf unsafe.Pointer // 环形队列指针 elemsize uint16 // 每个元素的大小 closed uint32 // 标识关闭状态 elemtype *_type // 元素类型 sendx uint // 发送操作处理到的位置 recvx uint // 接收操作处理到的位置 recvq waitq // 等待读消息的 goroutine 队列 sendq waitq // 等待写消息的 goroutine 队列 lock mutex // 互斥锁，chan 不允许并发读写 } 从数据结构可以看出 channel 由队列、类型信息、goroutine 等待队列组成，下面分别说明其原理。\n2.1 环形队列 # chan 内部实现了一个环形队列作为其缓冲区，队列的长度是创建 chan 时指定的。\n下图展示了一个可缓存 6 个元素的 channel 示意图：\n+-------------+ | hchan | +-------------+ | qcount=2 | +-------------+ | dataqsiz=6 | +-------------+ +-----------------------+ | buf |------\u0026gt;| 0 | 1 | 1 | 0 | 0 | 0 | +-------------+ +-----^-------^---------+ | sendx=3 |-------------|-------| +-------------+ | | recvq=1 |-------------+ +-------------+ dataqsiz 指示了队列长度为 6，即可缓存 6 个元素； buf 指向队列的内存，队列中还剩余两个元素； qcount 表示队列中还有两个元素； sendx 指示后续写入的数据存储的位置，取值 [0, 6)； recvx 指示从该位置读取数据，取值 [0, 6)； 2.2 等待队列 # 从 channel 读数据，如果 channel 缓冲区为空或者没有缓冲区，当前 goroutine 会被阻塞。 向 channel 写数据，如果 channel 缓冲区已满或者没有缓冲区，当前 goroutine 会被阻塞。\n被阻塞的 goroutine 将会挂在 channel 的等待队列中：\n因读阻塞的 goroutine 会被向 channel 写入数据的 goroutine 唤醒； 因写阻塞的 goroutine 会被从 channel 读数据的 goroutine 唤醒； 下图展示了一个没有缓冲区的 channel，有几个 goroutine 阻塞等待读数据：\n+-------------+ | hchan | +-------------+ | qcount=0 | +-------------+ | dataqsiz=0 | +-------------+ | buf | +-------------+ | sendx=0 | +-------------+ | recvx=0 | +-------------+ +---+ +---+ +---+ | sendq |---\u0026gt;| G |---\u0026gt;| G |---\u0026gt;| G | +-------------+ +---+ +---+ +---+ | recvq | +-------------+ 注意，一般情况下 recvq 和 sendq 至少有一个为空。只有一个例外， 那就是同一个 goroutine 使用 select 语句向 channel 一边写数据，一边读数据。\n2.3 类型信息 # 一个 channel 只能传递一种类型的值，类型信息存储在 hchan 数据结构中。\nelemtype 代表类型，用于数据传递过程中的赋值； elemsize 代表类型大小，用于在 buf 中定位元素位置。 2.4 锁 # 一个 channel 同时仅允许被一个 goroutine 读写。\n3. channel 读写 # 3.1 创建 channel # 创建 channel 的过程实际上是初始化 hcha 结构。其中类型信息和缓冲区长度由 make 语句传入， buf 的大小则与元素 大小和缓冲区长度共同决定。\n创建 channel 的部分代码如下所示，只保留了核心创建逻辑：\nfunc makechan(t *chantype, size int) *hchan { elem := t.elem // ... mem, overflow := math.MulUintptr(elem.size, uintptr(size)) // ... var c *hchan switch { case mem == 0: c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(\u0026amp;c.lock, lockRankHchan) // ... return c } 3.2 发送数据 # 向一个 channel 中发送数据简单过程如下：\n如果等待接收队列 recvq 不为空，说明缓冲区中没有数据或者没有缓冲区， 此时直接从 recvq 取出 G, 并把数据写入，最后把该 G 唤醒，结束发送过程； 如果缓冲区中有空余位置，将数据写入缓冲区，结束发送过程； 如果缓冲区中没有空余位置，将待发送数据写入 G，将当前 G 加入 sendq，进入睡眠，等待被读 goroutine 唤醒； 简单的流程图总结如下： graph TD start(开始发送) --\u003e B{recq非空?} B --\u003e |Y| C[从recvq取出一个G] C --\u003e C1[数据写入G] C1 --\u003e C2[唤醒G] C2 --\u003e H(结束发送) B --\u003e |N| D{buf有空位?} D --\u003e |Y| E[将数据写入buf队尾] E --\u003e H D --\u003e |N| F[将当前goroutine加入senq,等待被唤醒] F -.-\u003e G[被唤醒,数据被取走] G -.-\u003e H 3.3 接收数据 # 从一个 channel 接收数据简单过程如下：\n如果等待发送队列 sendq 不为空，且没有缓冲区，直接从 sendq 中取出 G， 把 G 中数据读出，最后把 G 唤醒，结束读取过程； 如果等待发送队列 sendq 不为空，此时说明缓冲区已满，从缓冲区中首部读出数据， 把 G 中数据写入缓冲区尾部，把 G 唤醒，结束读取过程； 如果缓冲区中有数据，则从缓冲区取出数据，结束读取过程； 将当前 goroutine 加入 recvq，进入睡眠，等待被写 goroutine 唤醒； 简单的流程图总结如下： graph TD A(开始接收) --\u003e B{sendq非空?} B --\u003e |N| F{qcount\u003e0?} F --\u003e |Y| f0[从buf队头取数据] f0 --\u003e Z F --\u003e |N| f1[将当前goroutine假如有recq,等待被唤醒] f1 -.-\u003e f2[被唤醒,数据已写入] f2 -.-\u003e Z B --\u003e |Y| C{有缓冲区?} C --\u003e |Y| d0[从buf队头取数据] d0 --\u003e d1[从sendq中取出G] d1 --\u003e d2[把G中数据写入buf队尾] d2 --\u003e d3[唤醒G] d3 --\u003e Z(结束接收) C --\u003e |N| d1 d1 --\u003e e0[从G中读出数据] e0 --\u003e d3 3.4 关闭 channel # 关闭 channel 时会把 recvq 中的 G 全部唤醒，本该写入 G 的数据位置为 nil。把 sendq 中的 G 全部唤醒，但这些 G 会 panic。 除此之外，panic 出现的常见场景还有：\n关闭值为 nil 的 channel 关闭已经被关闭的 channel 向已经关闭的 channel 写数据 4. 参考资料 # Go Channel 实现原理精要 Go 专家编程：1.1 chan "},{"id":9,"href":"/docs/leetcode/0321/","title":"321. 拼接最大数","section":"LeetCode","content":" 321. 拼接最大数 # leetcode 链接： https://leetcode-cn.com/problems/create-maximum-number/\n给定长度分别为 m 和 n 的两个数组，其元素由 0-9 构成，表示两个自然数各位上的数字。现在从这两个数组中选出 k (k \u0026lt;= m + n) 个数字拼接成一个新的数，要求从同一个数组中取出的数字保持其在原数组中的相对顺序。\n求满足该条件的最大数。结果返回一个表示该最大数的长度为 k 的数组。\n说明：请尽可能地优化你算法的时间和空间复杂度。\n示例 1:\n输入： nums1 = [3, 4, 6, 5] nums2 = [9, 1, 2, 5, 8, 3] k = 5 输出： [9, 8, 6, 5, 3] 示例 2:\n输入： nums1 = [6, 7] nums2 = [6, 0, 4] k = 5 输出： [6, 7, 6, 0, 4] 示例 3:\n输入： nums1 = [3, 9] nums2 = [8, 9] k = 3 输出： [9, 8, 9] 方法一：单调栈\n为了求得长度为 k 的最大子序列，需要从两个数组中分别选出最大的子序列，然后将这两个子序列合并得到最大数。两个子序列的长度最小为 0，最大不能超过 k 且不能超过对应的数组长度。所以整个算法就分成三步：\n分别计算 num1 和 nums2 的最大子序列 按照字典序降序合并子序列 保存对应的整数较大的子序列 func maxNumber(nums1 []int, nums2 []int, k int) []int { initialSize := 0 if len(nums2) \u0026lt; k { initialSize = k - len(nums2) } var ans []int for size := initialSize; size \u0026lt;= k \u0026amp;\u0026amp; size \u0026lt;= len(nums1); size++ { sub1 := maxSubSequence(nums1, size) sub2 := maxSubSequence(nums2, k-size) merged := merge(sub1, sub2) if lexicographicalLess(ans, merged) { ans = merged } } return ans } // 按照字典序降序合并 func merge(sub1, sub2 []int) []int { var merged = make([]int, len(sub1)+len(sub2)) for i := range merged { if lexicographicalLess(sub1, sub2) { merged[i], sub2 = sub2[0], sub2[1:] } else { merged[i], sub1 = sub1[0], sub1[1:] } } return merged } // 判断字典序大小 func lexicographicalLess(sub1, sub2 []int) bool { for i := 0; i \u0026lt; len(sub1) \u0026amp;\u0026amp; i \u0026lt; len(sub2); i++ { if sub1[i] != sub2[i] { return sub1[i] \u0026lt; sub2[i] } } return len(sub1) \u0026lt; len(sub2) } // 单调栈求指定长度的最大子序列 func maxSubSequence(nums []int, k int) []int { var sub []int for i := range nums { for len(sub) \u0026gt; 0 \u0026amp;\u0026amp; nums[i] \u0026gt; sub[len(sub)-1] \u0026amp;\u0026amp; len(sub)+len(nums)-i-1 \u0026gt;= k { sub = sub[:len(sub)-1] } if len(sub) \u0026lt; k { sub = append(sub, nums[i]) } } return sub } "},{"id":10,"href":"/docs/kubernetes/kube-apiserver/authentication/","title":"认证机制","section":"kube-apisever","content":" 1. Kubernetes 中的用户 # 所有 Kubernetes 集群都有两类用户：由 Kubernetes 管理的 ServiceAccount 和普通用户。\n对于与普通用户，Kuernetes 使用以下方式管理：\n负责分发私钥的管理员 类似 Keystone 或者 Google Accounts 这类用户数据库 包含用户名和密码列表的文件 因此，kubernetes 并不提供普通用户的定义，普通用户是无法通过 API 调用写入到集群中的。\n尽管如此，通过集群的证书机构签名的合法证书的用户，kubernetes 依旧可以认为是合法用户。基于此，kubernetes 使用证书中的 subject.CommonName 字段来确定用户名，接下来，通过 RBAC 确认用户对某资源是否存在要求的操作权限。\n与此不同的 ServiceAccount，与 Namespace 绑定，与一组 Secret 所包含的凭据有关。这些凭据会挂载到 Pod 中，从而允许访问 kubernetes 的 API。\nAPI 请求要么与普通用户相关，要么与 ServiceAccount 相关，其他的视为匿名请求。这意味着集群内和集群外的每个进程向 kube-apiserver 发起请求时，都必须通过身份认证，否则会被视为匿名用户。\n2. 认证机制 # 目前 kubernetes 提供的认证机制丰富多样，尤其是身份验证，更是五花八门：\n身份验证 X509 Client Cert Static Token File Bootstrap Tokens Static Password File（deprecated in v1.16） ServiceAccount Token OpenID Connect Token Webhook Token Authentication Proxy 匿名请求 用户伪装 client-go 凭据插件 2.1 身份验证策略 # 2.1.1 X509 Client Cert # X509 客户端证书认证，也被称为 TLS 双向认证，即为服务端和客户端互相验证证书的正确性。使用此认证方式，只要是 CA 签名过的证书都能通过认证。\n启用\nkube-apiserver 通过指定 --client-ca-file 参数启用此认证方式。\n认证接口\n// staging/src/k8s.io/apiserver/pkg/authentication/authenticator/interfaces.go type Request interface { AuthenticateRequest(req *http.Request) (*Response, bool, error) } 该方法接收客户端请求。若验证失败，bool 返回 false，验证成功，bool 返回 true，Response 中携带身份验证用户的信息，例如 Name、UID、Groups、Extra。\n认证实现\n// staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go func (a *Authenticator) AuthenticateRequest(req *http.Request) (*authenticator.Response, bool, error) { if req.TLS == nil || len(req.TLS.PeerCertificates) == 0 { return nil, false, nil } // Use intermediates, if provided optsCopy, ok := a.verifyOptionsFn() // if there are intentionally no verify options, then we cannot authenticate this request if !ok { return nil, false, nil } if optsCopy.Intermediates == nil \u0026amp;\u0026amp; len(req.TLS.PeerCertificates) \u0026gt; 1 { optsCopy.Intermediates = x509.NewCertPool() for _, intermediate := range req.TLS.PeerCertificates[1:] { optsCopy.Intermediates.AddCert(intermediate) } } remaining := req.TLS.PeerCertificates[0].NotAfter.Sub(time.Now()) clientCertificateExpirationHistogram.Observe(remaining.Seconds()) // 校验证书，如果通过，可解析 user 信息 chains, err := req.TLS.PeerCertificates[0].Verify(optsCopy) if err != nil { return nil, false, err } var errlist []error for _, chain := range chains { user, ok, err := a.user.User(chain) if err != nil { errlist = append(errlist, err) continue } if ok { return user, ok, err } } return nil, false, utilerrors.NewAggregate(errlist) } 2.1.2 Static Token File # Token 也被称为令牌，服务端为了验证客户端身份，需要客户端向服务端提供一个可靠的验证信息，这个验证信息就是 Token。目前，令牌会长期有效，并且在不重启 API 服务器的情况下 无法更改令牌列表。\n启用\nkube-apiserver 通过指定 --token-auth-file 参数启用，令牌文件是一个 CSV 文件，包含至少 3 个列：令牌、用户名和用户的 UID。 其余列被视为可选的组名。示例如下：\ntoken,user,uid,\u0026#34;group1,group2,group3\u0026#34; 请求头配置\n在 HTTP 请求头中，设置 Authentication 的值，格式为 Bearer $TOKEN，格式如下：\nAuthorization: Bearer 31ada4fd-adec-460c-809a-9e56ceb75269 认证实现\n// staging/src/k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go func (a *TokenAuthenticator) AuthenticateToken(ctx context.Context, value string) (*authenticator.Response, bool, error) { user, ok := a.tokens[value] if !ok { return nil, false, nil } return \u0026amp;authenticator.Response{User: user}, true, nil } 该认证方式相对简单，a.tokens 保存了服务端 Token 列表，通过 map 查询客户端提供的 Token 是否存在，存在即认证成功，反之则认证失败。\n2.1.3 Bootstrap Tokens # Bootstrap Token 是一种简单的 Bearer Token，这种令牌是在新建集群或者在现在集群中加入节点时使用。一般是由 kubeadm 管理，以 secret 形式保存在 kube-system 命名空间，可以动态地创建删除，并且 kube-controller-manager 中 TokenCleaner 会在 Token 过期时删除。该能力目前依旧是 alpha 阶段，但官方预期也不会有大的突破性变化。\n启用\nkube-apiserver 设置 --enable-bootstrap-token 启动 Bootstrap Token 身份认证，并且依赖 kube-controller-manager 设置 --controllers=*,tokencleaner,bootstrapsigner 启动 TokenCleaner 和 BootstrapSigner。\n请求头配置\nToken 的格式为 [a-z0-9]{6}.[a-z0-9]{16}，第一部分是 token id，第二部分是 token 的 secret。可以用如下方式设置 HTTP Header：\nAuthorization: Bearer 781292.db7bc3a58fc5f07e 认证实现\n// plugin/pkg/auth/authenticator/token/bootstrap/bootstrap.go func (t *TokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { // 1. 校验 token 格式 tokenID, tokenSecret, err := bootstraptokenutil.ParseToken(token) if err != nil { return nil, false, nil } // 2. 拼接 secret name，获取 secret 对象 secretName := bootstrapapi.BootstrapTokenSecretPrefix + tokenID secret, err := t.lister.Get(secretName) if err != nil { if errors.IsNotFound(err) { klog.V(3).Infof(\u0026#34;No secret of name %s to match bootstrap bearer token\u0026#34;, secretName) return nil, false, nil } return nil, false, err } // 3. 校验 secret 有效，不在删除中 if secret.DeletionTimestamp != nil { tokenErrorf(secret, \u0026#34;is deleted and awaiting removal\u0026#34;) return nil, false, nil } // 4. 校验 secret 类型必须是 bootstrap.kubernetes.io/token if string(secret.Type) != string(bootstrapapi.SecretTypeBootstrapToken) || secret.Data == nil { tokenErrorf(secret, \u0026#34;has invalid type, expected %s.\u0026#34;, bootstrapapi.SecretTypeBootstrapToken) return nil, false, nil } // 5. 校验 token secret 有效 ts := bootstrapsecretutil.GetData(secret, bootstrapapi.BootstrapTokenSecretKey) if subtle.ConstantTimeCompare([]byte(ts), []byte(tokenSecret)) != 1 { tokenErrorf(secret, \u0026#34;has invalid value for key %s, expected %s.\u0026#34;, bootstrapapi.BootstrapTokenSecretKey, tokenSecret) return nil, false, nil } // 6. 校验 token id 有效 id := bootstrapsecretutil.GetData(secret, bootstrapapi.BootstrapTokenIDKey) if id != tokenID { tokenErrorf(secret, \u0026#34;has invalid value for key %s, expected %s.\u0026#34;, bootstrapapi.BootstrapTokenIDKey, tokenID) return nil, false, nil } // 7. 校验 token 是否过期 if bootstrapsecretutil.HasExpired(secret, time.Now()) { // logging done in isSecretExpired method. return nil, false, nil } // 8. 校验 secret 对象的 data 字段中，key 为 usage-bootstrap-authentication，value 为 true if bootstrapsecretutil.GetData(secret, bootstrapapi.BootstrapTokenUsageAuthentication) != \u0026#34;true\u0026#34; { tokenErrorf(secret, \u0026#34;not marked %s=true.\u0026#34;, bootstrapapi.BootstrapTokenUsageAuthentication) return nil, false, nil } // 9. 获取 secret.data[auth-extra-groups]，与 default group 组合 groups, err := bootstrapsecretutil.GetGroups(secret) if err != nil { tokenErrorf(secret, \u0026#34;has invalid value for key %s: %v.\u0026#34;, bootstrapapi.BootstrapTokenExtraGroupsKey, err) return nil, false, nil } return \u0026amp;authenticator.Response{ User: \u0026amp;user.DefaultInfo{ Name: bootstrapapi.BootstrapUserPrefix + string(id), Groups: groups, }, }, true, nil } 2.1.4 ServiceAccount Token # 其他认证方式都是从 kubernetes 集群外部访问 kube-apiserver 组件，而 ServiceAccount 是从 Pod 内部访问，提供给 Pod 中的进程使用。ServiceAccount 包含了 3 个部分的内容：\nNamespace：指定 Pod 所在的命名空间 CA：kube-apiserver CA 公钥证书，是 Pod 内部进程对 kube-apiserver 进行验证的证书 Token：用于身份验证，通过 kube-apiserver 私钥签发经过 Base64 编码的 Bearer Token 他们都通过 mount 命令挂载到 Pod 的文件系统中，Namespace 存储在 /var/run/secrets/kubernetes.io/serviceaccount/namespace，经过 Base64 加密；CA 的存储路径 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt；Token 存储在 /var/run/secrets/kubernetes.io/serviceaccount/token 文件中。\n启用\nkube-apiserver 指定以下参数启用\n--service-account-key-file ：包含用来给 Bearer Token 签名的 PEM 编码密钥，如果未指定，使用 kube-apiserver 的 TLS 私钥。 --service-account-lookup：用于验证 service account token 是否存在 etcd 中，默认为 true。 配置\nServiceAccount 通常是 kube-apiserver 自动创建，并通过准入控制器关联到 Pod 中。当然也可以在 Pod.spec.serviceAccountName 显示地指定。\n认证实现\n// pkg/serviceaccount/jwt.go func (j *jwtTokenAuthenticator) AuthenticateToken(ctx context.Context, tokenData string) (*authenticator.Response, bool, error) { // 1. 校验 token 格式正确 if !j.hasCorrectIssuer(tokenData) { return nil, false, nil } // 2. 解析 JWT 对象 tok, err := jwt.ParseSigned(tokenData) ... public := \u0026amp;jwt.Claims{} private := j.validator.NewPrivateClaims() // TODO: Pick the key that has the same key ID as `tok`, if one exists. var ( found bool errlist []error ) // 3. 使用--service-account-key-file 提供的密钥，反序列化 JWT for _, key := range j.keys { if err := tok.Claims(key, public, private); err != nil { errlist = append(errlist, err) continue } found = true break } ... // 4. 验证 namespace 是否正确、serviceAccountName、serviceAccountID 是否存在，token 是否失效 sa, err := j.validator.Validate(ctx, tokenData, public, private) if err != nil { return nil, false, err } return \u0026amp;authenticator.Response{ User: sa.UserInfo(), Audiences: auds, }, true, nil } 服务账号被身份认证后，所确定的用户名为 system:serviceaccount:\u0026lt;NAMESPACE\u0026gt;:\u0026lt;SERVICEACCOUNT\u0026gt;， 并被分配到用户组 system:serviceaccounts 和 system:serviceaccounts:\u0026lt;NAMESPACE\u0026gt;。\n2.1.5 OpenID Connect Token # OpenID Connect Token(OIDC) 是一套基于 OAuth2.0 协议的轻量级认证规范，其提供了通过 API 进行身份交互的框架。OIDC 认证除了认证请求外，还会标明请求的用户身份（ID Token）。其中 Token 被称为 ID Token，此 ID Token 是 JWT，具有服务器签名的相关字段。认证流程如下：\n用户想要访问 kube-apiserver，先通过认证服务（Auth Service，例如 Google Accounts 服务）认证自己，得到 access_token、id_token 和 refresh_token。 用户把 access_token、id_token 和 refresh_token 配置到客户端应用程序，例如：kubectl 或者 dashboard 工具 客户端使用 Token 以用户身份访问 kube-apiserver kube-apiserver 和 Auth Service 没有直接交互，而是鉴定客户端发送过来的 Token 是否合法。完整的 OIDC 认证过程如下图所示：\nsequenceDiagram participant user as 用户 participant idp as 身份提供者 participant kube as Kubectl participant api as API 服务器 user -\u003e\u003e idp: 1. 登录到 IdP activate idp idp --\u003e\u003e user: 2. 提供 access_token,\nid_token, 和 refresh_token deactivate idp activate user user -\u003e\u003e kube: 3. 调用 Kubectl 并\n设置 --token 为 id_token\n或者将令牌添加到 .kube/config deactivate user activate kube kube -\u003e\u003e api: 4. Authorization: Bearer... deactivate kube activate api api -\u003e\u003e api: 5. JWT 签名合法么？ api -\u003e\u003e api: 6. JWT 是否已过期？(iat+exp) api -\u003e\u003e api: 7. 用户被授权了么？ api --\u003e\u003e kube: 8. 已授权：执行\n操作并返回结果 deactivate api activate kube kube --x user: 9. 返回结果 deactivate kube 登录到身份服务（即 Auth Server） 身份服务将为你提供 access_token、id_token 和 refresh_token 用户在使用 kubectl 时，将 id_token 设置为 --token 标志值，或者将其直接添加到 kubeconfig 中 kubectl 将 id_token 设置为 Authorization 的请求头，发送给 API 服务器 API 服务器将负责通过检查配置中引用的证书来确认 JWT 的签名是合法的 检查确认 id_token 尚未过期 确认用户有权限执行操作 鉴权成功之后，API 服务器向 kubectl 返回响应 kubectl 向用户提供反馈信息 kube-apiserver 不与 Auth Service 交互就可以认证 Token 的合法性，关键在于第 5 步，所有 JWT 都由颁发给它的 Auth Service 进行了数字签名，只需要在 kube-apiserver 的启动参数中，配置信任的 Auth Server 证书，用它来验证 id_token 是否合法。\n启用 --oidc-ca-file：指向一个 CA 证书的路径，该 CA 负责对你的身份服务的 Web 证书提供签名。默认值为宿主系统的根 CA（/etc/kubernetes/ssl/kc-ca.pem）。 --oidc-client-id：所有令牌都应发放给此客户 ID。 --oidc-groups-claim：JWT 声明的用户组名称。 --oidc-groups-prefix：添加到组申领的前缀，用来避免与现有用户组名（如：system: 组）发生冲突。例如，此标志值为 oidc: 时，所得到的用户组名形如 oidc:engineering 和 oidc:infra。 --oidc-issuer-url：允许 API 服务器发现公开的签名密钥的服务的 URL。只接受模式为 https:// 的 URL。此值通常设置为服务的发现 URL，不含路径。例如：\u0026ldquo; https://accounts.google.com\u0026rdquo; 或 \u0026ldquo; https://login.salesforce.com\u0026rdquo;。此 URL 应指向 .well-known/openid-configuration 下一层的路径。 --oidc-required-claim：取值为一个 key=value 偶对，意为 ID 令牌中必须存在的申领。如果设置了此标志，则 ID 令牌会被检查以确定是否包含取值匹配的申领。此标志可多次重复，以指定多个申领。 --oidc-username-claim：JWT 声明的用户名称。默认情况下使用 sub 值，即最终用户的一个唯一的标识符。 --oidc-username-prefix：要添加到用户名申领之前的前缀，用来避免与现有用户名（例如：system: 用户）发生冲突。 认证实现 // staging/src/k8s.io/apiserver/plugin/pkg/authenticator/token/oidc/oidc.go func (a *Authenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { ... idToken, err := verifier.Verify(ctx, token) ... return \u0026amp;authenticator.Response{User: info}, true, nil } 整个认证逻辑，大体上是解析 id_token，把其中的 user、group 信息取出，组成 User 对象返回。在返回之前，要对针对 kube-apiserver 的各个 reqiured_claims 入参校验，看看从 id_token 中的值是否匹配。\n2.1.6 Webhook Token # webhook 也被称为钩子，是一种基于 HTTP 协议的回调机制，当客户端发送的认证请求到达 kube-apiserver 时，kubbe-apiserver 回调钩子方法，将验证信息发送给远程的 webhook 服务器进行验证，让根据返回的状态码判断是否认证通过。\n启用 --authentication-token-webhook-config-file：指向一个配置文件，其中描述 如何访问远程的 Webhook 服务。 --authentication-token-webhook-cache-ttl：用来设定身份认证决定的缓存时间。默认时长为 2 分钟。 配置文件使用 kubeconfig 文件的格式。文件中，clusters 指代远程服务，users 指代远程 API 服务 Webhook。下面是一个例子：\n# Kubernetes API 版本 apiVersion: v1 # API 对象类别 kind: Config # clusters 指代远程服务 clusters: - name: name-of-remote-authn-service cluster: certificate-authority: /path/to/ca.pem # 用来验证远程服务的 CA server: https://authn.example.com/authenticate # 要查询的远程服务 URL。必须使用 \u0026#39;https\u0026#39;。 # users 指代 API 服务的 Webhook 配置 users: - name: name-of-api-server user: client-certificate: /path/to/cert.pem # Webhook 插件要使用的证书 client-key: /path/to/key.pem # 与证书匹配的密钥 # kubeconfig 文件需要一个上下文（Context），此上下文用于本 API 服务器 current-context: webhook contexts: - context: cluster: name-of-remote-authn-service user: name-of-api-sever name: webhook 当客户端尝试在 API 服务器上使用持有者令牌完成身份认证（ 如前所述）时， 身份认证 Webhook 会用 POST 请求发送一个 JSON 序列化的对象到远程服务。 该对象是 authentication.k8s.io/v1beta1 组的 TokenReview 对象， 其中包含持有者令牌。 Kubernetes 不会强制请求提供此 HTTP 头部。\n要注意的是，Webhook API 对象和其他 Kubernetes API 对象一样，也要受到同一 版本兼容规则约束。 实现者要了解对 Beta 阶段对象的兼容性承诺，并检查请求的 apiVersion 字段， 以确保数据结构能够正常反序列化解析。此外，API 服务器必须启用 authentication.k8s.io/v1beta1 API 扩展组 （--runtime-config=authentication.k8s.io/v1beta1=true）。\n认证实现 // staging/src/k8s.io/apiserver/pkg/authentication/token/cache/cached_token_authenticator.go func (a *cachedTokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { record := a.doAuthenticateToken(ctx, token) if !record.ok || record.err != nil { return nil, false, record.err } for key, value := range record.annotations { audit.AddAuditAnnotation(ctx, key, value) } return record.resp, true, nil } a.doAuthenticateToken(ctx, token) 是认证过程的核心，首先从缓存中查找是否已认证，有则直接返回，没有调用远程 webhook 服务验证。\n// staging/src/k8s.io/apiserver/plugin/pkg/authenticator/token/webhook/webhook.go func (w *WebhookTokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { ... webhook.WithExponentialBackoff(ctx, w.initialBackoff, func() error { result, err = w.tokenReview.Create(ctx, r, metav1.CreateOptions{}) return err }, webhook.DefaultShouldRetry) ... if !r.Status.Authenticated { var err error if len(r.Status.Error) != 0 { err = errors.New(r.Status.Error) } return nil, false, err } ... return \u0026amp;authenticator.Response{ User: \u0026amp;user.DefaultInfo{ Name: r.Status.User.Username, UID: r.Status.User.UID, Groups: r.Status.User.Groups, Extra: extra, }, Audiences: auds, }, true, nil } 通过 w.tokenReview.Create 发送 POST 请求到远程 webhook 服务，并在 body 体中携带认真信息，根据返回值 Status.Authenticated 判断是否认证通过。\n2.1.7 Authentication Proxy # API 服务器可以配置成从请求的头部字段值（如 X-Remote-User）中辩识用户。这一设计是用来与某身份认证代理一起使用 API 服务器，代理负责设置请求的头部字段值。\n认证代理有几个列表，\n用户名列表：建议设置为 \u0026ldquo;X-Remote-User\u0026rdquo;。必选 组列表：建议设置为 \u0026ldquo;X-Remote-Group\u0026rdquo;。可选 额外列表：建议设置为 \u0026ldquo;X-Remote-Extra-\u0026quot;。可选 启用 --requestheader-client-ca-file：指定有效的客户端 CA 证书。 --requestheader-allowed-names：指定通用名称（Common Name） --requestheader-username-headers：指定用户名列表 --requestheader-group-headers：指定组名列表 --requestheader-extra-headers-prefix：指定额外列表 认证 // staging/src/k8s.io/apiserver/pkg/authentication/request/headerrequest/requestheader.go func (a *requestHeaderAuthRequestHandler) AuthenticateRequest(req *http.Request) (*authenticator.Response, bool, error) { // 用户信息 name := headerValue(req.Header, a.nameHeaders.Value()) if len(name) == 0 { return nil, false, nil } // 组信息 groups := allHeaderValues(req.Header, a.groupHeaders.Value()) // 额外信息 extra := newExtra(req.Header, a.extraHeaderPrefixes.Value()) ... return \u0026amp;authenticator.Response{ User: \u0026amp;user.DefaultInfo{ Name: name, Groups: groups, Extra: extra, }, }, true, nil } 在进行认证代理认证时，requestHeader 就是实现方式，分别从 HTTP Header 读出用户、组和额外信息，返回给客户端。\n2.2 其他 # 有关匿名请求、用户伪装和 client-go 插件代理，请移步官网： 用户认证\n"},{"id":11,"href":"/docs/kubernetes/kube-controller-manager/code-analysis-of-garbagecollector-controller/","title":"GC Controller 源码分析","section":"kube-controller-manager","content":" 1. 序言 # 垃圾回收相关，可参考 这里\n2. 源码解析 # GarbageCollectorController 负责回收集群中的资源对象，要做到这一点，首先得监控所有资源。gc controller 会监听集群中所有可删除资源的事件，这些事件会放到一个队列中，然后启动多个 worker 协程处理。对于删除事件，则根据删除策略删除对象；其他事件，更新对象之间的依赖关系。\n2.1 startGarbageCollectorController() # 首先来看 gc controller 的入口方法，也就是 kube-controller-manager 是如何启动它的。它的主要逻辑：\n判断是否启用 gc controller，默认是 true 初始化 clientset，使用 discoveryClient 获取集群中所有资源 注册不考虑 gc 的资源，默认为空 调用 garbagecollector.NewGarbageCollector() 方法 初始化 gc controller 对象 调用 garbageCollector.Run() 启动 gc controller，workers 默认是 20 调用 garbageCollector.Sync() 监听集群中的资源，当出现新的资源时，同步到 minitors 中 调用 garbagecollector.NewDebugHandler() 注册 debug 接口，用来提供集群内所有对象的关联关系； cmd/kube-controller-manager/app/core.go:538\nfunc startGarbageCollectorController(ctx ControllerContext) (http.Handler, bool, error) { // 1. 判断是否启用 gc controller，默认是 true if !ctx.ComponentConfig.GarbageCollectorController.EnableGarbageCollector { return nil, false, nil } // 2. 初始化 clientset gcClientset := ctx.ClientBuilder.ClientOrDie(\u0026#34;generic-garbage-collector\u0026#34;) config := ctx.ClientBuilder.ConfigOrDie(\u0026#34;generic-garbage-collector\u0026#34;) metadataClient, err := metadata.NewForConfig(config) if err != nil { return nil, true, err } // 3. 注册不考虑 gc 的资源，默认为空 ignoredResources := make(map[schema.GroupResource]struct{}) for _, r := range ctx.ComponentConfig.GarbageCollectorController.GCIgnoredResources { ignoredResources[schema.GroupResource{Group: r.Group, Resource: r.Resource}] = struct{}{} } // 4. 初始化 gc controller 对象 garbageCollector, err := garbagecollector.NewGarbageCollector( metadataClient, ctx.RESTMapper, ignoredResources, ctx.ObjectOrMetadataInformerFactory, ctx.InformersStarted, ) if err != nil { return nil, true, fmt.Errorf(\u0026#34;failed to start the generic garbage collector: %v\u0026#34;, err) } // 5. 启动 gc，workers 默认是 20 workers := int(ctx.ComponentConfig.GarbageCollectorController.ConcurrentGCSyncs) go garbageCollector.Run(workers, ctx.Stop) // Periodically refresh the RESTMapper with new discovery information and sync // the garbage collector. // 6. 监听集群中的资源，周期更新 go garbageCollector.Sync(gcClientset.Discovery(), 30*time.Second, ctx.Stop) // 7. 注册 debug 接口 return garbagecollector.NewDebugHandler(garbageCollector), true, nil } 在 startGarbageCollectorController() 中主要调用了 4 个方法：\ngarbagecollector.NewGarbageCollector() garbageCollector.Run() garbageCollector.Sync() garbagecollector.NewDebugHandler() 其中 garbagecollector.NewGarbageCollector() 只是初始化 GarbageCollector 和 GraphBuilder 对象，核心逻辑都在 Run() 和 Sync() 中，下面分别来看这几个方法分别做了那些事。 2.1.1 garbageCollector.Run() # garbageCollector.Run() 方法主要作用是启动生产者和消费者。生产者就是 monitors，监听集群中的资源对象，将产生的新事件分别放入 attemptToDelete 和 attemptToOrphan 两个队列中。消费者就是处理这 2 个队列中的事件，要么删除对象，要么更新对象依赖关系。该方法的核心在于 gc.dependencyGraphBuilder.Run() 启动生产者和 for 循环启动消费者。\npkg/controller/garbagecollector/garbagecollector.go:122\nfunc (gc *GarbageCollector) Run(workers int, stopCh \u0026lt;-chan struct{}) { ... // 1. 启动所有 monitors 即 informers，监听集群资源 // 并启动一个协程，处理 graphChanges 中事件， // 然后经过处理，分别放入 GraphBuilder 的 attemptToDelete 和 attemptToOrphan 两个队列中 go gc.dependencyGraphBuilder.Run(stopCh) // 2. 等待 informers 的 cache 同步完成 if !cache.WaitForNamedCacheSync(\u0026#34;garbage collector\u0026#34;, stopCh, gc.dependencyGraphBuilder.IsSynced) { return } klog.Infof(\u0026#34;Garbage collector: all resource monitors have synced. Proceeding to collect garbage\u0026#34;) // gc workers for i := 0; i \u0026lt; workers; i++ { // 3. 启动多个协程，处理 attemptToDelete 队列中的事件 go wait.Until(gc.runAttemptToDeleteWorker, 1*time.Second, stopCh) // 4. 启动多个协程，处理 attemptToOrphan 队列中的事件 go wait.Until(gc.runAttemptToOrphanWorker, 1*time.Second, stopCh) } \u0026lt;-stopCh } GraphBuilder GraphBuilder 在整个垃圾收集的过程中，起到了承上启下的作用。首先看下它的结构：\npkg/controller/garbagecollector/graph_builder.go:73\ntype GraphBuilder struct { restMapper meta.RESTMapper // monitor 就是 informer，一个 monitor list/watch 一种资源 monitors monitors monitorLock sync.RWMutex // 当 kube-controller-manager 中所有的 controllers 都启动后，就会被 close 掉 informersStarted \u0026lt;-chan struct{} // stopCh 是接收 shutdown 信号 stopCh \u0026lt;-chan struct{} // 当调用 GraphBuilder 的 run 方法时，running 会被设置为 true running bool metadataClient metadata.Interface // monitors 监听到的事件会放在 graphChanges 中 graphChanges workqueue.RateLimitingInterface // uidToNode 维护所有对象的依赖关系 uidToNode *concurrentUIDToNode // GarbageCollector 作为消费者要处理 attemptToDelete 和 attemptToOrphan 两个队列中的事件 attemptToDelete workqueue.RateLimitingInterface attemptToOrphan workqueue.RateLimitingInterface // absentOwnerCache 存放已知不存在的对象 absentOwnerCache *UIDCache sharedInformers controller.InformerFactory // 不需要被 gc 的资源，默认是空 ignoredResources map[schema.GroupResource]struct{} } 其中 uidToNode 字段作为维护对象之间依赖关系，比如创建一个 Deployment 时，会创建 ReplicaSet，ReplicaSet 才会创建 Pod。那么 Pod 的 owner 是 ReplicaSet，ReplicaSet 的 owner 是 Deployment。uidToNode 字段的结构定义如下：\npkg/controller/garbagecollector/graph.go:162\ntype concurrentUIDToNode struct { uidToNodeLock sync.RWMutex uidToNode map[types.UID]*node } type node struct { identity objectReference dependentsLock sync.RWMutex // dependents 保存的是 owner 是自己的对象集合 dependents map[*node]struct{} // this is set by processGraphChanges() if the object has non-nil DeletionTimestamp // and has the FinalizerDeleteDependents. deletingDependents bool deletingDependentsLock sync.RWMutex // this records if the object\u0026#39;s deletionTimestamp is non-nil. beingDeleted bool beingDeletedLock sync.RWMutex // 如果 virtual 为 true，表示这个对象不是通过 informer 观察到的，是虚拟构建的 virtual bool virtualLock sync.RWMutex // owners 保存的是自己的 owner owners []metav1.OwnerReference } GraphBuilder 主要有三个功能：\n监控集群中所有的可删除资源； 基于 informers 中的资源在 uidToNode 数据结构中维护着所有对象的依赖关系； 处理 graphChanges 中的事件并放到 attemptToDelete 和 attemptToOrphan 两个队列中； gc.dependencyGraphBuilder.Run() 继续回到 gc.dependencyGraphBuilder.Run() 方法，它的功能上文已经提到，就是启动生产者。代码如下：\npkg/controller/garbagecollector/graph_builder.go:290\nfunc (gb *GraphBuilder) Run(stopCh \u0026lt;-chan struct{}) { klog.Infof(\u0026#34;GraphBuilder running\u0026#34;) defer klog.Infof(\u0026#34;GraphBuilder stopping\u0026#34;) // 1. 设置 stop channel gb.monitorLock.Lock() gb.stopCh = stopCh gb.running = true gb.monitorLock.Unlock() // 2. 启动 monitor，除非 stop channel 收到信号 gb.startMonitors() // 调用 gb.runProcessGraphChanges，分类事件，放入 2 个队列中 // 此处为死循环，除非收到 stopCh 信号，否则下面的代码不会被执行到 wait.Until(gb.runProcessGraphChanges, 1*time.Second, stopCh) // 代码走到这里，说明 stopCh 收到了信号，停止所有 monitor gb.monitorLock.Lock() defer gb.monitorLock.Unlock() monitors := gb.monitors stopped := 0 for _, monitor := range monitors { if monitor.stopCh != nil { stopped++ close(monitor.stopCh) } } // reset monitors so that the graph builder can be safely re-run/synced. gb.monitors = nil klog.Infof(\u0026#34;stopped %d of %d monitors\u0026#34;, stopped, len(monitors)) } 继续看 gb.runProcessGraphChanges() 方法，这里是生产者的核心逻辑。总结一下：\n从 graphChanges 队列中取出一个 item 即 event； 获取 event 的 accessor，accessor 是一个 object 的 meta.Interface，里面包含访问 object meta 中所有字段的方法； 通过 accessor 获取 UID 判断 uidToNode 中是否存在该 object； 根据对象是否存在以及事件类型，分成三种情况\n若 uidToNode 中不存在该 node 且该事件是 addEvent 或 updateEvent 则为该 object 创建对应的 node，并调用 gb.insertNode() 将该 node 加到 uidToNode 中，然后将该 node 添加到其 owner 的 dependents 中，执行完 gb.insertNode() 中的操作后再调用 gb.processTransitions() 方法判断该对象是否处于删除状态，若处于删除状态会判断该对象是以 orphan 模式删除还是以 foreground 模式删除，若以 orphan 模式删除，则将该 node 加入到 attemptToOrphan 队列中，若以 foreground 模式删除则将该对象以及其所有 dependents 都加入到 attemptToDelete 队列中；\n若 uidToNode 中存在该 node 且该事件是 addEvent 或 updateEvent 此时可能是一个 update 操作，调用 referencesDiffs() 方法检查该对象的 OwnerReferences 字段是否有变化，若有变化\n调用 gb.addUnblockedOwnersToDeleteQueue() 将被删除以及更新的 owner 对应的 node 加入到 attemptToDelete 中，因为此时该 node 中已被删除或更新的 owner 可能处于删除状态且阻塞在该 node 处，此时有三种方式避免该 node 的 owner 处于删除阻塞状态，一是等待该 node 被删除，二是将该 node 自身对应 owner 的 OwnerReferences 字段删除，三是将该 node 的 OwnerReferences 字段中对应 owner 的 BlockOwnerDeletion 设置为 false； 更新该 node 的 owners 列表； 若有新增的 owner，将该 node 加入到新 owner 的 dependents 中； 若有被删除的 owner，将该 node 从已删除 owner 的 dependents 中删除；以上操作完成后，检查该 node 是否处于删除状态并进行标记，最后调用 gb.processTransitions() 方法检查该 node 是否要被删除； 举个例子，若以 foreground 模式删除 deployment 时，deployment 的 dependents 列表中有对应的 rs，那么 deployment 的删除会阻塞住等待其依赖 rs 的删除，此时 rs 有三种方法不阻塞 deployment 的删除操作，一是 rs 对象被删除，二是删除 rs 对象 OwnerReferences 字段中对应的 deployment，三是将 rs 对象 OwnerReferences 字段中对应的 deployment 配置 BlockOwnerDeletion 设置为 false，文末会有示例演示该操作。\n若该事件为 deleteEvent 首先从 uidToNode 中删除该对象，然后从该 node 所有 owners 的 dependents 中删除该对象，将该 node 所有的 dependents 加入到 attemptToDelete 队列中，最后检查该 node 的所有 owners，若有处于删除状态的 owner，此时该 owner 可能处于删除阻塞状态正在等待该 node 的删除，将该 owner 加入到 attemptToDelete 中； pkg/controller/garbagecollector/graph_builder.go:530\nfunc (gb *GraphBuilder) runProcessGraphChanges() { for gb.processGraphChanges() { } } func (gb *GraphBuilder) processGraphChanges() bool { // 1. 从 graphChanges 取出一个 event item, quit := gb.graphChanges.Get() if quit { return false } defer gb.graphChanges.Done(item) event, ok := item.(*event) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;expect a *event, got %v\u0026#34;, item)) return true } obj := event.obj accessor, err := meta.Accessor(obj) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;cannot access obj: %v\u0026#34;, err)) return true } klog.V(5).Infof(\u0026#34;GraphBuilder process object: %s/%s, namespace %s, name %s, uid %s, event type %v\u0026#34;, event.gvk.GroupVersion().String(), event.gvk.Kind, accessor.GetNamespace(), accessor.GetName(), string(accessor.GetUID()), event.eventType) // 2. 若存在 node 对象，从 uidToNode 中取出该 event 的 node 对象 existingNode, found := gb.uidToNode.Read(accessor.GetUID()) if found { existingNode.markObserved() } switch { // 3.1 若 event 为 add 或 update 类型且对应的 node 对象不存在时，表示新建对象 case (event.eventType == addEvent || event.eventType == updateEvent) \u0026amp;\u0026amp; !found: // 3.1.1 为 node 创建 event 对象 newNode := \u0026amp;node{ identity: objectReference{ OwnerReference: metav1.OwnerReference{ APIVersion: event.gvk.GroupVersion().String(), Kind: event.gvk.Kind, UID: accessor.GetUID(), Name: accessor.GetName(), }, Namespace: accessor.GetNamespace(), }, dependents: make(map[*node]struct{}), owners: accessor.GetOwnerReferences(), deletingDependents: beingDeleted(accessor) \u0026amp;\u0026amp; hasDeleteDependentsFinalizer(accessor), beingDeleted: beingDeleted(accessor), } // 3.1.2 在 uidToNode 中添加该 node 对象 gb.insertNode(newNode) // 3.1.3 检查对象的删除策略，并放到对应的队列中 gb.processTransitions(event.oldObj, accessor, newNode) // 3.2 若 event 为 add 或 update 类型且对应的 node 对象存在时，表示更新对象 case (event.eventType == addEvent || event.eventType == updateEvent) \u0026amp;\u0026amp; found: // 3.2.1 检查当前对象和老对象的 owner 的不同 added, removed, changed := referencesDiffs(existingNode.owners, accessor.GetOwnerReferences()) if len(added) != 0 || len(removed) != 0 || len(changed) != 0 { // 3.2.2 检查依赖变更是否解除了处于等待依赖删除的 owner 的阻塞状态 gb.addUnblockedOwnersToDeleteQueue(removed, changed) // 3.2.3 更新 node 本身 existingNode.owners = accessor.GetOwnerReferences() // 3.2.4 添加 node 到它的 owner 依赖中 gb.addDependentToOwners(existingNode, added) // 3.2.5 删除老的 owners 中的依赖 gb.removeDependentFromOwners(existingNode, removed) } if beingDeleted(accessor) { existingNode.markBeingDeleted() } // 3.2.6 检查对象的删除策略，并放到对应的队列中 gb.processTransitions(event.oldObj, accessor, existingNode) // 3.3 若为 delete event case event.eventType == deleteEvent: if !found { klog.V(5).Infof(\u0026#34;%v doesn\u0026#39;t exist in the graph, this shouldn\u0026#39;t happen\u0026#34;, accessor.GetUID()) return true } // 3.3.1 从 uidToNode 中删除该 node gb.removeNode(existingNode) existingNode.dependentsLock.RLock() defer existingNode.dependentsLock.RUnlock() if len(existingNode.dependents) \u0026gt; 0 { gb.absentOwnerCache.Add(accessor.GetUID()) } // 3.3.2 删除该 node 的 dependents for dep := range existingNode.dependents { gb.attemptToDelete.Add(dep) } // 3.3.2 删除该 node 处于删除阻塞状态的 owner for _, owner := range existingNode.owners { ownerNode, found := gb.uidToNode.Read(owner.UID) if !found || !ownerNode.isDeletingDependents() { continue } gb.attemptToDelete.Add(ownerNode) } } return true } 2.1.2 gc.runAttemptToDeleteWorker() # gc.runAttemptToDeleteWorker() 方法就是将 attemptToDelete 队列中的对象取出，并删除，如果删除失败则重进队列重试。\npkg/controller/garbagecollector/garbagecollector.go:285\nfunc (gc *GarbageCollector) runAttemptToDeleteWorker() { for gc.attemptToDeleteWorker() { } } func (gc *GarbageCollector) attemptToDeleteWorker() bool { // 取出对象 item, quit := gc.attemptToDelete.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit { return false } defer gc.attemptToDelete.Done(item) n, ok := item.(*node) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;expect *node, got %#v\u0026#34;, item)) return true } // 实际执行删除的方法 gc.attemptToDeleteItem() err := gc.attemptToDeleteItem(n) if err != nil { if _, ok := err.(*restMappingError); ok { klog.V(5).Infof(\u0026#34;error syncing item %s: %v\u0026#34;, n, err) } else { utilruntime.HandleError(fmt.Errorf(\u0026#34;error syncing item %s: %v\u0026#34;, n, err)) } // 删除失败则重新进入队列等待重试 gc.attemptToDelete.AddRateLimited(item) } else if !n.isObserved() { klog.V(5).Infof(\u0026#34;item %s hasn\u0026#39;t been observed via informer yet\u0026#34;, n.identity) // 如果 node 不是 informer 发现的，则也要删除 gc.attemptToDelete.AddRateLimited(item) } return true } gc.runAttemptToDeleteWorker() 中调用了 gc.attemptToDeleteItem() 执行实际的删除操作。下面继续来看 gc.attemptToDeleteItem() 的实现细节：\n判断 node 是否处于删除状态； 从 apiserver 获取该 node 最新的状态，该 node 可能为 virtual node，若为 virtual node 则从 apiserver 中获取不到该 node 的对象，此时会将该 node 重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 判断该 node 最新状态的 uid 是否等于本地缓存中的 uid，若不匹配说明该 node 已更新过此时将其设置为 virtual node 并重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 通过 node 的 deletingDependents 字段判断该 node 当前是否处于删除 dependents 的状态，若该 node 处于删除 dependents 的状态则调用 processDeletingDependentsItem 方法检查 node 的 blockingDependents 是否被完全删除，若 blockingDependents 已完全被删除则删除该 node 对应的 finalizer，若 blockingDependents 还未删除完，将未删除的 blockingDependents 加入到 attemptToDelete 中；上文中在 GraphBuilder 处理 graphChanges 中的事件时，若发现 node 处于删除状态，会将 node 的 dependents 加入到 attemptToDelete 中并标记 node 的 deletingDependents 为 true； 调用 gc.classifyReferences 将 node 的 ownerReferences 分类为 solid, dangling, waitingForDependentsDeletion 三类：dangling(owner 不存在）、waitingForDependentsDeletion(owner 存在，owner 处于删除状态且正在等待其 dependents 被删除）、solid（至少有一个 owner 存在且不处于删除状态）；对以上分类进行不同的处理： 第一种情况是若 solid 不为 0 即当前 node 至少存在一个 owner，该对象还不能被回收，此时需要将 dangling 和 waitingForDependentsDeletion 列表中的 owner 从 node 的 ownerReferences 删除，即已经被删除或等待删除的引用从对象中删掉； 第二种情况是该 node 的 owner 处于 waitingForDependentsDeletion 状态并且 node 的 dependents 未被完全删除，该 node 需要等待删除完所有的 dependents 后才能被删除； 第三种情况就是该 node 已经没有任何 dependents 了，此时按照 node 中声明的删除策略调用 apiserver 的接口删除即可； pkg/controller/garbagecollector/garbagecollector.go:409\nfunc (gc *GarbageCollector) attemptToDeleteItem(item *node) error { klog.V(2).InfoS(\u0026#34;Processing object\u0026#34;, \u0026#34;object\u0026#34;, klog.KRef(item.identity.Namespace, item.identity.Name), \u0026#34;objectUID\u0026#34;, item.identity.UID, \u0026#34;kind\u0026#34;, item.identity.Kind) // 1. 判断 node 是否处于删除状态 if item.isBeingDeleted() \u0026amp;\u0026amp; !item.isDeletingDependents() { klog.V(5).Infof(\u0026#34;processing item %s returned at once, because its DeletionTimestamp is non-nil\u0026#34;, item.identity) return nil } // 2. 从 apiserver 获取该 node 最新的状态 latest, err := gc.getObject(item.identity) switch { case errors.IsNotFound(err): klog.V(5).Infof(\u0026#34;item %v not found, generating a virtual delete event\u0026#34;, item.identity) gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil case err != nil: return err } // 3. 判断该 node 最新状态的 uid 是否等于本地缓存中的 uid if latest.GetUID() != item.identity.UID { klog.V(5).Infof(\u0026#34;UID doesn\u0026#39;t match, item %v not found, generating a virtual delete event\u0026#34;, item.identity) gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil } // 4. 判断该 node 当前是否处于删除 dependents 状态中 if item.isDeletingDependents() { return gc.processDeletingDependentsItem(item) } // 5. 检查 node 是否还存在 ownerReferences ownerReferences := latest.GetOwnerReferences() if len(ownerReferences) == 0 { klog.V(2).Infof(\u0026#34;object %s\u0026#39;s doesn\u0026#39;t have an owner, continue on next item\u0026#34;, item.identity) return nil } // 6. 对 ownerReferences 进行分类 solid, dangling, waitingForDependentsDeletion, err := gc.classifyReferences(item, ownerReferences) if err != nil { return err } klog.V(5).Infof(\u0026#34;classify references of %s.\\nsolid: %#v\\ndangling: %#v\\nwaitingForDependentsDeletion: %#v\\n\u0026#34;, item.identity, solid, dangling, waitingForDependentsDeletion) switch { // 7.1 存在不处于删除状态的 owner case len(solid) != 0: klog.V(2).Infof(\u0026#34;object %#v has at least one existing owner: %#v, will not garbage collect\u0026#34;, item.identity, solid) if len(dangling) == 0 \u0026amp;\u0026amp; len(waitingForDependentsDeletion) == 0 { return nil } klog.V(2).Infof(\u0026#34;remove dangling references %#v and waiting references %#v for object %s\u0026#34;, dangling, waitingForDependentsDeletion, item.identity) ownerUIDs := append(ownerRefsToUIDs(dangling), ownerRefsToUIDs(waitingForDependentsDeletion)...) patch := deleteOwnerRefStrategicMergePatch(item.identity.UID, ownerUIDs...) _, err = gc.patch(item, patch, func(n *node) ([]byte, error) { return gc.deleteOwnerRefJSONMergePatch(n, ownerUIDs...) }) return err // 7.2 node 的 owner 处于 waitingForDependentsDeletion 状态并且 node 的 dependents 未被完全删除 case len(waitingForDependentsDeletion) != 0 \u0026amp;\u0026amp; item.dependentsLength() != 0: deps := item.getDependents() // 删除 dependents for _, dep := range deps { if dep.isDeletingDependents() { klog.V(2).Infof(\u0026#34;processing object %s, some of its owners and its dependent [%s] have FinalizerDeletingDependents, to prevent potential cycle, its ownerReferences are going to be modified to be non-blocking, then the object is going to be deleted with Foreground\u0026#34;, item.identity, dep.identity) patch, err := item.unblockOwnerReferencesStrategicMergePatch() if err != nil { return err } if _, err := gc.patch(item, patch, gc.unblockOwnerReferencesJSONMergePatch); err != nil { return err } break } } klog.V(2).Infof(\u0026#34;at least one owner of object %s has FinalizerDeletingDependents, and the object itself has dependents, so it is going to be deleted in Foreground\u0026#34;, item.identity) // 以 Foreground 模式删除 node 对象 policy := metav1.DeletePropagationForeground return gc.deleteObject(item.identity, \u0026amp;policy) // 7.3 该 node 已经没有任何依赖了，按照 node 中声明的删除策略调用 apiserver 的接口删除 default: var policy metav1.DeletionPropagation switch { case hasOrphanFinalizer(latest): policy = metav1.DeletePropagationOrphan case hasDeleteDependentsFinalizer(latest): policy = metav1.DeletePropagationForeground default: policy = metav1.DeletePropagationBackground } klog.V(2).InfoS(\u0026#34;Deleting object\u0026#34;, \u0026#34;object\u0026#34;, klog.KRef(item.identity.Namespace, item.identity.Name), \u0026#34;objectUID\u0026#34;, item.identity.UID, \u0026#34;kind\u0026#34;, item.identity.Kind, \u0026#34;propagationPolicy\u0026#34;, policy) return gc.deleteObject(item.identity, \u0026amp;policy) } } 2.1.3 gc.runAttemptToOrphanWorker() # gc.runAttemptToOrphanWorker() 是处理以 Orphan 模式删除的 node，主要逻辑为：\n调用 gc.orphanDependents() 删除 owner 所有 dependents OwnerReferences 中的 owner 字段； 调用 gc.removeFinalizer() 删除 owner 的 orphan Finalizer； 以上两步中若有失败的会进行重试； pkg/controller/garbagecollector/garbagecollector.go:602\nfunc (gc *GarbageCollector) attemptToOrphanWorker() bool { item, quit := gc.attemptToOrphan.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit { return false } defer gc.attemptToOrphan.Done(item) owner, ok := item.(*node) if !ok { utilruntime.HandleError(fmt.Errorf(\u0026#34;expect *node, got %#v\u0026#34;, item)) return true } // we don\u0026#39;t need to lock each element, because they never get updated owner.dependentsLock.RLock() dependents := make([]*node, 0, len(owner.dependents)) for dependent := range owner.dependents { dependents = append(dependents, dependent) } owner.dependentsLock.RUnlock() err := gc.orphanDependents(owner.identity, dependents) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;orphanDependents for %s failed with %v\u0026#34;, owner.identity, err)) gc.attemptToOrphan.AddRateLimited(item) return true } // update the owner, remove \u0026#34;orphaningFinalizer\u0026#34; from its finalizers list err = gc.removeFinalizer(owner, metav1.FinalizerOrphanDependents) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;removeOrphanFinalizer for %s failed with %v\u0026#34;, owner.identity, err)) gc.attemptToOrphan.AddRateLimited(item) } return true } 2.1.4 小结 # 上面的业务逻辑不算复杂，但是方法嵌套有点多，整理一下方法的调用链：\ngraph LR\rp1(garbageCollector.Run) --\u003e p11(gc.dependencyGraphBuilder.Run)\rp11 --\u003e p111(gb.startMonitors)\rp11 --\u003e p112(gb.runProcessGraphChanges)\rp112 --\u003e p1121(gb.processGraphChanges)\rp1121 --\u003e p11211(gb.processTransitions)\rp1 --\u003e p12(gc.runAttemptToDeleteWorker)\rp12 --\u003e p121(gc.attemptToDeleteWorker)\rp121 --\u003e p1211(gc.attemptToDeleteItem)\rp1 --\u003e p13(gc.runAttemptToOrphanWorker)\rp13 --\u003e p131(gc.orphanDependents)\rp13 --\u003e p132(gc.removeFinalizer)\r2.2 garbageCollector.Sync() # garbageCollector.Sync() 方法主要是周期性地查询集群中的所有资源，过滤出 deletableResource，然后对比已经监控的 deletableResource 是否一致，如果不一致，则更新 GraphBuilder 的 monitors，并重启 monitors 监控新拿到的 deletableResource。主要逻辑：\n通过调用 GetDeletableResources() 获取集群内所有的 deletableResources 作为 newResources，deletableResources 指支持 “delete”, “list”, “watch” 三种操作的 resource，包括自定义资源 检查 oldResources, newResources 是否一致，不一致则需要同步； 调用 gc.resyncMonitors() 同步 newResources，在 gc.resyncMonitors() 中会重新调用 GraphBuilder 的 syncMonitors() 和 startMonitors() 两个方法完成 monitors 的刷新； 等待 newResources informer 中的 cache 同步完成； 将 newResources 作为 oldResources，继续进行下一轮的同步； pkg/controller/garbagecollector/garbagecollector.go:168\nfunc (gc *GarbageCollector) Sync(discoveryClient discovery.ServerResourcesInterface, period time.Duration, stopCh \u0026lt;-chan struct{}) { oldResources := make(map[schema.GroupVersionResource]struct{}) wait.Until(func() { // 1. 获取集群内所有的 deletableResources 作为 newResources newResources := GetDeletableResources(discoveryClient) // 正常情况下是不会走到这里，除非发生内部错误 if len(newResources) == 0 { klog.V(2).Infof(\u0026#34;no resources reported by discovery, skipping garbage collector sync\u0026#34;) return } // 2. 判断集群中的资源是否有变化 if reflect.DeepEqual(oldResources, newResources) { klog.V(5).Infof(\u0026#34;no resource updates from discovery, skipping garbage collector sync\u0026#34;) return } // 加锁保证在 informer 同步完成后处理 event gc.workerLock.Lock() defer gc.workerLock.Unlock() // 3. 开始更新 GraphBuilder 中的 monitors attempt := 0 wait.PollImmediateUntil(100*time.Millisecond, func() (bool, error) { attempt++ // 尝试次数大于 1，先判断 deletableResource 是否改变 if attempt \u0026gt; 1 { newResources = GetDeletableResources(discoveryClient) if len(newResources) == 0 { klog.V(2).Infof(\u0026#34;no resources reported by discovery (attempt %d)\u0026#34;, attempt) return false, nil } } klog.V(2).Infof(\u0026#34;syncing garbage collector with updated resources from discovery (attempt %d): %s\u0026#34;, attempt, printDiff(oldResources, newResources)) gc.restMapper.Reset() klog.V(4).Infof(\u0026#34;reset restmapper\u0026#34;) // 4. 调用 gc.resyncMonitors 同步 newResources if err := gc.resyncMonitors(newResources); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026#34;failed to sync resource monitors (attempt %d): %v\u0026#34;, attempt, err)) return false, nil } klog.V(4).Infof(\u0026#34;resynced monitors\u0026#34;) // 5. 等待所有 monitors 的 cache 同步完成 if !cache.WaitForNamedCacheSync(\u0026#34;garbage collector\u0026#34;, waitForStopOrTimeout(stopCh, period), gc.dependencyGraphBuilder.IsSynced) { utilruntime.HandleError(fmt.Errorf(\u0026#34;timed out waiting for dependency graph builder sync during GC sync (attempt %d)\u0026#34;, attempt)) return false, nil } return true, nil }, stopCh) // 6. 更新 oldResources oldResources = newResources klog.V(2).Infof(\u0026#34;synced garbage collector\u0026#34;) }, period, stopCh) } 方法主要调用了 GetDeletableResources() 和 gc.resyncMonitors() 两个方法。前者获取集群中可删除资源，后者更新 monitors。\n2.2.1 GetDeletableResources() # GetDeletableResources() 中首先通过调用 discoveryClient.ServerPreferredResources() 方法获取集群内所有的 resource 信息，然后通过调用 discovery.FilteredBy() 过滤出支持 “delete”, “list”, “watch” 三种方法的 resource 作为 deletableResources。\npkg/controller/garbagecollector/garbagecollector.go:658\nfunc GetDeletableResources(discoveryClient discovery.ServerResourcesInterface) map[schema.GroupVersionResource]struct{} { // 获取集群内所有的 resource 信息 preferredResources, err := discoveryClient.ServerPreferredResources() ... if preferredResources == nil { return map[schema.GroupVersionResource]struct{}{} } // 过滤出 deletableResources deletableResources := discovery.FilteredBy(discovery.SupportsAllVerbs{Verbs: []string{\u0026#34;delete\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;}}, preferredResources) deletableGroupVersionResources := map[schema.GroupVersionResource]struct{}{} for _, rl := range deletableResources { gv, err := schema.ParseGroupVersion(rl.GroupVersion) if err != nil { klog.Warningf(\u0026#34;ignoring invalid discovered resource %q: %v\u0026#34;, rl.GroupVersion, err) continue } for i := range rl.APIResources { deletableGroupVersionResources[schema.GroupVersionResource{Group: gv.Group, Version: gv.Version, Resource: rl.APIResources[i].Name}] = struct{}{} } } return deletableGroupVersionResources } 2.2.2 gc.resyncMonitors() # gc.resyncMonitors() 的功能主要是更新 GraphBuilder 的 monitors 并重新启动 monitors 监控所有的 deletableResources，GraphBuilder 的 startMonitors() 方法在前面的流程中已经分析过，此处不再详细说明。syncMonitors() 只不过是拿最新的 deletableResources，把老的 monitors 字段值更新，该删的删，该加的加而已。\npkg/controller/garbagecollector/garbagecollector.go:113\nfunc (gc *GarbageCollector) resyncMonitors(deletableResources map[schema.GroupVersionResource]struct{}) error { if err := gc.dependencyGraphBuilder.syncMonitors(deletableResources); err != nil { return err } gc.dependencyGraphBuilder.startMonitors() return nil } 2.3 garbagecollector.NewDebugHandler() # garbagecollector.NewDebugHandler() 主要功能是对外提供一个接口供用户查询当前集群中所有资源的依赖关系，依赖关系可以以图表的形式展示。\nfunc NewDebugHandler(controller *GarbageCollector) http.Handler { return \u0026amp;debugHTTPHandler{controller: controller} } func (h *debugHTTPHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) { if req.URL.Path != \u0026#34;/graph\u0026#34; { http.Error(w, \u0026#34;\u0026#34;, http.StatusNotFound) return } var graph graph.Directed if uidStrings := req.URL.Query()[\u0026#34;uid\u0026#34;]; len(uidStrings) \u0026gt; 0 { uids := []types.UID{} for _, uidString := range uidStrings { uids = append(uids, types.UID(uidString)) } graph = h.controller.dependencyGraphBuilder.uidToNode.ToGonumGraphForObj(uids...) } else { graph = h.controller.dependencyGraphBuilder.uidToNode.ToGonumGraph() } data, err := dot.Marshal(graph, \u0026#34;full\u0026#34;, \u0026#34;\u0026#34;, \u0026#34; \u0026#34;) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/vnd.graphviz\u0026#34;) w.Header().Set(\u0026#34;X-Content-Type-Options\u0026#34;, \u0026#34;nosniff\u0026#34;) w.Write(data) w.WriteHeader(http.StatusOK) } 使用该服务的方法如下：\ncurl http://192.168.199.100:10252/debug/controllers/garbagecollector/graph \u0026gt; tmp.dot curl http://192.168.199.100:10252/debug/controllers/garbagecollector/graph?uid=f9555d53-2b5f-4702-9717-54a313ed4fe8 \u0026gt; tmp.dot // 生成 svg 文件 $ dot -Tsvg -o graph.svg tmp.dot // 然后在浏览器中打开 svg 文件 依赖关系如下图所示（点击图片查看更多）：\n2.4 总结 # GarbageCollectorController 是一种典型的生产者消费者模型，所有 deletableResources 的 informer 都是生产者，每种资源的 informer 监听到变化后都会将对应的事件 push 到 graphChanges 中，graphChanges 是 GraphBuilder 对象中的一个数据结构，GraphBuilder 会启动另外的 goroutine 对 graphChanges 中的事件进行分类并放在其 attemptToDelete 和 attemptToOrphan 两个队列中，garbageCollector 会启动多个 goroutine 对 attemptToDelete 和 attemptToOrphan 两个队列中的事件进行处理，处理的结果就是回收一些需要被删除的对象。最后，再用一个流程图总结一下 GarbageCollectorController 的主要流程： graph LR\ra[mirrors] --\u003e|produce| b(graphChanges)\rb --\u003e c(processGraphChanges)\rc --\u003e d(attemptToDelete)\rc --\u003e e(attemptToOrphan)\rd --\u003e|consume| f[AttemptToDeleteWorker]\re --\u003e|consume| g[AttemptToOrphanWorker]\r3. 参考资料 # 垃圾收集 garbage collector controller 源码分析 Kubernetes API 资源对象的删除和 GarbageCollector Controller "},{"id":12,"href":"/docs/kubernetes/kube-controller-manager/code-analysis-of-daemonset-controller/","title":"DaemonSet Controller 源码分析","section":"kube-controller-manager","content":" 1. DaemonSet 简介 # 我们知道，Deployment 是用来部署一定数量的 Pod。但是，当你希望 Pod 在集群中的每个节点上运行，并且每个节点上都需要一个 Pod 实例时，Deployment 就无法满足需求。\n这类需求包括 Pod 执行系统级别与基础结构相关的操作，比如：希望在每个节点上运行日志收集器和资源监控组件。另一个典型的例子，就是 Kubernetes 自己的 kube-proxy 进程，它需要在所有节点上都运行，才能使得 Service 正常工作。\n如此，DaemonSet 应运而生。它能确保集群中每个节点或者是满足某些特性的一组节点都运行一个 Pod 副本。当有新节点加入时，也会立即为它部署一个 Pod；当有节点从集群中删除时，Pod 也会被回收。删除 DaemonSet，也会删除所有关联的 Pod。\n1.1 应用场景 # 在每个节点上运行集群存守护进程 在每个节点上运行日志收集守护进程 在每个节点上运行监控守护进程 一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。 一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志，并且对不同硬件类型具有不同的内存、CPU 等要求。\n1.2 基本功能 # 创建 删除 级联删除：kubectl delete ds/nginx-ds 非级联删除：kubectl delete ds/nginx-ds --cascade=false 更新 RollingUpdate OnDelete 回滚 1.3 示例 # apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can\u0026#39;t run pods - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 2. 源码分析 # kubernetes version: v1.19\n2.1 startDaemonSetController() # 与其他资源的 Controller 启动方式一致，在 startDaemonSetController() 中初始化 DaemonSetController 对象，并调用 Run() 方法启动。从该方法可以看出，DaemonSet Controller 关心的是 daemonset、controllerrevision、pod、node 四种资源的变动，其中 ConcurrentDaemonSetSyncs 默认是 2。\ncmd/kube-controller-manager/app/apps.go:36\nfunc startDaemonSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \u0026#34;apps\u0026#34;, Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;daemonsets\u0026#34;}] { return nil, false, nil } dsc, err := daemon.NewDaemonSetsController( ctx.InformerFactory.Apps().V1().DaemonSets(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Core().V1().Nodes(), ctx.ClientBuilder.ClientOrDie(\u0026#34;daemon-set-controller\u0026#34;), flowcontrol.NewBackOff(1*time.Second, 15*time.Minute), ) if err != nil { return nil, true, fmt.Errorf(\u0026#34;error creating DaemonSets controller: %v\u0026#34;, err) } go dsc.Run(int(ctx.ComponentConfig.DaemonSetController.ConcurrentDaemonSetSyncs), ctx.Stop) return nil, true, nil } 2.2 Run() # Run() 方法中执行 2 个核心操作：sync 和 gc。其中 sync 操作是 controller 的核心代码，响应上述所有操作。在初始化 controller 对象时，指定了 failedPodsBackoff 的参数，defaultDuration = 1s，maxDuration = 15min；gc 的主要作用是发现 daemonset 的 pod 的 phase 为 failed，就会重启该 Pod，如果已经超时（2*15min）会删除该条记录。\npkg/controller/daemon/daemon_controller.go:281\nfunc (dsc *DaemonSetsController) Run(workers int, stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() defer dsc.queue.ShutDown() klog.Infof(\u0026#34;Starting daemon sets controller\u0026#34;) defer klog.Infof(\u0026#34;Shutting down daemon sets controller\u0026#34;) if !cache.WaitForNamedCacheSync(\u0026#34;daemon sets\u0026#34;, stopCh, dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) { return } for i := 0; i \u0026lt; workers; i++ { // sync go wait.Until(dsc.runWorker, time.Second, stopCh) } // gc go wait.Until(dsc.failedPodsBackoff.GC, BackoffGCInterval, stopCh) \u0026lt;-stopCh } 2.3 syncDaemonSet() # DaemonSet 的 Pod 的创建/删除都是和 Node 相关，所以每次 sync 操作，需要遍历所有的 Node 进行判断。syncDaemonSet() 的主要逻辑为：\n通过 key 获取 ns 和 name 从 dsLister 获取 ds 对象 从 nodeLister 获取全部 node 对象 获取 dsKey， 即：\u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; 判断 ds 是否处于删除中，如果正在删除，则等待删除完毕后再次进入 sync 获取 cur 和 old controllerrevision 判断是否满足 expectation 机制，expectation 机制就是为了减少不必要的 sync 操作 调用 dsc.manage()，执行实际的 sync 操作 判断是否为更新操作，并执行 调用 dsc.cleanupHistory() 根据 spec.revisionHistroyLimit 清理过期的 controllerrevision 调用 dsc.updateDaemonSetStatus()，更新 status 子资源 pkg/controller/daemon/daemon_controller.go:1129\nfunc (dsc *DaemonSetsController) syncDaemonSet(key string) error { ... // 1. 通过 key 获取 ns 和 name namespace, name, err := cache.SplitMetaNamespaceKey(key) ... // 2. 从 dsLister 获取 ds 对象 ds, err := dsc.dsLister.DaemonSets(namespace).Get(name) ... // 3. 从 nodeLister 获取全部 node 对象 nodeList, err := dsc.nodeLister.List(labels.Everything()) ... // 4. 获取 dsKey， 即：\u0026lt;namespace\u0026gt;/\u0026lt;name\u0026gt; dsKey, err := controller.KeyFunc(ds) ... // 5. 判断 ds 是否处于删除中，如果正在删除，则等待删除完毕后再次进入 sync if ds.DeletionTimestamp != nil { return nil } // 6. 获取 cur 和 old controllerrevision cur, old, err := dsc.constructHistory(ds) // hash 就是当前 controllerrevision 的 controller-revision-hash 值 hash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey] // 7. 判断是否满足 expectation 机制 if !dsc.expectations.SatisfiedExpectations(dsKey) { // 不满足，只更新 status 子资源 return dsc.updateDaemonSetStatus(ds, nodeList, hash, false) } // 8. 实际执行的 sync 操作 err = dsc.manage(ds, nodeList, hash) ... // 9. 判断是否为更新操作，并执行 if dsc.expectations.SatisfiedExpectations(dsKey) { switch ds.Spec.UpdateStrategy.Type { case apps.OnDeleteDaemonSetStrategyType: case apps.RollingUpdateDaemonSetStrategyType: err = dsc.rollingUpdate(ds, nodeList, hash) } if err != nil { return err } } // 10. 清理过期的 controllerrevision err = dsc.cleanupHistory(ds, old) if err != nil { return fmt.Errorf(\u0026#34;failed to clean up revisions of DaemonSet: %v\u0026#34;, err) } // 11. 最后更新 status 子资源 return dsc.updateDaemonSetStatus(ds, nodeList, hash, true) } 2.4 dsc.manage() # dsc.manage() 是为了保证 ds 的 Pod 正常运行在应该存在的节点，该方法做了这几件事：\n调用 dsc.getNodesToDaemonPods()，获取当前的节点和 daemon pod 的映射关系（map[nodeName][]*v1.Pod） 遍历所有节点，判断每个节点是需要创建还是删除 daemonset pod 从 1.12 开始，daemon pod 已经由 kube-scheduler 负责调度，可能会出现把 daemon pod 调度到不存在的节点上，如果存在这种情况，就要删除该 pod 调用 dsc.syncNodes() 为对应的 node 创建 daemon pod 以及删除多余的 pods； pkg/controller/daemon/daemon_controller.go:881\nfunc (dsc *DaemonSetsController) manage(ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error { // 1. 找到节点和 ds 创建的 Pod 的映射关系（nodeName:[]Pod{}） nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) ... // 2. 检查每个节点是否应当运行该 daemonset 的 Pod，如果不该运行就要删掉，反之就要创建 var nodesNeedingDaemonPods, podsToDelete []string for _, node := range nodeList { nodesNeedingDaemonPodsOnNode, podsToDeleteOnNode, err := dsc.podsShouldBeOnNode( node, nodeToDaemonPods, ds) if err != nil { continue } nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, nodesNeedingDaemonPodsOnNode...) podsToDelete = append(podsToDelete, podsToDeleteOnNode...) } // 3. 调用 getUnscheduledPodsWithoutNode() 方法找到把调度到不存在的节点的 Pod podsToDelete = append(podsToDelete, getUnscheduledPodsWithoutNode(nodeList, nodeToDaemonPods)...) // 4. 调用 dsc.syncNodes()，删除多余的 Pod，为应该运行 daemonset Pod 的 node 创建 Pod if err = dsc.syncNodes(ds, podsToDelete, nodesNeedingDaemonPods, hash); err != nil { return err } return nil } 下面继续看下 dsc.podsShouldBeOnNode() 和 dsc.syncNodes() 两个方法的具体逻辑：\n其中，podsShouldBeOnNode() 主要是为了确定在给定节点上的是需要创建还是删除 daemon pod。主要逻辑为：\n调用 dsc.nodeShouldRunDaemonPod 判断该 node 是否要运行以及是否能继续运行 ds pod 获取该节点上的 daemon pod 列表 根据 shouldRun、shouldContinueRunning 和 exists（daemon pod 的存在状态），进行下一步 如果节点应该运行而没有运行，则创建该 Pod 如果 daemon pods 应该继续在此节点上运行，遍历每个 daemon pod，且 pod 在删除中，暂且不管 pod 处于 failed 状态，则删除 pod daemon pods 数量 \u0026gt; 1，则保留最早创建的 pod，其余都删除 如果 pod 不需要继续运行但 pod 已存在，则需要删除 最终返回需要运行 daemon pod 的节点集合和待删除 pod 的集合 func (dsc *DaemonSetsController) podsShouldBeOnNode( node *v1.Node, nodeToDaemonPods map[string][]*v1.Pod, ds *apps.DaemonSet, ) (nodesNeedingDaemonPods, podsToDelete []string, err error) { // 1. 判断该 node 是否要运行以及是否能继续运行 ds pod shouldRun, shouldContinueRunning, err := dsc.nodeShouldRunDaemonPod(node, ds) if err != nil { return } // 2. 获取该节点上的该 daemon pod 列表 daemonPods, exists := nodeToDaemonPods[node.Name] switch { case shouldRun \u0026amp;\u0026amp; !exists: // 3.1 如果节点应该运行而没有运行，则创建该 Pod nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, node.Name) case shouldContinueRunning: // 3.2. 如果 daemon pod 应该继续在此节点上运行 var daemonPodsRunning []*v1.Pod for _, pod := range daemonPods { // 3.2.1 pod 在删除中，暂且不管 if pod.DeletionTimestamp != nil { continue } // 3.2.2 pod 处于 failed 状态，则删除 pod if pod.Status.Phase == v1.PodFailed { // This is a critical place where DS is often fighting with kubelet that rejects pods. // We need to avoid hot looping and backoff. backoffKey := failedPodsBackoffKey(ds, node.Name) now := dsc.failedPodsBackoff.Clock.Now() inBackoff := dsc.failedPodsBackoff.IsInBackOffSinceUpdate(backoffKey, now) if inBackoff { delay := dsc.failedPodsBackoff.Get(backoffKey) klog.V(4).Infof(\u0026#34;Deleting failed pod %s/%s on node %s has been limited by backoff - %v remaining\u0026#34;, pod.Namespace, pod.Name, node.Name, delay) dsc.enqueueDaemonSetAfter(ds, delay) continue } dsc.failedPodsBackoff.Next(backoffKey, now) msg := fmt.Sprintf(\u0026#34;Found failed daemon pod %s/%s on node %s, will try to kill it\u0026#34;, pod.Namespace, pod.Name, node.Name) klog.V(2).Infof(msg) // Emit an event so that it\u0026#39;s discoverable to users. dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, FailedDaemonPodReason, msg) podsToDelete = append(podsToDelete, pod.Name) } else { daemonPodsRunning = append(daemonPodsRunning, pod) } } // 3.2.3 如果 daemon pod 应该在此节点上运行，但存在的 pod 数 \u0026gt; 1，则保留最早创建的 pod，其余删除 if len(daemonPodsRunning) \u0026gt; 1 { sort.Sort(podByCreationTimestampAndPhase(daemonPodsRunning)) for i := 1; i \u0026lt; len(daemonPodsRunning); i++ { podsToDelete = append(podsToDelete, daemonPodsRunning[i].Name) } } case !shouldContinueRunning \u0026amp;\u0026amp; exists: // 3.3 如果 pod 不需要继续运行但 pod 已存在则需要删除 pod for _, pod := range daemonPods { if pod.DeletionTimestamp != nil { continue } podsToDelete = append(podsToDelete, pod.Name) } } // 4. 最终返回需要运行 daemon pod 的节点集合和待删除 pod 集合 return nodesNeedingDaemonPods, podsToDelete, nil } 继续看 dsc.syncNode()，该方法主要是为需要 daemon pod 的 node 创建 pod 以及删除多余的 pod，其主要逻辑为：\n将 createDiff 和 deleteDiff 与 burstReplicas 进行比较，burstReplicas 默认值为 250，即每个 syncLoop 中创建或者删除的 pod 数最多为 250 个，若超过其值则剩余需要创建或者删除的 pod 在下一个 syncLoop 继续操作； 将 createDiff 和 deleteDiff 写入到 expectations 中； 并发创建 pod，通过 nodeAffinity 来保证每个节点都运行一个 pod； 并发删除 deleteDiff 中的所有 pod； func (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error { // 1. 取 ds 的 key (namespace/name) dsKey, err := controller.KeyFunc(ds) ... // 2. 设置创删过程中可超过的副本数 createDiff := len(nodesNeedingDaemonPods) deleteDiff := len(podsToDelete) if createDiff \u0026gt; dsc.burstReplicas { createDiff = dsc.burstReplicas } if deleteDiff \u0026gt; dsc.burstReplicas { deleteDiff = dsc.burstReplicas } // 3. 设置 expectation dsc.expectations.SetExpectations(dsKey, createDiff, deleteDiff) // error channel to communicate back failures. make the buffer big enough to avoid any blocking errCh := make(chan error, createDiff+deleteDiff) createWait := sync.WaitGroup{} generation, err := util.GetTemplateGeneration(ds) ... template := util.CreatePodTemplate(ds.Spec.Template, generation, hash) // 4. 并发创建 pod，创建的 pod 数依次为 1, 2, 4, 8, ... batchSize := integer.IntMin(createDiff, controller.SlowStartInitialBatchSize) for pos := 0; createDiff \u0026gt; pos; batchSize, pos = integer.IntMin(2*batchSize, createDiff-(pos+batchSize)), pos+batchSize { errorCount := len(errCh) createWait.Add(batchSize) for i := pos; i \u0026lt; pos+batchSize; i++ { go func(ix int) { defer createWait.Done() podTemplate := template.DeepCopy() // 5. 使用节点亲和性完成 daemon pod 调度 podTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity( podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix]) // 6. 创建 Pod 带上 ControllerRef err := dsc.podControl.CreatePodsWithControllerRef(ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind)) if err != nil { if errors.HasStatusCause(err, v1.NamespaceTerminatingCause) { // 如果此时 namespace 被删除，这里错误可以忽略 return } } if err != nil { klog.V(2).Infof(\u0026#34;Failed creation, decrementing expectations for set %q/%q\u0026#34;, ds.Namespace, ds.Name) dsc.expectations.CreationObserved(dsKey) errCh \u0026lt;- err utilruntime.HandleError(err) } }(i) } createWait.Wait() // 6. 将创建失败的 Pod 记录到 expectation 中 skippedPods := createDiff - (batchSize + pos) if errorCount \u0026lt; len(errCh) \u0026amp;\u0026amp; skippedPods \u0026gt; 0 { klog.V(2).Infof(\u0026#34;Slow-start failure. Skipping creation of %d pods, decrementing expectations for set %q/%q\u0026#34;, skippedPods, ds.Namespace, ds.Name) dsc.expectations.LowerExpectations(dsKey, skippedPods, 0) // skippedPod 会在下轮 sync 时重试 break } } klog.V(4).Infof(\u0026#34;Pods to delete for daemon set %s: %+v, deleting %d\u0026#34;, ds.Name, podsToDelete, deleteDiff) deleteWait := sync.WaitGroup{} deleteWait.Add(deleteDiff) for i := 0; i \u0026lt; deleteDiff; i++ { // 7. 并发删除 deleteDiff 中的 pod go func(ix int) { defer deleteWait.Done() if err := dsc.podControl.DeletePod(ds.Namespace, podsToDelete[ix], ds); err != nil { dsc.expectations.DeletionObserved(dsKey) if !apierrors.IsNotFound(err) { klog.V(2).Infof(\u0026#34;Failed deletion, decremented expectations for set %q/%q\u0026#34;, ds.Namespace, ds.Name) errCh \u0026lt;- err utilruntime.HandleError(err) } } }(i) } deleteWait.Wait() // 收集错误返回 errors := []error{} close(errCh) for err := range errCh { errors = append(errors, err) } return utilerrors.NewAggregate(errors) } 到此，总结一下，dsc.manage() 方法的主要流程：\ngraph LR op1(dsc.manage) --\u003e op2(dsc.getNodesToDaemonPods) op1 --\u003e op3(dsc.podsShouldBeOnNode) op1 --\u003e op4(dsc.syncNodes) op3 --\u003e op5(dsc.nodeShouldRunDaemonPod) 2.5 dsc.rollingUpdate() # daemonset update 的方式有两种 OnDelete 和 RollingUpdate，当为 OnDelete 时需要用户手动删除每一个 pod 后完成更新操作，当为 RollingUpdate 时，daemonset controller 会自动控制升级进度。\n当为 RollingUpdate 时，主要逻辑为：\n获取 node 与 daemon pods 的映射关系 根据 controllerrevision 的 hash 值获取所有未更新的 pods； 获取 maxUnavailable, numUnavailable 的 pod 数值，maxUnavailable 是从 ds 的 rollingUpdate 字段中获取的默认值为 1，numUnavailable 的值是通过 daemonset pod 与 node 的映射关系计算每个 node 下是否有 available pod 得到的； 通过 oldPods 获取 oldAvailablePods, oldUnavailablePods 的 pod 列表； 遍历 oldUnavailablePods 列表将需要删除的 pod 追加到 oldPodsToDelete 数组中。oldUnavailablePods 列表中的 pod 分为两种，一种处于更新中，即删除状态，一种处于未更新且异常状态，处于异常状态的都需要被删除； 遍历 oldAvailablePods 列表，此列表中的 pod 都处于正常运行状态，根据 maxUnavailable 值确定是否需要删除该 pod 并将需要删除的 pod 追加到 oldPodsToDelete 数组中； 调用 dsc.syncNodes() 删除 oldPodsToDelete 数组中的 pods，syncNodes 方法在 manage 阶段已经分析过，此处不再赘述； pkg/controller/daemon/update.go:44\nfunc (dsc *DaemonSetsController) rollingUpdate(ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error { // 1. 获取 node 与 daemon pods 的映射关系 nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) if err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t get node to daemon pod mapping for daemon set %q: %v\u0026#34;, ds.Name, err) } // 2. 获取所有未更新的 pods _, oldPods := dsc.getAllDaemonSetPods(ds, nodeToDaemonPods, hash) // 3. 计算 maxUnavailable, numUnavailable 的 pod 数值 maxUnavailable, numUnavailable, err := dsc.getUnavailableNumbers(ds, nodeList, nodeToDaemonPods) if err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t get unavailable numbers: %v\u0026#34;, err) } // 4. 把未更新的 pods 分成 available 和 unavailable oldAvailablePods, oldUnavailablePods := util.SplitByAvailablePods(ds.Spec.MinReadySeconds, oldPods) // 5. 将 unavailable 状态且没有删除标记的 pods 加入到 oldPodsToDelete 中 var oldPodsToDelete []string klog.V(4).Infof(\u0026#34;Marking all unavailable old pods for deletion\u0026#34;) for _, pod := range oldUnavailablePods { // Skip terminating pods. We won\u0026#39;t delete them again if pod.DeletionTimestamp != nil { continue } klog.V(4).Infof(\u0026#34;Marking pod %s/%s for deletion\u0026#34;, ds.Name, pod.Name) oldPodsToDelete = append(oldPodsToDelete, pod.Name) } // 6. 根据 maxUnavailable 值确定是否需要删除 pod klog.V(4).Infof(\u0026#34;Marking old pods for deletion\u0026#34;) for _, pod := range oldAvailablePods { if numUnavailable \u0026gt;= maxUnavailable { klog.V(4).Infof(\u0026#34;Number of unavailable DaemonSet pods: %d, is equal to or exceeds allowed maximum: %d\u0026#34;, numUnavailable, maxUnavailable) break } klog.V(4).Infof(\u0026#34;Marking pod %s/%s for deletion\u0026#34;, ds.Name, pod.Name) oldPodsToDelete = append(oldPodsToDelete, pod.Name) numUnavailable++ } // 7. 调用 syncNodes 方法删除 oldPodsToDelete 数组中的 pods return dsc.syncNodes(ds, oldPodsToDelete, []string{}, hash) } 2.6 dsc.updateDaemonSetStatus() # dsc.updateDaemonSetStatus() 是 sync 动作的最后一步，主要是用来更新 DaemonSet 的 status 子资源。ds.Status 的各个字段如下：\nstatus: collisionCount: 0 # hash 冲突数 currentNumberScheduled: 1 # 已经运行了 DaemonSet Pod 的节点数量 desiredNumberScheduled: 1 # 需要运行该 DaemonSet Pod 的节点数量 numberAvailable: 1 # DaemonSet Pod 状态为 Ready 且运行时间超过 Spec.MinReadySeconds 的节点数量 numberUnavailable: 0 # desiredNumberScheduled - numberAvailable 的节点数量 numberMisscheduled: 0 # 不需要运行 DeamonSet Pod 但是已经运行了的节点数量 numberReady: 1 # DaemonSet Pod 状态为 Ready 的节点数量 observedGeneration: 1 updatedNumberScheduled: 1 # 已经完成 DaemonSet Pod 更新的节点数量 主要逻辑为：\n调用 dsc.getNodesToDaemonPods() 获取 node 与已存在的 daemon pods 的映射关系； 遍历所有 node，调用 dsc.nodeShouldRunDaemonPod() 判断该 node 是否需要运行 daemon pod，然后计算 status 中的部分字段值； 调用 storeDaemonSetStatus() 更新 ds.status； 判断 ds 是否需要 resync； pkg/controller/daemon/daemon_controller.go:1075\nfunc (dsc *DaemonSetsController) updateDaemonSetStatus(ds *apps.DaemonSet, nodeList []*v1.Node, hash string, updateObservedGen bool) error { klog.V(4).Infof(\u0026#34;Updating daemon set status\u0026#34;) // 1. 获取 node 与 daemon pods 的映射关系 nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) if err != nil { return fmt.Errorf(\u0026#34;couldn\u0026#39;t get node to daemon pod mapping for daemon set %q: %v\u0026#34;, ds.Name, err) } var desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable int for _, node := range nodeList { // 2. 判断该 node 是否需要运行 daemon pod shouldRun, _, err := dsc.nodeShouldRunDaemonPod(node, ds) if err != nil { return err } scheduled := len(nodeToDaemonPods[node.Name]) \u0026gt; 0 // 3. 计算 status 中的字段值 if shouldRun { desiredNumberScheduled++ if scheduled { currentNumberScheduled++ // 按照创建时间排序，最早创建在最前 daemonPods, _ := nodeToDaemonPods[node.Name] sort.Sort(podByCreationTimestampAndPhase(daemonPods)) pod := daemonPods[0] if podutil.IsPodReady(pod) { numberReady++ if podutil.IsPodAvailable(pod, ds.Spec.MinReadySeconds, metav1.Now()) { numberAvailable++ } } // If the returned error is not nil we have a parse error. // The controller handles this via the hash. generation, err := util.GetTemplateGeneration(ds) if err != nil { generation = nil } if util.IsPodUpdated(pod, hash, generation) { updatedNumberScheduled++ } } } else { if scheduled { numberMisscheduled++ } } } numberUnavailable := desiredNumberScheduled - numberAvailable // 4. 更新 ds.status err = storeDaemonSetStatus(dsc.kubeClient.AppsV1().DaemonSets(ds.Namespace), ds, desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable, numberUnavailable, updateObservedGen) if err != nil { return fmt.Errorf(\u0026#34;error storing status for daemon set %#v: %v\u0026#34;, ds, err) } // Resync the DaemonSet after MinReadySeconds as a last line of defense to guard against clock-skew. // 5. 判断 ds 是否需要 resync if ds.Spec.MinReadySeconds \u0026gt; 0 \u0026amp;\u0026amp; numberReady != numberAvailable { dsc.enqueueDaemonSetAfter(ds, time.Duration(ds.Spec.MinReadySeconds)*time.Second) } return nil } 2.7 源码分析小结 # graph LR op(startDaeonSetController) --\u003e op0(dsc.Run) op0 --\u003e op1(dsc.syncDaemonSet) op1 --\u003e op1.1(dsc.manage) op1 --\u003e op1.2(dsc.rollingUpdate) op1 --\u003e op1.3(dsc.updateDaemonSetStatus) op1.1 --\u003e op1.1.1(dsc.getNodesToDaemonPods) op1.1 --\u003e op1.1.2(dsc.podsShouldBeOnNode) op1.1 --\u003e op1.1.3(dsc.syncNodes) op1.1.2 --\u003e op1.1.2.1(dsc.nodeShouldRunDaemonPod) 3. 总结 # 在 daemonset controller 中可以看到许多功能都是 deployment 和 statefulset 已有的。在创建 pod 的流程与 replicaset controller 创建 pod 的流程是相似的，都使用了 expectations 机制并且限制了在一个 syncLoop 中最多创建或删除的 pod 数。更新方式与 statefulset 一样都有 OnDelete 和 RollingUpdate 两种， OnDelete 方式与 statefulset 相似，都需要手动删除对应的 pod，而 RollingUpdate 方式与 statefulset 和 deployment 都有点区别，RollingUpdate 方式更新时不支持暂停操作并且 pod 是先删除再创建的顺序进行。版本控制方式与 statefulset 的一样都是使用 controllerRevision。最后要说的一点是在 v1.12 及以后的版本中，使用 daemonset 创建的 pod 已不再使用直接指定 .spec.nodeName 的方式绕过调度器进行调度，而是走默认调度器通过 nodeAffinity 的方式调度到每一个节点上。\n4. 参考资料 # DaemonSet 概念 DaemonSet 滚动更新 daemonset controller 源码分析 "},{"id":13,"href":"/docs/leetcode/0416/","title":"416. 分割等和子集","section":"LeetCode","content":" 416. 分割等和子集 # leetcode 链接： https://leetcode-cn.com/problems/partition-equal-subset-sum/\n给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。\n注意：\n每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1:\n输入：[1, 5, 11, 5] 输出：true 解释：数组可以分割成 [1, 5, 5] 和 [11]. 示例 2:\n输入：[1, 2, 3, 5] 输出：false 解释：数组不能分割成两个元素和相等的子集。 方法一：0-1 背包问题\n// 0-1 背包问题 // dp[i][target] 表示 nums[0, i] 区间内是否能找到和为 target 的组合 // 对于每个 nums[i]，如果 nums[i] \u0026lt;= target，可以选择 or 不选，但只要有一个为 true，dp[i][target]=true // dp[i][target] = dp[i-1][target] || dp[i][target-nums[i]] // 如果 nums[i] \u0026gt; target，只能不选，故： // dp[i][target] = dp[i-1][target] func canPartition(nums []int) bool { n := len(nums) if n \u0026lt; 2 { return false } sum := 0 maxNum := 0 for _, num := range nums { sum += num if num \u0026gt; maxNum { maxNum = num } } // 和为奇数 if sum%2 != 0 { return false } // 最大值超过和的一半 target := sum / 2 if maxNum \u0026gt; target { return false } var dp = make([][]bool, n) for i := range dp { dp[i] = make([]bool, target+1) // nums 数组均为正数，如果不选取任何 nums[i]，则被选取的正整数的和等于 0 dp[i][0] = true } // 只有一个元素 nums[0] 可取时，构成初始值 dp[0][nums[0]] 为 true dp[0][nums[0]] = true for i := 1; i \u0026lt; n; i++ { for j := 1; j \u0026lt;= target; j++ { if nums[i] \u0026lt;= j { dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i]] } else { dp[i][j] = dp[i-1][j] } // 剪枝 if dp[i][target] { return true } } } return dp[n-1][target] } 方法 2：动态规划降维\nfunc canPartition(nums []int) bool { n := len(nums) if n \u0026lt; 2 { return false } sum := 0 maxNum := 0 for _, num := range nums { sum += num if num \u0026gt; maxNum { maxNum = num } } // 和为奇数 if sum%2 != 0 { return false } // 最大值超过和的一半 target := sum / 2 if maxNum \u0026gt; target { return false } var dp = make([]bool, target+1) dp[0] = true dp[nums[0]] = true for i := 1; i \u0026lt; len(nums); i++ { // 采用逆序，如果采用正序 dp[j-nums[i]] 会被之前的操作更新为新值 // dp[j](new) = dp[j](old) || dp[j - nums[i]](old) for j := target; j \u0026gt;= nums[i]; j-- { if dp[target] { return true } dp[j] = dp[j] || dp[j-nums[i]] } } return dp[target] } "},{"id":14,"href":"/docs/kubernetes/kube-apiserver/garbage-collector/","title":"垃圾回收","section":"kube-apisever","content":" 1. 序言 # 1.1 什么是垃圾回收 # 参考 Java 中的概念，垃圾回收（Garbage Collection）是 JVM 垃圾回收器提供的一种用于在空闲时间不定时回收无任何对象引用的对象占据的内存空间的一种机制。 垃圾回收回收的是无任何引用的对象占据的内存空间而不是对象本身。换言之，垃圾回收只会负责释放那些对象占有的内存。 对象是个抽象的词，包括引用和其占据的内存空间。当对象没有任何引用时其占据的内存空间随即被收回备用，此时对象也就被销毁。\n因此，垃圾回收关注的是无任何引用的对象。在 kubernetes 中，对象的引用关系又是怎样的呢？\n1.2 k8s 中的对象引用 # 某些 kubernetes 对象是其他一些对象的属主。例如一个 ReplicaSet 是一组 Pod 的属主；反之这组 Pod 就是此 ReplicaSet 的附属。 每个附属对象具有一个指向属主对象的 metadata.ownerReference 字段。\nKubernetes 会自动为 ReplicationController、ReplicaSet、StatefulSet、DaemonSet、Deployment、Job 和 CronJob 自动设置 ownerReference 的值。 也可以通过手动设置 ownerReference 的值，来指定属主和附属之间的关系。\n先看一个 Pod 的详细信息，例如下面的配置显示 Pod 的属主是名为 my-replicaset 的 ReplicaSet：\napiVersion: v1 kind: Pod metadata: ... ownerReferences: - apiVersion: apps/v1 controller: true blockOwnerDeletion: true kind: ReplicaSet name: my-rs uid: d9607e19-f88f-11e6-a518-42010a800195 ... 下面的源码是 ownerReference 的结构：\ntype OwnerReference struct { APIVersion string `json:\u0026#34;apiVersion\u0026#34; protobuf:\u0026#34;bytes,5,opt,name=apiVersion\u0026#34;` Kind string `json:\u0026#34;kind\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=kind\u0026#34;` Name string `json:\u0026#34;name\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=name\u0026#34;` UID types.UID `json:\u0026#34;uid\u0026#34; protobuf:\u0026#34;bytes,4,opt,name=uid,casttype=k8s.io/apimachinery/pkg/types.UID\u0026#34;` Controller *bool `json:\u0026#34;controller,omitempty\u0026#34; protobuf:\u0026#34;varint,6,opt,name=controller\u0026#34;` BlockOwnerDeletion *bool `json:\u0026#34;blockOwnerDeletion,omitempty\u0026#34; protobuf:\u0026#34;varint,7,opt,name=blockOwnerDeletion\u0026#34;` } 上面的结构中没有Namespace 属性，这是为什么呢？\n根据设计，kubernetes 不允许跨命名空间指定属主。也就是说：\n有 Namespace 属性的附属，只能指定同一 Namespace 中的或者集群范围的属主。 Cluster 级别的附属，只能指定集群范围的属主，不能指定命名空间范围的属主。 如此一来，有 Namespace 属性的对象，它的属主要么是与自己是在同一个 Namespace 下，可以复用；要么是集群级别的对象，没有 Namespace 属性。 而 Cluster 级别的对象，它的属主也必须是 Cluster 级别，没有 Namespace 属性。 因此，OwnerReference 结构中不需要 Namespace 属性。\n上面 OwnerReference 的结构体中，最后一个字段 BlockOwnerDeletion 字面意思就是阻止属主删除，那么 k8s 在删除对象上与垃圾收集有什么关系？ 垃圾收集具体是如何执行的呢？后文继续分析。\n2. k8s 中的垃圾回收 # 删除对象时，可以指定该对象的附属是否也自动删除。Kubernetes 中有三种删除模式：\n级联删除 Foreground 模式 Background 模式 非级联删除 Orphan 模式 在 kubernetes v1.9 版本之前，大部分 controller 的默认删除策略为 Orphan，从 v1.9 开始，对 apps/v1 下的资源默认使用 Background 模式。\n针对 Pod 存在特有的删除方式：Gracefully terminate，允许优雅地终止容器。 先发送 TERM 信号，过了宽限期还未终止，则发送 KILL 信号。 kube-apiserver 先设置 ObjectMeta.DeletionGracePeriodSeconds，默认为 30s， 再由 kubelet 发送删除请求，请求参数中 DeleteOptions.GracePeriodSeconds = 0， kube-apiserver 判断到 lastGraceful = options.GracePeriodSeconds = 0，就直接删除对象了。\n2.1 Foreground 模式 # 在 Foreground 模式下，待删除对象首先进入 deletion in progress 状态。 在此状态下存在如下的场景：\n对象仍然可以通过 REST API 获取。 会设置对象的 deletionTimestamp 字段。 对象的 metadata.finalizers 字段包含值 foregroundDeletion。 只有对象被设置为 deletion in progress 状态时，垃圾收集器才会删除对象的所有附属。 垃圾收集器在删除了所有有阻塞能力的附属（对象的 ownerReference.blockOwnerDeletion=true） 之后，再删除属主对象。\n注意，在 Foreground 模式下，只有设置了 ownerReference.blockOwnerDeletion=true 的附属才能阻止属主对象被删除。 在 Kubernetes 1.7 版本增加了准入控制器，基于属主对象上的删除权限来限制用户设置 ownerReference.blockOwnerDeletion=true， 这样未经授权的附属不能够阻止属主对象的删除。\n如果一个对象的 ownerReference 字段被一个控制器（例如 Deployment 或 ReplicaSet）设置， blockOwnerDeletion 也会被自动设置，不需要手动修改。\n2.2 Background 模式 # 在 Background 模式下，Kubernetes 会立即删除属主对象，之后垃圾收集器会在后台删除其附属对象。\n2.3 Orphan 模式 # 与 Foreground 模式类似，待删除对象首先进入 deletion in progress 状态。 在此状态下存在如下的场景：\n对象仍然可以通过 REST API 获取。 会设置对象的 deletionTimestamp 字段。 对象的 metadata.finalizers 字段包含值 orphan。 与 Foreground 模式不同的是，不会自动删除它的依赖或者是子对象，这些残留的依赖被称作是原对象的孤儿对象。\n例如执行以下命令时，会使用 Orphan 策略进行删除，此时 ds 的依赖对象 controllerrevision 不会被删除：\nkubectl delete rs my-rs --cascade=false 或者使用 curl 命令：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-rs \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; 2.4 finalizer 机制 # finalizer 是在对象删除之前需要执行的逻辑。每当 finalizer 成功运行之后，就会将它自己从 Finalizers 数组中删除， 当最后一个 finalizer 被删除之后，API Server 就会删除该对象。finalizer 提供了一个通用的 API， 它的功能不只是用于阻止级联删除，还能过通过它在对象删除之前加入钩子。\ntype ObjectMeta struct { // ... Finalizers []string } k8s 中默认有两种 finalizer：OrphanFinalizer 和 ForegroundFinalizer，分别对应 Orphan 模式和 Foreground 模式。 当然，finalizer 不仅仅支持以上两种字段，在使用自定义 controller 时，也可以在 CustomResource 中设置自定义的 finalizer 标识。\n在默认情况下，删除一个对象会删除它的全部依赖，但是在一些特定情况下，我们只是想删除当前对象本身并不想造成复杂地级联删除， 如此 OrphanFinalizer 应运而生。OrphanFinalizer 会监听对象的更新事件并将它自己从它全部依赖对象的 OwnerReferences 数组中删除， 与此同时会删除所有依赖对象中已经失效的 OwnerReferences 并将 OrphanFinalizer 从 Finalizers 数组中删除。\n通过 OrphanFinalizer 我们能够在删除一个 Kubernetes 对象时保留它的全部依赖，为使用者提供一种更灵活的方法来保留和删除对象。\n3. 参考资料 # 垃圾收集 garbage collector controller 源码分析 Kubernetes API 资源对象的删除和 GarbageCollector Controller "},{"id":15,"href":"/docs/kubernetes/kube-controller-manager/code-analysis-of-statefulset-controller/","title":"Statefulset Controller 源码分析","section":"kube-controller-manager","content":" 1. StatefulSet 简介 # Statefulset 是为了解决有状态服务的问题，而产生的一种资源类型（Deployment 和 ReplicaSet 是解决无状态服务而设计的）。\n这里可能有人说，MySQL 是有状态服务吧，但我使用的是 Deploment 资源类型，MySQL 的数据通过 PV 的方式存储在第三方文件系统中，也能解决 MySQL 数据存储问题。\n是的，如果你的 MySQL 是单节点，使用 Deployment 类型确实可以解决数据存储问题。但是如果你的有状态服务是集群，且每个节点分片存储的情况下，Deployment 则不适用这种场景，因为 Deployment 不会保证 Pod 的有序性，集群通常需要主节点先启动，从节点在加入集群，Statefulset 则可以保证，其次 Deployment 资源的 Pod 内的 PVC 是共享存储的，而 Statefulset 下的 Pod 内 PVC 是不共享存储的，每个 Pod 拥有自己的独立存储空间，正好满足了分片的需求，实现分片的需求的前提是 Statefulset 可以保证 Pod 重新调度后还是能访问到相同的持久化数据。\n适用 Statefulset 常用的服务有 Elasticsearch 集群，Mogodb 集群，Redis 集群等等。\n1.1 特点 # 稳定、唯一的网络标识符\n如：Redis 集群，在 Redis 集群中，它是通过槽位来存储数据的，假如：第一个节点是 0~1000，第二个节点是 1001~2000，第三个节点 2001~3000……，这就使得 Redis 集群中每个节点要通过 ID 来标识自己，如：第二个节点宕机了，重建后它必须还叫第二个节点，或者说第二个节点叫 R2，它必须还叫 R2，这样在获取 1001~2000 槽位的数据时，才能找到数据，否则 Redis 集群将无法找到这段数据。\n稳定、持久的存储\n可实现持久存储，新增或减少 Pod，存储不会随之发生变化。\n有序的、平滑的部署、扩容\n如 MySQL 集群，要先启动主节点， 若从节点没有要求，则可一起启动，若从节点有启动顺序要求，可先启动第一个从节点，接着第二从节点等；这个过程就是有顺序，平滑安全的启动。\n有序的、自动的缩容和删除\n我们先终止从节点，若从节点是有启动顺序的，那么关闭时，也要按照逆序终止，即启动时是从 S1~S4 以此启动，则关闭时，则是先关闭 S4，然后是 S3，依次关闭，最后在关闭主节点。\n有序的、自动的滚动更新\nMySQL 在更新时，应该先更新从节点，全部的从节点都更新完了，最后在更新主节点，因为新版本一般可兼容老版本，但是一定要注意，若新版本不兼容老版本就很很麻烦\n1.2 限制 # Pod 的存储必须使用 PersistVolume 删除或者缩容时，不会删除关联的卷 使用 headless service 关联 Pod，需要手动创建 Pod 的管理策略为 OrderedReady 时使用滚动更新能力，可能需要人工干预 1.3 基本功能 # 创建 删除 级联删除 非级连删除 扩容/缩容：扩容为顺序，缩容则为逆运算，即逆序 更新：与无状态应用不同的是，StatefulSet 是基于 ControllerRevision 保存更新记录 RollingUpdate OnDelete Pod 管理策略：对于某些分布式系统来说，StatefulSet 的顺序性保证是不必要的，所以引入了 statefulset.spec.podManagementPolicy 字段 OrderedReady Parallel：允许 StatefulSet Controller 并行终止所有 Pod，不必按顺序启动或删除 Pod 1.4 示例 # apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None # headless service selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx\u0026#34; podManagementPolicy: \u0026#34;OrderedReady\u0026#34; # default is OrderedReady replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi 2. 源码解析 # kubernetes version: v1.19\n2.1 startStatefulSetController() # startStatefulSetController() 是 StatefulSet Controller 的启动方法，其中调用 statefulset.NewStatefulSetController() 方法进行初始化，然后调用对象的 Run() 方法启动 Controller。其中 ConcurrentStatefulSetSyncs 默认是 5，即默认启动 5 个协程处理 StatefulSet 相关业务。\n可以看到 StatefulSetController 初始化时直接相关的对象类型分别是 Pod、StatefulSet、PVC 和 ControllerRevision。印证了之前提到的 StatefulSet 的特殊之处：使用 PV 作为存储；ControllerRevision 表示升级/回滚记录。\nfunc startStatefulSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \u0026#34;apps\u0026#34;, Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;statefulsets\u0026#34;}] { return nil, false, nil } go statefulset.NewStatefulSetController( ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Apps().V1().StatefulSets(), ctx.InformerFactory.Core().V1().PersistentVolumeClaims(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.ClientBuilder.ClientOrDie(\u0026#34;statefulset-controller\u0026#34;), ).Run(int(ctx.ComponentConfig.StatefulSetController.ConcurrentStatefulSetSyncs), ctx.Stop) return nil, true, nil } 2.2 ssc.sync() # run() 方法会通过 informer 同步 cache 并监听 pod、statefulset、pvc 和 controllerrevision 对象的变更事件，然后启动 5 个 worker 协程，每个 worker 调用 sync() 方法，正式进入业务逻辑处理。\nfunc (ssc *StatefulSetController) sync(key string) error { ... // 1. 解析 namespace 和 name namespace, name, err := cache.SplitMetaNamespaceKey(key) ... // 2. 根据 ns 和 name，获取 sts 对象 set, err := ssc.setLister.StatefulSets(namespace).Get(name) ... // 3. 获取 selector 对象，用于筛选 Pod selector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector) ... // ssc.adoptOrphanRevisions() // -\u0026gt; ssc.control.AdoptOrphanRevisions() // -\u0026gt; ssc.controllerHistory.AdoptControllerRevision() // -\u0026gt; Patch() // 4、筛选 sts 的孤儿 controllerrevisions，并尝试与 sts 重新关联（添加 ControllerRef） if err := ssc.adoptOrphanRevisions(set); err != nil { return err } // 5. 获取 sts 所有关联的 pod // ssc.getPodsForStatefulSet() // -\u0026gt; cm.ClaimPods() // -\u0026gt; m.ClaimObject() // -\u0026gt; release()/adopt() pods, err := ssc.getPodsForStatefulSet(set, selector) ... // 6. 真正执行 sync 操作 return ssc.syncStatefulSet(set, pods) } 则，sync() 的主要逻辑为：\n根据 ns/name 获取 sts 对象； 获取 sts 的 selector； 调用 ssc.adoptOrphanRevisions() 检查是否有孤儿 controllerrevisions 对象，若有且能匹配 selector 的则添加 ownerReferences 进行关联； 调用 ssc.getPodsForStatefulSet 通过 selector 获取 sts 关联的 pod，若有孤儿 pod 的 label 与 sts 的能匹配则进行关联，若已关联的 pod label 有变化则解除与 sts 的关联关系； 最后调用 ssc.syncStatefulSet 执行真正的 sync 操作； 2.3 ssc.syncStatefulSet() # 在 syncStatefulSet() 中仅仅是调用了 ssc.control.UpdateStatefulSet() 方法进行处理。ssc.control.UpdateStatefulSet() 会调用 ssc.performUpdate() 方法，最终走到更新逻辑 ssc.updateStatefulSet() 和 ssc.updateStatefulSetStatus()。\nfunc (ssc *StatefulSetController) syncStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error { ... // ssc.control.UpdateStatefulSet() // -\u0026gt; ssc.performUpdate() // -\u0026gt; ssc.updateStatefulSet() + ssc.updateStatefulSetStatus() if err := ssc.control.UpdateStatefulSet(set.DeepCopy(), pods); err != nil { return err } ... return nil } func (ssc *defaultStatefulSetControl) UpdateStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error { // 1. 获取历史 revisions，并排序 revisions, err := ssc.ListRevisions(set) ... history.SortControllerRevisions(revisions) // 2. 计算 currentRevision 和 updateRevision currentRevision, updateRevision, err := ssc.performUpdate(set, pods, revisions) if err != nil { return utilerrors.NewAggregate([]error{err, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)}) } // 3. 清理过期的历史版本 return ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision) } func (ssc *defaultStatefulSetControl) performUpdate( set *apps.StatefulSet, pods []*v1.Pod, revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, error) { // 2.1. 计算 currentRevision 和 updateRevision currentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions) if err != nil { return currentRevision, updateRevision, err } // 2.2. 执行实际的 sync 操作 status, err := ssc.updateStatefulSet(set, currentRevision, updateRevision, collisionCount, pods) if err != nil { return currentRevision, updateRevision, err } // 2.3. 更新 sts 状态 err = ssc.updateStatefulSetStatus(set, status) if err != nil { return currentRevision, updateRevision, err } ... return currentRevision, updateRevision, nil } 整体来看，ssc.control.UpdateStatefulSet() 方法的主要逻辑为：\n获取历史 revisions； 计算 currentRevision 和 updateRevision，若 sts 处于更新过程中则 currentRevision 和 updateRevision 值不同； 调用 ssc.updateStatefulSet() 执行实际的 sync 操作； 调用 ssc.updateStatefulSetStatus() 更新 status 子资源； 根据 sts 的 spec.revisionHistoryLimit 字段清理过期的 controllerrevision； 2.4 ssc.updateStatefulSet() # sts 通过 controllerrevision 保存历史版本，类似于 deployment 的 replicaset，与 replicaset 不同的是 controllerrevision 仅用于回滚阶段，在 sts 的滚动升级过程中是通过 currentRevision 和 updateRevision 进行控制并不会用到 controllerrevision。\nfunc (ssc *defaultStatefulSetControl) updateStatefulSet(...) (*apps.StatefulSetStatus, error) { // 1. 分别获取 currentRevision 和 updateRevision 对应的的 statefulset object currentSet, err := ApplyRevision(set, currentRevision) ... updateSet, err := ApplyRevision(set, updateRevision) ... // 设置 status 的 generation 和 revisions status := apps.StatefulSetStatus{} status.ObservedGeneration = set.Generation status.CurrentRevision = currentRevision.Name status.UpdateRevision = updateRevision.Name status.CollisionCount = new(int32) *status.CollisionCount = collisionCount // 3. 将 statefulset 的 pods 按序分到 replicas 和 condemned 两个切片 replicaCount := int(*set.Spec.Replicas) // 用于保存序号在 [0,replicas) 范围内的 pod replicas := make([]*v1.Pod, replicaCount) // 用于序号大于 replicas 的 Pod condemned := make([]*v1.Pod, 0, len(pods)) unhealthy := 0 firstUnhealthyOrdinal := math.MaxInt32 var firstUnhealthyPod *v1.Pod // 4. 计算 status 字段中的值，将 pod 分配到 replicas 和 condemned 两个数组中 for i := range pods { status.Replicas++ // 计算 Ready 的副本数 if isRunningAndReady(pods[i]) { status.ReadyReplicas++ } // 计算当前的副本数和已经更新的副本数 if isCreated(pods[i]) \u0026amp;\u0026amp; !isTerminating(pods[i]) { if getPodRevision(pods[i]) == currentRevision.Name { status.CurrentReplicas++ } if getPodRevision(pods[i]) == updateRevision.Name { status.UpdatedReplicas++ } } if ord := getOrdinal(pods[i]); 0 \u0026lt;= ord \u0026amp;\u0026amp; ord \u0026lt; replicaCount { // 保存序号在 [0,replicas) 范围内的 Pod replicas[ord] = pods[i] } else if ord \u0026gt;= replicaCount { // 序号大于 replicas 的 Pod，则保存在 condemned condemned = append(condemned, pods[i]) } // 其余的 Pod 忽略 } // 5. 检查 replicas 数组中 [0,set.Spec.Replicas) 下标是否有缺失的 pod，若有缺失的则创建对应的 Pod // 在 newVersionedStatefulSetPod 中会判断是使用 currentSet 还是 updateSet 来创建 for ord := 0; ord \u0026lt; replicaCount; ord++ { if replicas[ord] == nil { replicas[ord] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, ord) } } // 6. 对 condemned 数组进行排序 sort.Sort(ascendingOrdinal(condemned)) // 7、根据 ordinal 在 replicas 和 condemned 数组中找出第一个处于 NotReady 或 Terminating 的 Pod for i := range replicas { if !isHealthy(replicas[i]) { unhealthy++ if ord := getOrdinal(replicas[i]); ord \u0026lt; firstUnhealthyOrdinal { firstUnhealthyOrdinal = ord firstUnhealthyPod = replicas[i] } } } for i := range condemned { if !isHealthy(condemned[i]) { unhealthy++ if ord := getOrdinal(condemned[i]); ord \u0026lt; firstUnhealthyOrdinal { firstUnhealthyOrdinal = ord firstUnhealthyPod = condemned[i] } } } if unhealthy \u0026gt; 0 { klog.V(4).Infof(\u0026#34;StatefulSet %s/%s has %d unhealthy Pods starting with %s\u0026#34;, set.Namespace, set.Name, unhealthy, firstUnhealthyPod.Name) } // 8. 如果 sts 正在删除中，只要更新 status 即可 if set.DeletionTimestamp != nil { return \u0026amp;status, nil } // 9. 默认设置为非 Parallel monotonic := !allowsBurst(set) // 10. 确保 replicas 数组中所有的 pod 是 running 的 for i := range replicas { // 11. 对于 failed 的 pod 删除并重建 if isFailed(replicas[i]) { ssc.recorder.Eventf(set, v1.EventTypeWarning, \u0026#34;RecreatingFailedPod\u0026#34;, \u0026#34;StatefulSet %s/%s is recreating failed Pod %s\u0026#34;, set.Namespace, set.Name, replicas[i].Name) // 删除 if err := ssc.podControl.DeleteStatefulPod(set, replicas[i]); err != nil { return \u0026amp;status, err } if getPodRevision(replicas[i]) == currentRevision.Name { status.CurrentReplicas-- } if getPodRevision(replicas[i]) == updateRevision.Name { status.UpdatedReplicas-- } status.Replicas-- // 新建 replicas[i] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, i) } // 12、如果 pod.Status.Phase 不为空，说明该 pod 未创建，则直接重新创建该 pod if !isCreated(replicas[i]) { if err := ssc.podControl.CreateStatefulPod(set, replicas[i]); err != nil { return \u0026amp;status, err } status.Replicas++ if getPodRevision(replicas[i]) == currentRevision.Name { status.CurrentReplicas++ } if getPodRevision(replicas[i]) == updateRevision.Name { status.UpdatedReplicas++ } // 13. 如果为 Parallel，直接 return status 结束；如果为 OrderedReady，循环处理下一个 pod。 if monotonic { return \u0026amp;status, nil } // pod 创建完成，本轮循环结束 continue } // 14、如果 pod 正在删除中，且 Spec.PodManagementPolicy 不为 Parallel， // 直接 return status 结束，结束后会在下一个 syncLoop 继续进行处理，pod 状态的改变会触发下一次 syncLoop if isTerminating(replicas[i]) \u0026amp;\u0026amp; monotonic { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to Terminate\u0026#34;, set.Namespace, set.Name, replicas[i].Name) return \u0026amp;status, nil } // 15. 如果 pod 状态不是 Running \u0026amp; Ready，且 Spec.PodManagementPolicy 不为 Parallel // 则直接 return status 结束 if !isRunningAndReady(replicas[i]) \u0026amp;\u0026amp; monotonic { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to be Running and Ready\u0026#34;, set.Namespace, set.Name, replicas[i].Name) return \u0026amp;status, nil } // 16. 检查 pod 的信息是否与 statefulset 的匹配，若不匹配则更新 pod 的状态 if identityMatches(set, replicas[i]) \u0026amp;\u0026amp; storageMatches(set, replicas[i]) { continue } // 深拷贝对象，避免修改缓存 replica := replicas[i].DeepCopy() if err := ssc.podControl.UpdateStatefulPod(updateSet, replica); err != nil { return \u0026amp;status, err } } // 17. 逆序处理 condemned 中的 pod for target := len(condemned) - 1; target \u0026gt;= 0; target-- { // 18. 如果 pod 正在删除，检查 Spec.PodManagementPolicy 的值 // 如果为 Parallel，循环处理下一个 pod 否则直接退出 if isTerminating(condemned[target]) { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to Terminate prior to scale down\u0026#34;, set.Namespace, set.Name, condemned[target].Name) // 如果不为 Parallel，直接 return if monotonic { return \u0026amp;status, nil } continue } // 19. 不满足以下条件说明该 pod 是更新前创建的，正处于创建中 if !isRunningAndReady(condemned[target]) \u0026amp;\u0026amp; monotonic \u0026amp;\u0026amp; condemned[target] != firstUnhealthyPod { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to be Running and Ready prior to scale down\u0026#34;, set.Namespace, set.Name, firstUnhealthyPod.Name) return \u0026amp;status, nil } klog.V(2).Infof(\u0026#34;StatefulSet %s/%s terminating Pod %s for scale down\u0026#34;, set.Namespace, set.Name, condemned[target].Name) // 20、否则直接删除该 pod if err := ssc.podControl.DeleteStatefulPod(set, condemned[target]); err != nil { return \u0026amp;status, err } if getPodRevision(condemned[target]) == currentRevision.Name { status.CurrentReplicas-- } if getPodRevision(condemned[target]) == updateRevision.Name { status.UpdatedReplicas-- } // 21. 如果为 OrderedReady 方式则返回否则继续处理下一个 pod if monotonic { return \u0026amp;status, nil } } // 22. 对于 OnDelete 策略直接返回 if set.Spec.UpdateStrategy.Type == apps.OnDeleteStatefulSetStrategyType { return \u0026amp;status, nil } // 23. 若为 RollingUpdate 策略，则倒序处理 replicas 数组中下标大于等于 // Spec.UpdateStrategy.RollingUpdate.Partition 的 pod updateMin := 0 if set.Spec.UpdateStrategy.RollingUpdate != nil { updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition) } // 用与更新版本不匹配的最大序号终止 Pod for target := len(replicas) - 1; target \u0026gt;= updateMin; target-- { // 24. 如果 Pod 的 Revision 不等于 updateRevision，且 pod 没有处于删除状态，则直接删除 pod if getPodRevision(replicas[target]) != updateRevision.Name \u0026amp;\u0026amp; !isTerminating(replicas[target]) { klog.V(2).Infof(\u0026#34;StatefulSet %s/%s terminating Pod %s for update\u0026#34;, set.Namespace, set.Name, replicas[target].Name) err := ssc.podControl.DeleteStatefulPod(set, replicas[target]) status.CurrentReplicas-- return \u0026amp;status, err } // 25. 如果 pod 非 healthy 状态直接返回 if !isHealthy(replicas[target]) { klog.V(4).Infof( \u0026#34;StatefulSet %s/%s is waiting for Pod %s to update\u0026#34;, set.Namespace, set.Name, replicas[target].Name) return \u0026amp;status, nil } } return \u0026amp;status, nil } 综上，updateStatefulSet() 把 statefulset 的创建、删除、更新、扩缩容的操作都包含在内。主要逻辑为：\n分别获取 currentRevision 和 updateRevision 所对应的 sts 对象 取出 sts.status 并设置相关新值，用于更新 将 sts 关联的 Pod，按照序号分到 replicas 和 condemned 两个切片中，replicas 保存的是序号在 [0, spec.replicas) 之间的 Pod，表示可用，condemned 保存序号大于 spec.replicas 的 Pod，表示待删除； 找出 replicas 和 condemned 组中的 unhealthy pod，healthy pod 指 running \u0026amp; ready 并且不处于删除状态； 判断 sts 是否处于删除状态； 遍历 replicas，确保其中的 Pod 处于 running \u0026amp; ready 状态，其中处于 Failed 状态的 Pod 删除重建；未创建的容器则直接创建；处于删除中的，等待优雅删除结束，即下一轮循环再处理；最后检查 pod 的信息是否与 statefulset 的匹配，若不匹配则更新 pod。在此过程中每一步操作都会检查 .Spec.podManagementPolicy 是否为 Parallel，若设置了则循环处理 replicas 中的所有 pod，否则每次处理一个 pod，剩余 pod 则在下一个 syncLoop 继续进行处理； 按 pod 名称逆序删除 condemned 数组中的 pod，删除前也要确保 pod 处于 running \u0026amp; ready 状态，在此过程中也会检查 .Spec.podManagementPolicy 是否为 Parallel，以此来判断是顺序删除还是在下一个 syncLoop 中继续进行处理； 判断 sts 的更新策略 .Spec.UpdateStrategy.Type，若为 OnDelete 则直接返回； 此时更新策略为 RollingUpdate，更新序号大于等于 .Spec.UpdateStrategy.RollingUpdate.Partition 的 pod；更新策略为 RollingUpdate，并不会关注 .Spec.podManagementPolicy，都是顺序进行处理，且等待当前 pod 删除成功后才继续逆序删除一下 pod，所以 Parallel 的策略在滚动更新时无法使用。 updateStatefulSet() 这个方法中包含了 statefulset 的创建、删除、扩缩容、更新等操作，在源码层面对于各个功能无法看出明显的界定，没有 deployment sync 方法中写的那么清晰，下面按 statefulset 的功能再分析一下具体的操作：\n创建：在创建 sts 后，sts 对象已被保存至 etcd 中，此时 sync 操作仅仅是创建出需要的 pod，即执行到第 6 步就会结束； 扩缩容：对于扩若容操作仅仅是创建或者删除对应的 pod，在操作前也会判断所有 pod 是否处于 running \u0026amp; ready 状态，然后进行对应的创建/删除操作，在上面的步骤中也会执行到第 6 步就结束； 更新：可以看出在第 6 步之后的所有操作就是与更新相关，所以更新操作会执行完整个方法，在更新过程中通过 pod 的 currentRevision 和 updateRevision 来计算 currentReplicas、updatedReplicas 的值，最终完成所有 pod 的更新； 删除：删除操作就比较明显，会止于第 5 步，但是在此之前检查 pod 状态以及分组的操作确实是多余的； 3. 参考链接 # StatefulSet 概念 StatefulSet 基础 StatefulSet 源码解析 "},{"id":16,"href":"/docs/kubernetes/kube-controller-manager/k8s-apps-rolling-update/","title":"无状态应用滚动更新","section":"kube-controller-manager","content":" 1. 概念 # 滚动更新，通常出现在软件或者是系统中。滚动更新与传统更新的不同之处在于： 滚动更新不但提供了更新服务，而且通常还提供了滚动进度查询，滚动历史记录， 以及最重要的回滚等能力。通俗地说，就是具有系统或是软件的主动降级的能力。\n2. Deployment 滚动更新 # Deployment 更新方式有 2 种：\nRollingUpdate Recreate 其中，滚动更新是最常见的，阅读代码 pkg/controller/deployment/deployment_controller.go:648， 可以看到 2 种方式分别对应的业务逻辑：\nfunc (dc *DeploymentController) syncDeployment(key string) error { ... switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) } ... } 根据 d.Spec.Strategy.Type，若更新策略为 RollingUpdate， 则执行 dc.rolloutRecreate() 方法，具体逻辑如下：\nfunc (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error { // 1、获取所有的 rs，若没有 newRS 则创建 newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs := append(oldRSs, newRS) // 2、newRS 执行 scale up 操作 scaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d) if err != nil { return err } if scaledUp { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } // 3、oldRS 执行 scale down 操作 scaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } // 4、清理过期的 rs if deploymentutil.DeploymentComplete(d, \u0026amp;d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // 5、同步 deployment 状态 return dc.syncRolloutStatus(allRSs, newRS, d) } 2.1 滚动更新概述 # 上面代码中 5 个重要的步骤总结如下：\n调用 getAllReplicaSetsAndSyncRevision() 获取所有的 rs，若没有 newRS 则创建； 调用 reconcileNewReplicaSet() 判断是否需要对 newRS 进行 scaleUp 操作；如果需要 scaleUp，更新 Deployment 的 status， 添加相关的 condition，该 condition 的 type 是 Progressing，表明该 deployment 正在更新中，然后直接返回； 调用 reconcileOldReplicaSets() 判断是否需要为 oldRS 进行 scaleDown 操作；如果需要 scaleDown， 把 oldRS 关联的 pod 删掉 maxScaledDown 个，然后更新 Deployment 的 status，添加相关的 condition，直接返回。 这样一来就保证了在滚动更新过程中，新老版本的 Pod 都存在； 如果两者都不是则滚动升级很可能已经完成，此时需要检查 deployment.Status 是否已经达到期望状态， 并且根据 deployment.Spec.RevisionHistoryLimit 的值清理 oldRSs； 最后，同步 deployment 的状态，使其与期望一致； 从上面的步骤可以看出，滚动更新的过程主要分成一下三个阶段：\ngraph LR\rstart(start) --\u003e condtion1{newRS need scale up ?}\rcondtion1 -- No --\u003e condtion2{oldRS need scale down ?}\rcondtion2 -- NO --\u003e x3(3. sync deploment status)\rcondtion1 -- YES --\u003e x1(1. newRS scale up)\rx1 --\u003e stop(end)\rcondtion2 -- YES --\u003e x2(2. oldRS scale down)\rx2 --\u003e stop\rx3 --\u003e stop\r2.1.1 newRS scale up # 阅读代码 pkg/controller/deployment/rolling.go:68，详细如下：\nfunc (dc *DeploymentController) reconcileNewReplicaSet(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { // 1、判断副本数是否已达到了期望值 if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) { // Scaling not required. return false, nil } // 2、判断是否需要 scale down 操作 if *(newRS.Spec.Replicas) \u0026gt; *(deployment.Spec.Replicas) { // Scale down. scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment) return scaled, err } // 3、计算 newRS 所需要的副本数 newReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS) if err != nil { return false, err } // 4、如果需要 scale ，则更新 rs 的 annotation 以及 rs.Spec.Replicas scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment) return scaled, err } 从上面的源码可以得出，reconcileNewReplicaSet() 的主要逻辑如下：\n判断 newRS.Spec.Replicas 和 deployment.Spec.Replicas 是否相等， 如果相等则直接返回，说明已经达到期望状态； 若 newRS.Spec.Replicas \u0026gt; deployment.Spec.Replicas， 则说明 newRS 副本数已经超过期望值，调用 dc.scaleReplicaSetAndRecordEvent() 进行 scale down； 此时 newRS.Spec.Replicas \u0026lt; deployment.Spec.Replicas， 调用 deploymentutil.NewRSNewReplicas() 为 newRS 计算所需要的副本数， 计算原则遵守 maxSurge 和 maxUnavailable 的约束； 调用 dc.scaleReplicaSetAndRecordEvent() 更新 newRS 对象，设置 rs.Spec.Replicas、rs.Annotations[DesiredReplicasAnnotation] 以及 rs.Annotations[MaxReplicasAnnotation]； 其中，计算 newRS 的副本数，是滚动更新核心过程的第一步， 阅读源码 pkg/controller/deployment/util/deployment_util.go:816：\nfunc NewRSNewReplicas(deployment *apps.Deployment, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet) (int32, error) { switch deployment.Spec.Strategy.Type { case apps.RollingUpdateDeploymentStrategyType: // 1、计算 maxSurge 值，向上取整 maxSurge, err := intstrutil.GetValueFromIntOrPercent(deployment.Spec.Strategy.RollingUpdate.MaxSurge, int(*(deployment.Spec.Replicas)), true) if err != nil { return 0, err } // 2、累加 rs.Spec.Replicas 获取 currentPodCount currentPodCount := GetReplicaCountForReplicaSets(allRSs) maxTotalPods := *(deployment.Spec.Replicas) + int32(maxSurge) if currentPodCount \u0026gt;= maxTotalPods { // Cannot scale up. return *(newRS.Spec.Replicas), nil } // 3、计算 scaleUpCount，结果不超过期望值 scaleUpCount := maxTotalPods - currentPodCount scaleUpCount = int32(integer.IntMin(int(scaleUpCount), int(*(deployment.Spec.Replicas)-*(newRS.Spec.Replicas)))) return *(newRS.Spec.Replicas) + scaleUpCount, nil case apps.RecreateDeploymentStrategyType: return *(deployment.Spec.Replicas), nil default: return 0, fmt.Errorf(\u0026#34;deployment type %v isn\u0026#39;t supported\u0026#34;, deployment.Spec.Strategy.Type) } } 可知 NewRSNewReplicas() 的主要逻辑如下：\n判断更新策略； 计算 maxSurge 值； 通过 allRSs 计算 currentPodCount 的值； 最后计算 scaleUpCount 值； 2.1.2 oldRS scale down # 同理，oldRS 规模缩小，阅读源码 pkg/controller/deployment/rolling.go:68:\nfunc (dc *DeploymentController) reconcileOldReplicaSets(allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { // 1、计算 oldPodsCount oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs) if oldPodsCount == 0 { // Can\u0026#39;t scale down further return false, nil } // 2、计算 maxUnavailable allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) klog.V(4).Infof(\u0026#34;New replica set %s/%s has %d available pods.\u0026#34;, newRS.Namespace, newRS.Name, newRS.Status.AvailableReplicas) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) // 3、计算 maxScaledDown minAvailable := *(deployment.Spec.Replicas) - maxUnavailable newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount if maxScaledDown \u0026lt;= 0 { return false, nil } // 4、清理异常的 rs oldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(oldRSs, deployment, maxScaledDown) if err != nil { return false, nil } klog.V(4).Infof(\u0026#34;Cleaned up unhealthy replicas from old RSes by %d\u0026#34;, cleanupCount) // 5、缩容 old rs allRSs = append(oldRSs, newRS) scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(allRSs, oldRSs, deployment) if err != nil { return false, nil } klog.V(4).Infof(\u0026#34;Scaled down old RSes of deployment %s by %d\u0026#34;, deployment.Name, scaledDownCount) totalScaledDown := cleanupCount + scaledDownCount return totalScaledDown \u0026gt; 0, nil } 通过上面的代码可知，reconcileOldReplicaSets() 的主要逻辑如下：\n通过 oldRSs 和 allRSs 获取 oldPodsCount 和 allPodsCount； 计算 deployment 的 maxUnavailable、minAvailable、newRSUnavailablePodCount、maxScaledDown 值， 当 deployment 的 maxSurge 和 maxUnavailable 值为百分数时， 计算 maxSurge 向上取整而 maxUnavailable 则向下取整； 清理异常的 rs； 计算 oldRS 的 scaleDownCount； 最后 oldRS 缩容； 2.2 滚动更新总结 # 通过上面的代码可以看出，滚动更新过程中主要是通过调用 reconcileNewReplicaSet() 对 newRS 不断扩容， 调用 reconcileOldReplicaSets() 对 oldRS 不断缩容，最终达到期望状态，并且在整个升级过程中， 都严格遵守 maxSurge 和 maxUnavailable 的约束。\n不论是在 scale up 或者 scale down 中都是调用 scaleReplicaSetAndRecordEvent() 执行， 而 scaleReplicaSetAndRecordEvent() 又会调用 scaleReplicaSet()， 扩缩容都是更新 rs.Annotations 以及 rs.Spec.Replicas。\n整体流程如下图所示：\ngraph LR\rop1(newRS scale up) --\u003e op3[dc.scaleReplicaSetAndRecordEvent]\rop2(oldRS scale down) --\u003e op3(dc.scaleReplicaSetAndRecordEvent)\rop3(dc.scaleReplicaSetAndRecordEvent) --\u003e op4(dc.scaleReplicaSet)\r2.3 滚动更新示例 # 创建一个 deployment，replica = 10 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 10 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.18.0 ports: - containerPort: 80 10 个 Pod 创建成功后如下所示：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-67dfd6c8f9 10 10 10 70s 更新 nginx-deployment 的镜像，默认使用滚动更新的方式 $ kubectl set image deploy/nginx-deployment nginx-deployment=nginx:1.19.1 此时通过源码可知会计算该 deployment 的 maxSurge=3，maxUnavailable=2，maxAvailable=13，计算方法如下所示：\n// 向上取整 maxSurge = 10 * 0.25 = 3 maxSurge = replicas * deployment.spec.strategy.rollingUpdate.maxSurge // 向下取整 maxUnavailable = 10 * 0.25 = 2 maxUnavailable = replicas * deployment.spec.strategy.rollingUpdate.maxUnavailable // maxAvailable = 10 + 3 = 13 maxAvailable = replicas + MaxSurge 如上面代码所说，更新时首先创建 newRS，然后为其设定 replicas，计算 newRS 的 replicas 值的方法在 NewRSNewReplicas() 中， 此时计算出 replicas 结果为 3，然后更新 deployment 的 annotation，创建 events，本次 syncLoop 完成。 等到下一个 syncLoop 时，所有 rs 的 replicas 已经达到最大值 10 + 3 = 13，此时需要 oldRS 缩容。 scale down 的数量是通过以下公式得到的：\n// 13 = 10 + 3 allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) // 8 = 10 - 2 minAvailable := *(deployment.Spec.Replicas) - maxUnavailable // ??? newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas // 13 - 8 - ??? maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount allPodsCount = 13，minAvailable = 8 ，newRSUnavailablePodCount 此时不确定，但是值在 [0,3] 范围内。 此时假设 newRS 的 3 个 pod 还处于 containerCreating 状态，则 newRSUnavailablePodCount = 3， 根据以上公式计算所知 maxScaledDown = 2，则 oldRS 需要缩容 2 个 pod，其 replicas 需要改为 8，此时该 syncLoop 完成。 下一个 syncLoop 时在 scaleUp 处计算得知 scaleUpCount = 13 - 8 - 3 = 2， 此时 newRS 需要更新 replicase 增加 2。以此轮询直到 newRS 扩容到 10，oldRS 缩容至 0。\n对于上面的示例，可以使用 kubectl get rs -w 进行观察，以下为输出：\n$ kubectl get rs -w NAME DESIRED CURRENT READY AGE nginx-deployment-5bbdfb5879 10 10 5 3s nginx-deployment-67dfd6c8f9 3 3 3 4m47s nginx-deployment-5bbdfb5879 10 10 6 3s nginx-deployment-67dfd6c8f9 2 3 3 4m47s nginx-deployment-67dfd6c8f9 2 2 2 4m47s nginx-deployment-5bbdfb5879 10 10 7 4s nginx-deployment-67dfd6c8f9 1 2 2 4m48s nginx-deployment-67dfd6c8f9 1 1 1 4m48s nginx-deployment-5bbdfb5879 10 10 8 4s nginx-deployment-67dfd6c8f9 0 1 1 4m48s nginx-deployment-67dfd6c8f9 0 0 0 4m48s nginx-deployment-5bbdfb5879 10 10 9 5s nginx-deployment-5bbdfb5879 10 10 10 6s "},{"id":17,"href":"/docs/kubernetes/kube-apiserver/key-design-of-etcd-watch/","title":"watch 关键设计","section":"kube-apisever","content":" 注：本文转自 图解 kubernetes 中基于 etcd 的 watch 关键设计\n本文介绍了 kubernetes 针对 etcd 的 watch 场景，k8s 在性能优化上面的一些设计， 逐个介绍缓存、定时器、序列化缓存、bookmark 机制、forget 机制、 针对数据的索引与 ringbuffer 等组件的场景以及解决的问题， 希望能帮助到那些对 apiserver 中的 watch 机制实现感兴趣的朋友。\n1. 事件驱动与控制器 # k8s 中并没有将业务的具体处理逻辑耦合在 rest 接口中，rest 接口只负责数据的存储， 通过控制器模式，分离数据存储与业务逻辑的耦合，保证 apiserver 业务逻辑的简洁。\n控制器通过 watch 接口来感知对应的资源的数据变更，从而根据资源对象中的期望状态与当前状态之间的差异， 来决策业务逻辑的控制，watch 本质上做的事情其实就是将感知到的事件发生给关注该事件的控制器。\n2. Watch 的核心机制 # 这里我们先介绍基于 etcd 实现的基础的 watch 模块。\n2.1 事件类型与 etcd # 一个数据变更本质上无非就是三种类型：新增、更新和删除， 其中新增和删除都比较容易因为都可以通过当前数据获取，而更新则可能需要获取之前的数据， 这里其实就是借助了 etcd 中 revision 和 mvcc 机制来实现，这样就可以获取到之前的状态和更新后的状态， 并且获取后续的通知。\n2.2 事件管道 # 事件管道则是负责事件的传递，在 watch 的实现中通过两级管道来实现消息的分发， 首先通过 watch etcd 中的 key 获取感兴趣的事件，并进行数据的解析， 完成从 bytes 到内部事件的转换并且发送到输入管道 (incomingEventChan) 中， 然后后台会有线程负责输入管道中获取数据，并进行解析发送到输出管道 (resultChan) 中， 后续会从该管道来进行事件的读取发送给对应的客户端。\n2.3 事件缓冲区 # 事件缓冲区是指的如果对应的事件处理程序与当前事件发生的速率不匹配的时候， 则需要一定的 buffer 来暂存因为速率不匹配的事件， 在 go 里面大家通常使用一个有缓冲的 channel 构建。\n到这里基本上就实现了一个基本可用的 watch 服务，通过 etcd 的 watch 接口监听数据， 然后启动独立 goroutine 来进行事件的消费，并且发送到事件管道供其他接口调用。\n3. Cacher # kubernetes 中所有的数据和系统都基于 etcd 来实现，如何减轻访问压力呢， 答案就是缓存，watch 也是这样，本节我们来看看如何实现 watch 缓存机制的实现， 这里的 cacher 是针对 watch 的。\n3.1 Reflector # Reflector 是 client-go 中的一个组件，其通过 listwatch 接口获取数据存储在自己内部的 store 中， cacher 中通过该组件对 etcd 进行 watch 操作，避免为每个组件都创建一个 etcd 的 watcher。\n3.2 watchCache # watchCache 负责存储 watch 到的事件，并且将 watch 的事件建立对应的本地索引缓存， 同时在构建 watchCache 还负责将事件的传递， 其将 watch 到的事件通过 eventHandler 来传递给上层的 Cacher 组件。\n3.3 cacheWatcher # cacheWatcher 顾名思义其是就是针对 cache 的一个 watcher(watch.Interface) 实现， 前端的 watchServer 负责从 ResultChan 里面获取事件进行转发。\n3.4 Cacher # Cacher 基于 etcd 的 store 结合上面的 watchCache 和 Reflector 共同构建带缓存的 REST store， 针对普通的增删改功能其直接转发给 etcd 的 store 来进行底层的操作，而对于 watch 操作则进行拦截， 构建并返回 cacheWatcher 组件。\n4. Cacher 的优化 # 看完基础组件的实现，接着我们看下针对 watch 这个场景 k8s 中还做了那些优化，学习针对类似场景的优化方案。\n4.1 序列化缓存 # 如果我们有多个 watcher 都 watch 同一个事件，在最终的时候我们都需要进行序列化， cacher 中在分发的时候，如果发现超过指定数量的 watcher， 则会在进行 dispatch 的时候， 为其构建构建一个缓存函数，针对多个 watcher 只会进行一次的序列化。\n4.2 nonblocking # 在上面我们提到过事件缓冲区，但是如果某个 watcher 消费过慢依然会影响事件的分发， 为此 cacher 中通过是否阻塞（是否可以直接将数据写入到管道中）来将 watcher 分为两类， 针对不能立即投递事件的 watcher， 则会在后续进行重试。\n4.3 TimeBudget # 针对阻塞的 watcher 在进行重试的时候，会通过 dispatchTimeoutBudget 构建一个定时器来进行超时控制， 那什么叫 Budget 呢，其实如果在这段时间内，如果重试立马就成功，则本次剩余的时间， 在下一次进行定时的时候，则可以使用之前剩余的余额，但是后台也还有个线程，用于周期性重置。\n4.4 forget 机制 # 针对上面的 TimeBudget 如果在给定的时间内依旧无法进行重试成功， 则就会通过 forget 来删除对应的 watcher， 由此针对消费特别缓慢的 watcher 则可以通过后续的重试来重新建立 watch， 从而减小对 a piserver 的 watch 压力。\n4.5 bookmark 机制 # bookmark 机制是大阿里提供的一种优化方案，其核心是为了避免单个某个资源一直没有对应的事件， 此时对应的 informer 的 revision 会落后集群很大， bookmark 通过构建一种 BookMark 类型的事件来进行 revision 的传递， 从而让 informer 在重启后不至于落后特别多。\n4.6 watchCache 中的 ringbuffer # watchCache 中通过 store 来构建了对应的索引缓存，但是在 listwatch 操作的时候， 则通常需要获取某个 revision 后的所有数据， 针对这类数据 watchCache 中则构建了一个 ringbuffer 来进行历史数据的缓存。\n5. 设计总结 # 本文介绍了 kubernetes 针对 etcd 的 watch 场景，k8s 在性能优化上面的一些设计， 逐个介绍缓存、定时器、序列化缓存、bookmark 机制、forget 机制、 针对数据的索引与 ringbuffer 等组件的场景以及解决的问题， 希望能帮助到那些对 apiserver 中的 watch 机制实现感兴趣的朋友。\n"},{"id":18,"href":"/docs/kubernetes/kubelet/kubelet-topology-manager/","title":"Topology Manager 设计方案","section":"kubelet","content":" 注：本文翻译自 Node Topology Manager\n1. 概要 # 越来越多的系统将 CPU 和硬件加速器组合使用，以支撑高延迟和搞吞吐量的并行计算。 包括电信、科学计算、机器学习、金融服务和数据分析等领域的工作。这种的混血儿构成了一个高性能环境。\n为了达到最优性能，需要对 CPU 隔离、内存和设备的物理位置进行优化。 然而，在 kubernetes 中，这些优化没有一个统一的组件管理。\n本次建议提供一个新机制，可以协同 kubernetes 各个组件，对硬件资源的分配可以有不同的细粒度。\n2. 启发 # 当前，kubelet 中多个组件决定系统拓扑相关的分配：\nCPU 管理器 CPU 管理器限制容器可以使用的 CPU。该能力，在 1.8 只实现了一种策略——静态分配。 该策略不支持在容器的生命周期内，动态上下线 CPU。 设备管理器 设备管理器将某个具体的设备分配给有该设备需求的容器。设备通常是在外围互连线上。 如果设备管理器和 CPU 管理器策略不一致，那么 CPU 和设备之间的所有通信都可能导致处理器互连结构上的额外跳转。 容器运行时（CNI） 网络接口控制器，包括 SR-IOV 虚拟功能（VF）和套接字有亲和关系，socket 不同，性能不同。 相关问题：\n节点层级的硬件拓扑感知（包括 NUMA） 发现节点的 NUMA 架构 绑定特定 CPU 支持虚拟函数 提议：CPU 亲和与 NUMA 拓扑感知 注意，以上所有的关注点都只适用于多套接字系统。 内核能从底层硬件接收精确的拓扑信息（通常是通过 SLIT 表），是正确操作的前提。 更多信息请参考 ACPI 规范的 5.2.16 和 5.2.17 节。\n2.1 目标 # 根据 CPU 管理器和设备管理器的输入，给容器选择最优的 NUMA 亲和节点。 集成 kubelet 中其他支持拓扑感知的组件，提供一个统一的内部接口。 2.2 非目标 # 设备间连接：根据直接设备互连来决定设备分配。此问题不同于套接字局部性。设备间的拓扑关系， 可以都在设备管理器中考虑，可以做到套接字的亲和性。实现这一策略，可以从逐渐支持任何设备之间的拓扑关系。 大页：本次提议有 2 个前提，一是集群中的节点已经预分配了大页； 二是操作系统能给容器做好本地页分配（只需要本地内存节点上有空闲的大页即可）。 容器网络接口：本次提议不包含修改 CNI。但是，如果 CNI 后续支持拓扑管理， 此次提出的方案应该具有良好的扩展性，以适配网络接口的局部性。对于特殊的网络需求， 可以使用设备插件 API 作为临时方案，以减少网络接口的局限性。 2.3 用户故事 # 故事 1: 快速虚拟化的网络功能\n要求在一个首选的 NUMA 节点上，既要“网络快”，又能自动完成各个组件（大页，cpu 集，网络设备）的协同。 在大多数场景下，只有极少数的 NUMA 节点才能满足。\n故事 2: 加速神经网络训练\nNUMA 节点中的已分配的 CPU 和设备，可满足神经网络训练的加速器和独占的 CPU 的需求，以达到性能最优。\n3. 提议 # 主要思想：两相拓扑一致性协议 拓扑亲和性在容器级别，与设备和 CPU 的亲和类似。在 Pod 准入期间， 一个新组件可以从设备管理器和 CPU 管理器收集 Pod 中每个容器的配置，这个组件名为拓扑管理器。 当这些组件进行资源分配时，拓扑管理器扮演本地对齐的预分配角色。 我们希望各个组件能利用 Pod 中隐含的 QoS 类型进行优先级排序，以满足局部重要性最优。\n3.1 提议修改点 # 3.1.1 新概念：拓扑管理器 # 这个提议主要关注 kubelet 中一个新组件，叫做拓扑管理器。拓扑管理器实现了 Pod 的 Admit() 接口， 并参与 kubelet 的对 Pod 的准入。当 Admit() 方法被调用，拓扑管理器根据 kubelet 标志， 逐个 Pod 或逐个容器收集 kubelet 其他组件的的拓扑信息。\n如果提示不兼容，拓扑管理器可以选择拒绝 Pod，这是由 kubelet 配置的拓扑策略所决定的。 拓扑管理器支持 4 中策略：none（默认）、best-erffort、restricted 和 single-numa-node。\n拓扑信息中包含了对本地资源的偏好。拓扑信息当前由以下组成：\n位掩码表——可能满足请求的 NUMA 节点 首选属性 属性定义如下： 每个拓扑信息提供者，都有一个满足请求的可能资源分配，这样就可以尽可能减少 NUMA 节点数量（节点为空那样计算） 有一种可能的分配方式，相关 NUMA 节点联合总量，不大于任何个单个资源的的请求量 Pod 的有效资源请求/限制\n所有提供拓扑信息的组件，都应该先考虑资源的请求和限制，再计算得出可靠的拓扑提示， 这个规则是由 init 容器的概念定义的。\nPod 对资源的请求和限制的有效值，由以下 2 个条件中较大的一个决定：\n所有 init 容器中，请求或限制的最大值（max([]initcontainer.Request)，max([]initcontainer.Limit)） 所有应用的请求和限制的总和（sum([]containers.Request)，sum([]containers.Limit)） 下面这个例子简要说明它是如何工作的：\napiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: appContainer1 resources: requests: cpu: 2 memory: 1G - name: appContainer2 resources: requests: cpu: 1 memory: 1G initContainers: - name: initContainer1 resources: requests: cpu: 2 memory: 1G - name: initContainer2 resources: requests: cpu: 2 memory: 3G #资源请求的有效值：CPU: 3, Memory: 3G debug/ephemeral 容器不能指定资源请求/限制，因为不会影响拓扑提示的结果。\n范围\n拓扑管理器将根据新 kubelet 标志 --topology-manager-scope 的值， 尝试逐个 Pod 或逐个容器地对资源进行对齐。该标志可以显示的值详细如下：\nContainer（默认）：逐个容器地收集拓扑信息。 然后，拓扑策略将为每个容器单独调整资源，只有调整成功，Pod 准入成功。\nPod：逐个 Pod 地收集拓扑信息。 然后，拓扑策略将为所有容器集体调整资源，只有调整成功，Pod 准入成功。\n策略\nnone（默认）：kubelet 不会参考拓扑管理器的决定 best-effort：拓扑管理器会基于拓扑信息，给出首选分配。 在此策略下，即使分配结果不合理，Pod 也成功准入。 restricted：与 best-effort 不同，在此策略下，如果分配结果不合理，Pod 会被拒绝。 同时，因准入失败，进入 Terminated 状态。 single-numa-node：拓扑管理器会在 NUMA 节点上强制执行资源分配，如果分配失败，Pod 会被拒绝。 同时，因准入失败，进入 Terminated 状态。 拓扑管理器组件默认被禁用，直到从 alpha 到 beta 级别解禁。\n亲和计算\n拓扑管理策略基于收集的所有拓扑信息，执行亲和计算，然后决定接受或拒绝 Pod。\n亲和算法\nbest-effort/restricted （亲和算法相同） 循环遍历所有拓扑信息提供者，并以列表保存每个信息源的返回。 遍历步骤 1 中的列表，执行按位与运算，合并为单个亲和信息。如果循环中任何字段的亲和返回了 false，则最终结果该字段也为 false。 返回的亲和信息的最小集，最小集意味着至少有一个 NUMA 节点满足资源请求。 如果没有找到任何 NUMA 节点集的提示，则返回一个默认提示，该值包含了所有 NUMA 节点，并把首选设置为 false。 single-numa-node 循环遍历所有拓扑信息提供者，并以列表保存每个信息源的返回 过滤步骤 1 中累积的列表，使其只包含具有单个 NUMA 节点和空 NUMA 节点的提示 遍历步骤 1 中的列表，执行按位与运算，合并为单个亲和信息。如果循环中任何字段的亲和返回了 false，则最终结果该字段也为 false 如果没有找到具有单 NUMA 节点集的提示，则返回一个默认提示，该提示包含所有 NUMA 节点集并且首选设置为 false。 策略决断\nbest-effort：总是遵循拓扑信息提示，准入 Pod restricted：只有拓扑提示的首选字段为 true，才准入 Pod single-numa-node：既需要拓扑的首选字段为 true，又需要位掩码设置为单个 NUMA 节点，才准入 Pod 新的接口\n清单：拓扑管理器和相关接口\npackage bitmask // BitMask interface allows hint providers to create BitMasks for TopologyHints type BitMask interface { Add(sockets ...int) error Remove(sockets ...int) error And(masks ...BitMask) Or(masks ...BitMask) Clear() Fill() IsEqual(mask BitMask) bool IsEmpty() bool IsSet(socket int) bool IsNarrowerThan(mask BitMask) bool String() string Count() int GetSockets() []int } func NewBitMask(sockets ...int) (BitMask, error) { ... } package topologymanager // Manager interface provides methods for Kubelet to manage Pod topology hints type Manager interface { // Implements Pod admit handler interface lifecycle.PodAdmitHandler // Adds a hint provider to manager to indicate the hint provider //wants to be consoluted when making topology hints AddHintProvider(HintProvider) // Adds Pod to Manager for tracking AddContainer(Pod *v1.Pod, containerID string) error // Removes Pod from Manager tracking RemoveContainer(containerID string) error // Interface for storing Pod topology hints Store } // TopologyHint encodes locality to local resources. Each HintProvider provides // a list of these hints to the TopoologyManager for each container at Pod // admission time. type TopologyHint struct { NUMANodeAffinity bitmask.BitMask // Preferred is set to true when the BitMask encodes a preferred // allocation for the Container. It is set to false otherwise. Preferred bool } // HintProvider is implemented by Kubelet components that make // topology-related resource assignments. The Topology Manager consults each // hint provider at Pod admission time. type HintProvider interface { // GetTopologyHints returns a map of resource names with a list of possible // resource allocations in terms of NUMA locality hints. Each hint // is optionally marked \u0026#34;preferred\u0026#34; and indicates the set of NUMA nodes // involved in the hypothetical allocation. The topology manager calls // this function for each hint provider, and merges the hints to produce // a consensus \u0026#34;best\u0026#34; hint. The hint providers may subsequently query the // topology manager to influence actual resource assignment. GetTopologyHints(Pod v1.Pod, containerName string) map[string][]TopologyHint // GetPodLevelTopologyHints returns a map of resource names with a list of // possible resource allocations in terms of NUMA locality hints. // The returned map contains TopologyHint of requested resource by all containers // in a Pod spec. GetPodLevelTopologyHints(Pod *v1.Pod) map[string][]TopologyHint // Allocate triggers resource allocation to occur on the HintProvider after // all hints have been gathered and the aggregated Hint is available via a // call to Store.GetAffinity(). Allocate(Pod *v1.Pod, container *v1.Container) error } // Store manages state related to the Topology Manager. type Store interface { // GetAffinity returns the preferred affinity as calculated by the // TopologyManager across all hint providers for the supplied Pod and // container. GetAffinity(PodUID string, containerName string) TopologyHint } // Policy interface for Topology Manager Pod Admit Result type Policy interface { // Returns Policy Name Name() string // Returns a merged TopologyHint based on input from hint providers // and a Pod Admit Handler Response based on hints and policy type Merge(providersHints []map[string][]TopologyHint) (TopologyHint, lifecycle.PodAdmitResult) } 图：拓扑管理器组件\n图：拓扑管理器实例化并出现在 Pod 准入生命周期中\n3.1.2 特性门禁和 kubelet 启动参数 # 将添加一个特性门禁，控制拓扑管理器特性的启动。此门禁将在 Kubelet 启用，并在 Alpha 版本中默认关闭。\n门禁定义建议：\n--feature-gate=TopologyManager=true\n如上所述，kubelet 还新增一个标志，用于标识拓扑管理器策略。默认策略将会是 none。\n策略标志建议：\n--topology-manager-policy=none|best-effort|restricted|single-numa-node\n根据选择的策略，以下标志将确定应用策略的范围（逐个 Pod 或逐个容器）。范围的默认值是 container\n范围标志建议：\n--topology-manager-scope=container|Pod\n3.1.3 现有组件变更 # Kubelet 向拓扑管理器咨询 Pod 准入（上面讨论过） 添加两个拓扑管理器接口的实现和一个特性门禁 当功能门被禁用时，尽可能保证拓扑管理器功能失效。 添加一个功能性拓扑管理器，用来查询拓扑信息，以便为每个容器计算首选套接字掩码。 CPU 管理器添加 2 个方法：GetTopologyHints() 和 GetPodLevelTopologyHints() CPU 管理器的 static 策略在决定 CPU 的亲和性是，调用拓扑管理器的 GetAffinity() 方法 设备管理器添加 2 个方法：GetTopologyHints() 和 GetPodLevelTopologyHints() 在设备插件接口的设备结构中添加 TopologyInfo。 插件在枚举受支持的设备时应该能够确定 NUMA 节点。请参阅下面的协议差异。 设备管理器决定设备分配时，调用拓扑管理器的 GetAffinity() 方法 清单：修改后的设备插件 gRPC 协议\ndiff --git a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto index efbd72c133..f86a1a5512 100644 --- a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto +++ b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto @@ -73,6 +73,10 @@ message ListAndWatchResponse { repeated Device devices = 1; } +message TopologyInfo { + repeated NUMANode nodes = 1; +} + +message NUMANode { + int64 ID = 1; +} + /* E.g: * struct Device { * ID: \u0026#34;GPU-fef8089b-4820-abfc-e83e-94318197576e\u0026#34;, * State: \u0026#34;Healthy\u0026#34;, + * Topology: + * Nodes: + * ID: 1 @@ -85,6 +89,8 @@ message Device { string ID = 1; // Health of the device, can be healthy or unhealthy, see constants.go string health = 2; +\t// Topology details of the device +\tTopologyInfo topology = 3; } 图：拓扑管理器提示提供者注册\n图：拓扑管理器从 HintProvider 获取拓扑提示\n此外，我们提议将设备插件接口扩展为“最后一级”过滤器，以帮助影响设备管理器做出的总体分配决策。 下面的差异显示了提议的改动：\ndiff --git a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto index 758da317fe..1e55d9c541 100644 --- a/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto +++ b/pkg/kubelet/apis/deviceplugin/v1beta1/api.proto @@ -55,6 +55,11 @@ service DevicePlugin { // returns the new list rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} + // GetPreferredAllocation returns a preferred set of devices to allocate + // from a list of available ones. The resulting preferred allocation is not + // guaranteed to be the allocation ultimately performed by the + // `devicemanager`. It is only designed to help the `devicemanager` make a + // more informed allocation decision when possible. + rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {} + // Allocate is called during container creation so that the Device // Plugin can run device specific operations and instruct Kubelet // of the steps to make the Device available in the container @@ -99,6 +104,31 @@ message PreStartContainerRequest { message PreStartContainerResponse { } +// PreferredAllocationRequest is passed via a call to +// `GetPreferredAllocation()` at Pod admission time. The device plugin should +// take the list of `available_deviceIDs` and calculate a preferred allocation +// of size `size` from them, making sure to include the set of devices listed +// in `must_include_deviceIDs`. +message PreferredAllocationRequest { + repeated string available_deviceIDs = 1; + repeated string must_include_deviceIDs = 2; + int32 size = 3; +} + +// PreferredAllocationResponse returns a preferred allocation, +// resulting from a PreferredAllocationRequest. +message PreferredAllocationResponse { + ContainerAllocateRequest preferred_allocation = 1; +} + // - Allocate is expected to be called during Pod creation since allocation // failures for any container would result in Pod startup failure. // - Allocate allows kubelet to exposes additional artifacts in a Pod\u0026#39;s 使用这个新的 API 调用，设备管理器将在 Pod 准入时调用一个插件， 要求它从可用设备列表中获得一个给定大小的首选设备分配。Pod 中的每个容器都会调用一次。\n传给 GetPreferredAllocation() 方法的可用设备列表不一定与系统上可用的完整列表相匹配。 相反，在考虑所有 TopologyHint 之后，设备管理器调用 GetPreferredAllocation() 方法， 是最后一次筛选，方法执行结束必须要做出选择。因此，这个可用列表已经经过 TopologyHint 的预筛选。\n首选分配并不保证是最终由设备管理器执行的分配。它的设计只是为了帮助设备管理者在可能的情况下做出更明智的分配决策。\n在决定首选分配时，设备插件可能会考虑设备管理器不知道的内部拓扑约束。分配 NVIDIA 图形处理器对， 总是包括一个 NVLINK，就是一个很好的例子。\n在一台 8 GPU 的机器上，如果需要 2 个 GPU，NVLINK 提供的最佳连接对可能是：\n{{0,3}, {1,2}, {4,7}, {5,6}} 使用 GetPreferredAllocation () ，NVIDIA 设备插件可以将这些首选分配之一转发给设备管理器， 如果仍然有合适的设备集可用的话。如果没有这些额外的信息， 设备管理器最终会在 TopologyHint 过滤之后从可用的 gpu 列表中随机选择 gpu。 这个 API 允许它最终以最小的成本执行更好的分配。\n"},{"id":19,"href":"/docs/kubernetes/kube-scheduler/advanced-scheduling/","title":"高级调度","section":"kube-scheduler","content":" 1. 使用 taint 和 toleration 阻止节点调度到特定节点 # 1.1 taint 和 toleration # taint，是在不修改已有 Pod 的前提下，通过在节点上添加污点信息，拒绝 Pod 的部署。 只有当一个 Pod 容忍某个节点的 taint 时，才能被调度到此节点上。\n显示节点 taint 信息\nkubectl describe node master.k8s ... Name: master.k8s Role: Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=master.k8s Annotations: node.alpha.kubernetes.io/ttl=0 Taints: node-role.kubernetes.io/master:NoSchedule ... 主节点上包含一个污点，污点包含一个 key 和 value，以及一个 effect，格式为=:。 上面的污点信息表示，key 为 node-role.kubernetes.io/master，value 是空，effect 是 NoSchedule。\n显示 Pod tolerations\nkubectl describe Pod kube-proxy-as92 -n kube-system ... Tolerations: node-role.kubernetes.io/master:=NoSchedule node.alpha.kubernetes.io/notReady=:Exists:NoExecute node.alpha.kubernetes.io/unreachable=:Exists:NoExecute ... 第一个 toleration 匹配了主节点的 taint，表示允许这个 Pod 调度到主节点上。\n了解污点效果 另外 2 个在 kube-proxy Pod 上容忍定义了当前节点状态是没有 ready 或者是 unreachable 时， 该 Pod 允许运行在该节点上多长时间。污点可以包含以下三种效果：\nNoSchedule，表示如果 pod 没有容忍此污点，将无法调度 PreferNoSchedule，是上面的宽松版本，如果没有其他节点可调度，依旧可以调度到本节点 NoExecute，上 2 个在调度期间起作用，而此设定也会影响正在运行中的 Pod。 如果节点上的 Pod 没有容忍此污点，会被驱逐。 1.2 在节点上定义污点 # kubectl taint node node1.k8s node-type=production:NoSchedule 此命令给节点添加污点，key 为 node-type，value 为 production，effect 为 NoSchedule。 如果现在部署一个常规 Pod 多个副本，没有一个 Pod 会调度到此节点上。\n1.3 在 Pod 上添加容忍 # apiVersion: apps/v1 kind: Deployment metadata: name: prod spec: replicas: 5 template: spec: ... tolertations: - key: node-type operator: Equals value: production effect: NoSchedule 部署此 deployment，Pod 即可调度到 node1.k8s 节点上。\n1.4 了解污点和容忍的使用场景 # 调度时使用污点和容忍\n污点可以组织新 Pod 的调度，或者定义非有限调度节点，甚至是将已有 Pod 驱逐。 例如，一个集群分成多个部分，只允许开发团队将 Pod 部署到特定节点上；或者某些特殊 Pod 依赖硬件配置。\n配置节点失效后的 pod 重新调度最长等待时间\n容忍程度也可以配置，当某个 Pod 所在节点 NotReady 或者 Unreachable 是， k8s 可以等待该 Pod 重新调度之前的最长等待时间。\n... tolerations: - effect: NoExecute key: node.alpha.kubernetes.io/notReady operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.alpha.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 ... 上面 2 个容忍表示，该 Pod 容忍所在节点处于 notReady 和 unreachable 状态维持 300s。超时后，再重新调度。\n2. 使用节点亲和将 Pod 调度到指定节点 # 污点可以拒绝调度，亲和允许调度。早期的 k8s 版本，节点亲和，就是 Pod 中的 nodeSelector 字段。 与节点选择器类似，每个节点都可以配置亲和性规则，可以是硬性标注，也可以是偏好。\n检查节点默认标签\nkubectl describe node gke-kubia-default-adj12knzf-2dascjb Name: gke-kubia-default-adj12knzf-2dascjb Role: Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux failure-domain.beta.kubernetes.io/region=europe-west1 failure-domain.beta.kubernetes.io/zone=europe-west1-d kubernetes.io/hostname=gke-kubia-default-adj12knzf-2dascjb 这是一个 gke 的节点，最后三个标签涉及到亲和性，分别表示所在地理地域，可用性区域，和主机名。\n2.1 指定强制节点亲和性规则 # 下面展示了只能被部署到含有 gpu=true 标签的节点上的 Pod：\napiVersion: v1 kind: Pod metadata: name: kubia-gpu spec: affinity: nodeAffinity: requiredDuringSchdulingIgnoredDuringExecution: lableSelectorTerms: - matchExpressions: - key: gpu operator: In values: - \u0026#34;true\u0026#34; 较长的节点亲和性属性名的含义\nrequireDuringScheduling\u0026hellip; 表示该字段下定义的规则，为了让 Pod 调度到该节点上，必须满足的标签。 \u0026hellip;IgnoredDuringExecution 表明该字段下的规则，不会影响已经在节点上的 Pod。 了解节点选择器的条件\nlableSelectorTerms 和 matchExpressions 定义了节点的标签必须满足哪一种表达方式。\n2.2 调度 Pod 时优先考虑某些节点 # 节点亲和性和节点选择器相比，最大的好处就是， 当调度某一个 Pod，指定可以优先考某些节点，如果节点均无法满足，调度结果也是可以接受的。 这是由 preferdDuringSchdulingIgnoredDuringExecution 字段实现的。\n添加标签\nkubectl label node node1.k8s availability-zone=zone1 kubectl label node node1.k8s share-type=dedicated kubectl label node node2.k8s availability-zone=zone2 kubectl label node node2.k8s share-type=shared 指定优先级节点亲和性\napiVersion: apps/v1 kind: Deployment metadata: name: pref spec: template: ... spec: affinity: nodeAffinity: prefereddDuringSchdulingIgnoredDuringExecution: - weight: 80 preference: - matchExpressions: - key: availability-zone operator: In values: - zone1 - weight: 20 preference: - matchExpressions: - key: share-type operator: In values: - dedicated 上面描述一个节点亲和优先级，并不是强制要求。想要 Pod 调度到含有 availability-zone=zone1 和 share-type=dedicated 的节点上。第一个优先级相对重要，weight=80，第二个相对不重要，weight=20。\n了解优先级是如何工作的\n如果集群中包含很多节点，上面的 deployment，将会把节点分成四种。2 个标签都包含的，优秀级最高； 只包含 weight=80 的标签节点次之；然后只是包含 weight=20 的节点；剩下的节点排在最后。\n在一个包含 2 个节点的集群中部署\n如果在只有 node1 和 node2 的集群中部署一个 replica=5 的 Deployment，Pod 更多部署到 node1， 有极少数调度到 node2 上。这是因为 kube-schduler 除了节点亲和的优先级函数， 还有其他函数来决定 Pod 调度到哪里。其中之一就是 Selector SpreadPriority 函数， 此函数保证同一个 ReplicaSet 或者 Service 的 Pod，将分散到不同节点上。 避免单个节点失效而出现整个服务挂掉。\n3. 使用亲和/反亲和部署 Pod # 刚才描述了 Pod 和节点之间的亲和关系，也有 Pod 与 Pod 之间的亲和关系， 例如前端 Pod 和后端 Pod，尽可能部署较近，减少网络时延。\n3.1 使用 Pod 亲和将多个 Pod 部署到同一个节点上 # 部署 1 个后端 Pod 和 5 个前端 Pod。先部署后端：\nkubectl run backend -l app=backend --image busybox --sleep 9999 这里没什么区别，只是给 Pod 加了个标签，app=backend。\n在 Pod 中指定亲和性\napiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 template: ... spec: affinity: podAffinity: requireddDuringSchdulingIgnoredDuringExecution: - topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: backend 这里强制要求，frontend 的 Pod 被调度到和其他包含 app=backend 标签的 Pod 所在的相同节点上。 这是通过 topologyKey 指定的。\n了解调度器如何使用 pod 亲和规则\n上面的应用部署后，5 个前端 Pod 和 1 个后端 Pod 都在 node2 上。 假设后端 Pod 被误删，重新调度后，依旧在 node2 上。 虽然后端 Pod 本身没有定义任何亲和性规则，如果调度到其他节点，即会打破已有的亲和规则。\n3.2 将 Pod 部署到同一个机柜、可用性区域或者地理地域 # 同一个可用性区域协调部署\n如果想要 Pod 在同一个 available zone 部署应用， 可以将 topologyKey 设置成 failure-domain.beta.kubernetes.io/zone。\n同一个地域协调部署 如果想要 Pod 在同一个 available zone 部署应用， 可以将 topologyKey 设置成 failure-domain.beta.kubernetes.io/region。\n了解 topologyKey 如何工作 topologyKey 的工作方式很简单，上面提到的三个属性并没有什么特别。可以设置成任意你想要的值。\n当调度器决定决定 Pod 调度到哪里时，首先检查 podAffinity 配置，选择满足条件的 Pod， 接着查询这些 Pod 运行在那些节点上。特别寻找能匹配 podAffinity 配置的 topologyKey 的节点。 接着，会优先选择所有标签匹配的 Pod 的值的节点。\n3.3 Pod 优先级亲和性 # 与节点亲和一样，pod 之间也可以用 prefereddDuringSchdulingIgnoredDuringExecution。\napiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 template: ... spec: affinity: podAffinity: prefereddDuringSchdulingIgnoredDuringExecution: - weight: 80 podAffinityTerm: topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: backend 和 nodeAffinity 一样，设置了权重。也需要设置 topologyKey 和 labelSelector。\n3.4 Pod 反亲和分开调度 # 同理，有 nodeAntiAffinity，也有 podAntiAffinity。 这将导致调度器永远不会选择包含 podAntiAffinity 的 Pod 所在的节点。\n使用反亲和分散 Pod\napiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 template: metadata: labels: app: frontend spec: affinity: podAntiAffinity: requiredDuringSchdulingIgnoredDuringExecution: topologyKey: kubernetes.io/hostname labelSelector: matchLabels: app: frontend 上面的 Deployment 创建后，5 个实例应当分布在 5 个不同节点上。\n理解 Pod 反亲和优先级\n在这种情况下，可能使用软反亲和策略（prefereddDuringSchdulingIgnoredDuringExecution）。 毕竟 2 个前端 Pod 在同一个节点上也不是什么问题。如果运行在一个节点上出现问题， 那么使用 requiredDuringSchdulingIgnoredDuringExecution 就比较合适了。 与 Pod 亲和一样，topologyKey 决定了 Pod 不能被调度的范围。\n"},{"id":20,"href":"/docs/kubernetes/kubelet/kubelet-eviction-manager/","title":"Eviction Manager 工作机制","section":"kubelet","content":" 1. 概述 # 在可用计算资源较少时，kubelet 为保证节点稳定性，会主动地结束一个或多个 pod 以回收短缺地资源， 这在处理内存和磁盘这种不可压缩资源时，驱逐 pod 回收资源的策略，显得尤为重要。 下面来具体研究下 Kubelet Eviction Policy 的工作机制。\nkubelet 预先监控本节点的资源使用，防止资源被耗尽，保证节点稳定性。 kubelet 会预先 Fail N(\u0026gt;=1) 个 Pod，以回收出现紧缺的资源。 kubelet 在 Fail 一个 pod 时，kill 掉 pod 内所有 container，并设置 pod.status.phase = Failed。 kubelet 按照事先设定好的 Eviction Threshold 来触发驱逐动作，实现资源回收。 1.1 驱逐信号 # 在源码 pkg/kubelet/eviction/api/types.go 中定义了以下及几种 Eviction Signals：\nEviction Signal Description memory.available := node.status.capacity[memory] - node.stats.memory.workingSet nodefs.available := node.stats.fs.available nodefs.inodesFree := node.stats.fs.inodesFree imagefs.available := node.stats.runtime.imagefs.available imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree allocatableMemory.available := pod.allocatable - pod.workingSet pid.available := node.MaxPID - node.NumOfRunningProcesses 上表主要涉及三个方面，memory、file system 和 pid。其中 kubelet 值支持 2 种文件系统分区：\nnodefs：kubelet 用来存储 volume 和 daemon logs 等 imagesfs：容器运行时 ( docker 等）用来保存镜像和容器的 writable layer 1.2 驱逐阈值 # kubelet 的入参接收用户定义的 eviction signal 和 eviction threshold 的映射关系，格式如下：\n[eviction-signal] [opterator] [quantity]\n支持的 signal 如上表所示； operator 是关系运算符，例如\u0026lt;； quantity 是驱逐阈值，合法的值必须是 kubernetes 使用的数量表示，例如 1Gi 和 10% 等； 1.2.1 软驱逐策略 # Soft Eviction Thresholds，它与以下三个参数配合使用：\neviction-soft：(e.g. memory.available\u0026lt;1.5Gi) 触发软驱逐的阈值； eviction-soft-grace-period：(e.g. memory.available=1m30s) 当达到软驱逐的阈值，需要等待的时间； 在这段时间内，每 10s 会重新获取监控数据并更新 threshold 值， 如果在等待期间，最后一次的数据仍然超过阈值，才会触发驱逐 pod 的行为。 eviction-max-pod-grace-period：(e.g. 30s) 当满足软驱逐阈值并终止 pod 时允许的最大宽限期值。 如果待 Evict 的 Pod 指定了pod.Spec.TerminationGracePeriodSeconds， 则取min(eviction-max-pod-grace-period, pod.Spec.TerminationGracePeriodSeconds) 作为 Pod Termination 真正的 Grace Period。 因此，在软驱逐策略下，从 kubelet 检测到驱逐信号达到了阈值设定开始，到 pod 真正被 kill 掉， 共花费的时间是：sum(eviction-max-pod-grace-period, min(eviction-max-pod-grace-period, pod.Spec.TerminationGracePeriodSeconds))\n1.2.2 硬驱逐 # Hard Eviction Thresholds 比 Soft Eviction Thresholds 简单粗暴，没有宽限期， 即使 pod 配置了 pod.Spec.TerminationGracePeriodSeconds，一旦达到阈值配置， kubelet 立马回收关联的短缺资源，并且使用的就立即结束，而不是优雅终止。 此特性已经标记为 Deprecated。\n源码 pkg/kubelet/apis/config/v1beta1/defaults_linux.go 给出了默认的硬驱逐配置：\nmemory.available \u0026lt; 100Mi nodefs.available \u0026lt; 10% nodefs.inodesFree \u0026lt; 5% imagefs.available \u0026lt; 15% 1.3 驱逐周期 # 有了驱逐信号和阈值，也有了策略，接下来就是 Eviction Monitoring Interval。 kubelet 对应的监控周期，就通过 cAdvisor 的 housekeeping-interval 配置的，默认 10s。\n1.4 节点状态 # kubelet 监测到配置的驱逐策略被触发，会将驱逐信号映射到对应的节点状态。 Kubelet 会将对应的 Eviction Signals 映射到对应的 Node Conditions， 源码 [pkg/kubelet/eviction/helpers.go]，其映射关系如下：\n节点状态 驱逐信号 描述 MemoryPressure memory.avaliable, allocatableMemory.available 节点或 pod 的可用内存触发驱逐阈值 DiskPressure nodefs.avaliable, nodefs.inodesFree, imagefs.available, imagesfs.inodesFree 节点的 root fs 或 i mage fs 上的可用磁盘空间和索引节点已满足收回阈值 PIDPressure pid.available 节点的可用 PID 触发驱逐阈值 kubelet 映射了 Node Condition 之 后，会继续按照--node-status-update-frequency(default 10s) 配置的时间间隔， 周期性的与 kube-apiserver 进行 node status updates。\n1.5 节点状态振荡 # ​考虑这样一种场景，节点上监控到 soft eviction signal 的值，始终在 eviction threshold 上下波动， 那么 kubelet 就会将该 node 对应的 node condition 在 true 和 false 之间来回切换。 给 kube-scheduler 产生错误的调度结果。\n​因此，kubelet 添加参数 eviction-pressure-transition-period (default 5m0s) 配置， 使 Kubelet 在解除由 Evicion Signal 映射的 Node Pressure 之前，必须等待 5 分钟。\n​驱逐逻辑添加了一步：\nSoft Evction Singal 高于 Soft Eviction Thresholds 时， Kubelet 还是会立刻设置对应的 MemoryPressure 或 DiskPressure 为 True。 当 MemoryPressure 或 DiskPressure 为 True 的前提下， 发生了 Soft Evction Singal 低于 Soft Eviction Thresholds 的情况， 则需要等待 eviction-pressure-transition-period(default 5m0s) 配置的这么长时间， 才会将 condition pressure 切换回 False。 一句话总结：Node Condition Pressure 成为 True 容易，切换回 False 则要等eviction-pressure-transition-period。\n1.6 回收节点层级资源 # 如果满足驱逐阈值并超过了宽限期，kubelet 将启动回收压力资源的过程， 直到它发现低于设定阈值的信号为止。 kubelet 将尝试在驱逐终端用户 pod 前回收节点层级资源。 发现磁盘压力时，如果节点针对容器运行时配置有独占的 imagefs， kubelet 回收节点层级资源的方式将会不同。\n1.6.1 使用 imagefs # 如果 nodefs 文件系统满足驱逐阈值，kubelet 通过驱逐 pod 及其容器来释放磁盘空间。 如果 imagefs 文件系统满足驱逐阈值，kubelet 通过删除所有未使用的镜像来释放磁盘空间。 1.6.2 未使用 imagefs # 删除停止运行的 pod/container 删除全部没有被使用的镜像 1.7 驱逐策略 # ​kubelet 根据 Pod 的 QoS Class 实现了一套默认的 Evication 策略， kubelet 首先根据他们对短缺资源的使用是否超过请求来排除 pod 的驱逐行为， 然后通过优先级，然后通过相对于 pod 的调度请求消耗急需的计算资源。图解如下：\n​对于每一种 Resource 都可以将容器分为 3 种 QoS Classes: Guaranteed, Burstable 和 Best-Effort， 它们的 QoS 级别依次递减。\nBestEffort，按照短缺资源占用量排序，占用量越高，被 kill 的优先级越高； Burstable，对使用量高于请求量的 pod 排序，占用越多，回收优先级越高； 如果没有 pod 的使用超过请求，按照 BestEffort 策略回收； Guaranteed，Guaranteed pod 只有为所有的容器指定了要求和限制并且它们相等时才能得到保证。 由于另一个 pod 的资源消耗，这些 pod 保证永远不会被驱逐。 如果系统守护进程（例如 kubelet、docker、和 journald） 消耗的资源多于通过 system-reserved 或 kube-reserved 分配保留的资源， 并且该节点只有 Guaranteed 或 Burstable pod 使用少于剩余的请求， 然后节点必须选择驱逐这样的 pod 以保持节点的稳定性并限制意外消耗对其他 pod 的影响。 在这种情况下，它将首先驱逐优先级最低的 pod。 1.8 最小驱逐回收 # 有些情况下，可能只回收一小部分的资源就能使得 Evication Signal 的值低于 eviction thresholds。 但是，可能随着资源使用的波动或者新的调度 Pod 使得在该 Node 上很快又会触发 evict pods 的动作， eviction 毕竟是耗时的动作，所以应该尽量避免这种情况的发生。\n为了减少这类问题，每次 Evict Pods 后，Node 上对应的 Resource 不仅要比 Eviction Thresholds 低， 还要保证最少比 Eviction Thresholds，再低 --eviction-minimum-reclaim 中配置的数量。\n例如使用下面的配置：\n--eviction-hard=memory.available\u0026lt;500Mi,nodefs.available\u0026lt;1Gi,imagefs.available\u0026lt;100Gi --eviction-minimum-reclaim=\u0026#34;memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi\u0026#34; 如果 memory.available 驱逐阈值被触发，kubelet 将保证 memory.available 至少为 500Mi。 对于 nodefs.available，kubelet 将保证 nodefs.available 至少为 1.5Gi。 对于 imagefs.available，kubelet 将保证 imagefs.available 至少为 102Gi， 直到不再有相关资源报告压力为止。\n所有资源的默认 eviction-minimum-reclaim 值为 0。\n"},{"id":21,"href":"/docs/kubernetes/sig-network/learn-about-Service/","title":"深入了解 Service","section":"sig-network","content":" 1. 基本概念 # 1.1 Service 定义详解 # Service 是对一组提供相同功能的 Pods 的抽象，并为它们提供一个统一的入口。借助 Service， 应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service 通过标签来选取服务后端， 一般配合 Replication Controller 或者 Deployment 来保证后端容器的正常运行。 这些匹配标签的 Pod IP 和端口列表组成 endpoints， 由 kube-proxy 负责将服务 IP 负载均衡到这些 endpoints 上。\napiVersion: v1 kind: Service metadata: name: string namespace: string labels: - name: string annotations: - name: string spec: selector: [] # ClusterIP、NodePort、LoadBalancer type: string # type=ClusterIP, 有自动分配的能力；type=LoadBalancer，需指定 clusterIP: string # 是否支持 session，默认为空，可选值 ClutserIP，同一个 client 的 request，都发送到同一个后端 Pod sessionAffinity: string ports: - name: string # tcp、udp，默认 tcp protocol: string port: int targetPort: int nodePort: int # spec.type=LoadBalancer, 设置外部负载均衡器地址，用于公有云环境 status: loadBalancer: ingress: ip: string hostname: string 1.2 Service 分类 # ClusterIP：默认类型，自动分配一个仅 cluster 内部可以访问的虚拟 IP NodePort：在 ClusterIP 基础上为 Service 在每台机器上绑定一个端口， 这样就可以通过 http://\u0026lt;NodeIP\u0026gt;:NodePort 来访问该服务。 如果 kube-proxy 设置了 --nodeport-addresses=10.240.0.0/16（v1.10 支持）， 那么仅该 NodePort 仅对设置在范围内的 IP 有效。 LoadBalancer：在 NodePort 的基础上，借助 cloud provider 创建一个外部的负载均衡器， 并将请求转发到 \u0026lt;NodeIP\u0026gt;:NodePort ExternalName：将服务通过 DNS CNAME 记录方式转发到指定的域名（通过 spec.externlName 设定）。 需要 kube-dns 版本在 1.7 以上。 2. Service 基本用法 # 一般来说，对外提供服务的应用程序需要通过某种机制来实现， 对于容器应用最简便的方式就是通过 TCP/IP 机制及监听 IP 和端口号来实现。\n直接通过 Pod 的 IP 地址和端口号可以访问到容器应用内的服务，但是 Pod 的 IP 地址是不可靠的， 例如当 Pod 所在的 Node 发生故障时，Pod 将被 Kubernetes 重新调度到另一个 Node， Pod 的 IP 地址将发生变化。 更重要的是，如果容器应用本身是分布式的部署方式，通过多个实例共同提供服务， 就需要在这些实例的前端设置一个负载均衡器来实现请求的分发。 Kubernetes 中的 Service 就是用于解决这些问题的核心组件。\nService 定义中的关键字段是 ports 和 selector。通过 selector 与 Pod 关联， port 描述 service 本身的端口，targetPort 表示流量转发的端口，也就是 Pod 的端口， 从而完成访问 service 负载均衡到后端任意一个 Pod。Kubernetes 提供了两种负载分发策略： RoundRobin 和 SessionAffinity，具体说明如下：\nRoundRobin：轮询模式，即轮询将请求转发到后端的各个 Pod 上。 SessionAffinity：基于客户端 IP 地址进行会话保持的模式， 即第 1 次将某个客户端发起的请求转发到后端的某个 Pod 上， 之后从相同的客户端发起的请求都将被转发到后端相同的 Pod 上。 在默认情况下，Kubernetes 采用 RoundRobin 模式对客户端请求进行负载分发， 但我们也可以通过设置 service.spec.sessionAffinity=ClientIP 来启用 SessionAffinity 策略。 这样，同一个客户端 IP 发来的请求就会被转发到后端固定的某个 Pod 上了。\n2.1 集群内访问集群外服务 # 到现在为止，我们己经讨论了后端是集群中运行的一个或多个 Pod 的服务。 但也存在希望通过 Kubernetes 服务特性暴露外部服务的情况。 不要让服务将连接重定向到集群中的 Pod，而是让它重定向到外部 IP 和端口。 这样做可以让你充分利用服务负载平衡和服务发现。 在集群中运行的客户端 Pod 可以像连接到内部服务一样连接到外部服务。\n首先要知道，service 和 Pod 并不是直接相连的，此二者之间还有一个对象叫 endpoint。 service 根据 selector 找到后端 Pod，用 Pod IP 和端口创建与 service 同名的 endpoint， 记录 Pod IP。当 Pod 异常被删除重建后，获得的新地址，只需要更新 endpoint 中记录的 Pod I P 即可。 因此，想要访问集群外部的服务，可手动配置 service 的 endpoint。\n2.1.1 创建没有 selector 的 Service # 创建没有 selector 的 Service apiVersion: v1 kind: Service metadata: name: external-service spec: ports: - port: 80 为没有选择器的服务创建 Endpoint 资源 apiVersion: v1 kind: Endpoint metadata: # endpoint 的名称必须和服务的名称相匹配 name: external-service subsets: - addresses: # 将 service 重定向到 endpoint 的地址 - ip: 11.11.11.11 - ip: 22.22.22.22 ports: # endpoint 目标端口 - port: 80 Endpoint 对象需要与服务具有相同的名称，并包含该服务的目标 IP 地址和端口列表。 服务和 Endpoint 资源都发布到服务器后，这样服务就可以像具有 Pod 选择器那样的服务正常使用。 在服务创建后创建的容器将包含服务的环境变量，并且与其 IP:Port 对的所有连接都将在服务端点之间进行负载均衡。\n2.1.2 创建 ExternalName 的 service # 除了手动配置服务的 Endpoint 来代替公开外部服务方法，有一种更简单的方法， 就是通过其完全限定域名 (FQDN) 访问外部服务。\n要创建一个具有别名的外部服务的服务时，要将创建服务资源的一个 type 字段设置为 ExternalName。 例如，设想一下在 api.somecompany.com 上有公共可用的 API 可以定义一个指向它的服务， 如下面的代码清单所示：\napiVersion: v1 kind: Service metadata: name: external-service spec: type: ExternalName externalName: someapi.somecompany.com ports: - port: 80 服务创建完成后，Pod 可以通过 external-service.default.svc.cluster.local 域名 （甚至是 external-service) 连接到外部服务，而不是使用服务的实际 FQDN。 这隐藏了实际的服务名称及其使用该服务的 Pod 的位置，允许修改服务定义， 并且在以后如果将其指向不同的服务，只需简单地修改 externalName 属性， 或者将类型重新变回 Cluster IP 并为服务创建 Endpoint ——无论是手动创建， 还是对服务上指定标签选择器使其自动创建。\nExternalName 服务仅在 DNS 级别实施——为服务创建了简单的 CNAME DNS 记录。 因此，连接到服务的客户端将直接连接到外部服务，完全绕过服务代理。 出于这个原因，这些类型的服务甚至不会获得集群 IP。\n2.2 集群外访问集群内服务 # 2.2.1 NodePort 服务 # 将服务的类型设置成 NodePort：每个集群节点都会在节点上打开一个端口，对于 NodePort 服务， 每个集群节点在节点本身（因此得名叫 NodePort) 上打开一个端口， 并将在该端口上接收到的流量重定向到基础服务。该服务仅在内部集群 IP 和端口上才可访间， 但也可通过所有节点上的专用端口访问。\napiVersion: v1 kind: Service metadata: name: kubia-nodeport spec: type: NodePort ports: - port: 80 targetPort: 8080 nodePort: 30123 selector: app: kubia 2.2.2 LoadBalancer 服务 # 在云提供商上运行的 Kubernetes 集群通常支持从云基础架构自动提供负载平衡器。 所有需要做的就是设置服务的类型为 LoadBadancer 而不是 NodePort。 负载均衡器拥有自己独一无二的可公开访问的 IP 地址，并将所有连接重定向到服务。 可以通过负载均衡器的 IP 地址访问服务。\n如果 Kubemetes 在不支持 LoadBadancer 服务的环境中运行，则不会调配负载平衡器， 但该服务仍将表现得像一个 NodePort 服务。这是因为 LoadBadancer 服务是 NodePort 服务的扩展。\napiVersion: v1 kind: Service metadata: name: kubia-loadbalancer spec: type: LoadBalancer ports: - port: 80 targetPort: 8080 selector: app: kubia clusterIP: 10.0.171.239 loadBalancerIP: 78.11.24.19 status: loadBalancer: ingress: - ip: 146.148.47.155 3. 通过 Ingress 暴露服务 # 为什么需要 Ingress？ 一个重要的原因是每个 LoadBalancer 服务都需要自己的负载均衡器，以及独有的公有 IP 地址， 而 Ingress 只需要一个公网 IP 就能为许多服务提供访问。当客户端向 Ingress 发送 HTTP 请求时， Ingress 会根据请求的主机名和路径决定请求转发到的服务。\n3.1 创建 Ingress Controller 和默认的 backend 服务 # 在定义 Ingress 策略之前，需要先部署 Ingress Controller， 以实现为所有后端 Service 都提供一个统一的入口。 Ingress Controller 需要实现基于不同 HTTP URL 向后转发的负载分发规则， 并可以灵活设置 7 层负载分发策略。如果公有云服务商能够提供该类型的 HTTP 路由 LoadBalancer， 则也可设置其为 Ingress Controller。\n在 Kubernetes 中，Ingress Controller 将以 Pod 的形式运行， 监控 API Server 的 ingress 接口后端的 backend services， 如果 Service 发生变化，则 Ingress Controller 应自动更新其转发规则。\n下面的例子使用 Nginx 来实现一个 Ingress Controller，需要实现的基本逻辑如下：\n监听 API Server，获取全部 Ingress 的定义。 基于 Ingress 的定义，生成 Nginx 所需的配置文件 /etc/nginx/nginx.conf。 执行 nginx -s reload 命令，重新加载 nginx.conf 配置文件的内容。 为了让 Ingress Controller 正常启动，还需要为它配置一个默认的 backend， 用于在客户端访问的 URL 地址不存在时，返回一个正确的 404 应答。 这个 backend 服务用任何应用实现都可以，只要满足对根路径“/”的访问返回 404 应答， 并且提供 /healthz 路径以使 kubelet 完成对它的健康检查。 另外，由于 Nginx 通过 default-backend-service 的服务名称（Service Name）去访问它， 所以需要 DNS 服务正确运行。\n3.2 创建 Ingress 资源 # 3.2.1 转发到单个后端服务上 # 基于这种设置，客户端到 Ingress Controller 的访问请求都将被转发到后端的唯一 Service 上， 在这种情况下 Ingress 无须定义任何 rule。\n通过如下所示的设置，对 Ingress Controller 的访问请求都将被转发到“myweb:8080”这个服务上。\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec: backend: serviceName: webapp servicePort: 8080 3.2.2 将不同的服务映射到相同主机的不同路径 # 这种配置常用于一个网站通过不同的路径提供不同的服务的场景， 例如 /web 表示访问 Web 页面，/api 表示访问 API 接口，对应到后端的两个服务， 通过 Ingress 的设置很容易就能将基于 URL 路径的转发规则定义出来。\n通过如下所示的设置，对 mywebsite.com/web 的访问请求将被转发到 web-service:80 服务上； 对 mywebsite.com/api 的访问请求将被转发到 api-service:80 服务上：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec rules: - host mywebsite.com http: paths: - path: /web backend: serviceName: web-service servicePort: 80 - path: /api backend: serviceName: api-service servicePort: 80 3.2.3 不同的域名（虚拟主机名）被转发到不同的服务上 # 这种配置常用于一个网站通过不同的域名或虚拟主机名提供不同服务的场景， 例如 foo.example.com 域名由 foo 提供服务，bar.example.com 域名由 bar 提供服务。\n通过如下所示的设置，对“foo.example.com”的访问请求将被转发到“foo:80”服务上， 对“bar.example.com”的访问请求将被转发到“bar:80”服务上：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec: rules: - host: foo.example.com http: paths: - path: I backend: serviceName: foo servicePort: 80 - host: bar.example.com http: paths: - path: I backend: serviceName: bar servicePort: 80 3.2.4 不使用域名的转发规则 # 这种配置用于一个网站不使用域名直接提供服务的场景， 此时通过任意一台运行 ingress-controller 的 Node 都能访问到后端的服务。\n下面的配置为将“/demo”的访问请求转发到“webapp:8080/demo”服务上：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec: rules: - http: paths: - path: /demo backend: serviceName: webapp servicePort: 8080 注意，使用无域名的 Ingress 转发规则时，将默认禁用非安全 HTTP，强制启用 HTTPS。 可以在 Ingress 的定义中设置一个 annotation[ingress.kubernetes.io/ssl-redirect: \u0026ldquo;false\u0026rdquo;] 来关闭强制启用 HTTPS 的设置：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress annotations: ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; spec: rules: - http: paths: - path: /demo backend: serviceName: webapp servicePort: 8080 3.3 Ingress 的 TLS 安全设置 # 当客户端创建到 Ingress 控制器的 TLS 连接时，控制器将终止 TLS 连接。 客户端和控制器之间的通信是加密的，而控制器和后端 Pod 之间的通信则不是运行在 Pod 上的应用程序不需要支持 TLS。 例如，如果 Pod 运行 web 服务器，则它只能接收 HTTP 通信，并让 Ingress 控制器负责处理与 TLS 相关的所有内容。 要使控制器能够这样做，需要将证书和私钥附加到 Ingress。 这两个必需资源存储在称为 Secret 的 Kubernetes 资源中，然后在 Ingress manifest 中引用它。\n为了 Ingress 提供 HTTPS 的安全访问，可以为 Ingress 中的域名进行 TLS 安全证书的设置。设置的步骤如下。\n创建自签名的密钥和 SSL 证书文件 openssl genrsa -out tls.key 2048 openssl req -new - x509 -key tls.key -out tls.cert -days 360 -subject/CN=kubia.example.com 将证书保存到 Kubernetes 中的一个 Secret 资源对象上 kubectl create secret tls mywebsite-tls-secret --cert=tls.cert --key=tls.key 将该 Secret 对象设置到 Ingress 中 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: mywebsite-ingress-tls spec: tls: - hosts: - mywebsite.com secretName: mywebsite-tls-secret rules: - http: paths: - path: /demo backend: serviceName: webapp servicePort: 8080 根据提供服务的网站域名是一个还是多个，可以使用不同的操作完成前两步 SSL 证书和 Secret 对象的创建， 在只有一个域名的情况下设置相对简单。第 3 步对于这两种场景来说是相同的。\n"},{"id":22,"href":"/docs/kubernetes/kube-apiserver/crd/","title":"CRD 入门和使用","section":"kube-apisever","content":" 1. Customer Resource # 自定义资源是 Kubernetes API 的扩展，本文将讨什么时候应该向 Kubernetes 集群添加自定义资源以及何时使用独立服务。它描述了添加自定义资源的两种方法以及如何在它们之间进行选择。\n1.1 自定义资源概述 # 资源就是 Kubernetes API 集合中的某个的对象。例如，内置 pods 资源包含 Pod 对象的集合。\n自定义资源是扩展了 Kubernetes API，但在默认的 API 集合中是不可用的，因为没有对应的 controller 处理业务逻辑。使用自定义资源，不仅可以解耦大多数的自研功能，还可用使得 Kubernetes 更加模块化。\n自定义资源可以通过动态注册的方法，于正在 running 的集群中创建、删除和更新，并且与集群本身的资源是相互独立的。当自定义资源被创建，就可以通过 kubectl 创建和访问对象，和对内置资源的操作完全一致。\n1.2 是否需要自定义资源 # 在创建新的 API 时，应该考虑与 Kubernetes 的 API 聚合，还是让 API 独立运行。\nAPI 聚合 API 独立 声明式 非声明式 kubectl 可读可写 不需要 kubectl 支持 接受 k8s 的 REST 限制 特定 API 路径 可限定为 cluster 或者 namespace 不适用 namespace 重用 k8s API 支持的功能 不需要这些功能 1.3 声明式 API VS 命令式 API # 在声明式 API，通过具有以下特性：\n对象颗粒度小 基础结构的定义 读写多，更新少，操作主要为 CRUD API 表期望状态 命令式 API，通过具有以下特性：\n同步响应 RPC 调用 大量数据存储（大对象或多对象） 高带宽访问 操作非 CRUD API 不易抽象 2. Customer Resource Definition # Kubernetes 提供了两种向集群添加自定义资源的方法：\n创建自定义 API server 并聚合到 API 中 CRDs 2.1 Customer Resource Definition 介绍 # kubernetes: v1.19.0\n通过 CRD 创建自定义资源，只要符合 CRD 结构体定义，均可以被 Kubernetes 所接收，CRD 的定义和 k8s 原生资源的定义非常类似，都是由 4 个子资源组成：TypeMeta、ObjectMeta、Spec、Status，具体代码如下：\nstaging/src/k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/types.go:354\ntype CustomResourceDefinition struct { metav1.TypeMeta metav1.ObjectMeta Spec CustomResourceDefinitionSpec Status CustomResourceDefinitionStatus } 下面主要来看下 Spec 和 Status 字段：\ntype CustomResourceDefinitionSpec struct { Group string // 定义复数、单数，简称，Kind，ListKind，Categories Names CustomResourceDefinitionNames // 表示集群级别或 namespace 级别 Scope ResourceScope // 定义 CR 所有支持的版本号 Versions []CustomResourceDefinitionVersion // 定义不同版本之间的转换方法 Conversion *CustomResourceConversion // 是否保留未知字段，apiVersion、kind、metadata 等总是保留 PreserveUnknownFields *bool } type CustomResourceDefinitionStatus struct { // 描述资源的当前状态，即 status.conditions Conditions []CustomResourceDefinitionCondition // 与 spec.names 类似，即 status.acceptNames AcceptedNames CustomResourceDefinitionNames // 资源曾经存在的版本，可在 etcd 中存储，供迁移用 StoredVersions []string } 2.2 CRD 定义示例 # apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # 名称必须符合下面的格式：\u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt; name: foos.samplecontroller.io spec: # REST API 使用的组名称：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; group: samplecontroller.io # REST API 使用的版本号：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt; versions: - name: v1alpha1 served: true storage: true schema: # 校验方法 openAPIV3Schema: type: object properties: spec: type: object properties: #类型校验 replicas: type: integer deploymentName: type: string # Namespaced 或 Cluster scope: Namespaced names: # URL 中使用的复数名称：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt; plural: foos # CLI 中使用的单数名称 singular: foo # CamelCased 格式的单数类型。在清单文件中使用 kind: Foo # 资源的 List 类型，默认是 “`kind`List” listKind: FooList # CLI 中使用的资源简称 shortNames: - fo # 分组 categories: - all 2.3 Customer Controller # 只定义资源，不实现资源控制器是没有任何意义的。自定义控制器能够完成业务逻辑，最主要是依赖 client-go 库的各个组件的交互。下图展示它们之间的关系：\n通过图示，可以看到几个核心组件的交互流程，蓝色表示 client-go，黄色是自定义 controller，各组件作用介绍如下：\n2.3.1 client-go 组件 # Reflector：reflector 用来 watch 特定的 k8s API 资源。具体的实现是通过 ListAndWatch 的方法，watch 可以是 k8s 内建的资源或者是自定义的资源。当 reflector 通过 watch API 接收到有关新资源实例存在的通知时，它使用相应的列表 API 获取新创建的对象，并将其放入 watchHandler 函数内的 Delta FIFO 队列中。 Informer：informer 从 Delta FIFO 队列中弹出对象。执行此操作的功能是 processLoop。base controller 的作用是保存对象以供以后检索，并调用我们的控制器将对象传递给它。 Indexer：索引器提供对象的索引功能。典型的索引用例是基于对象标签创建索引。 Indexer 可以根据多个索引函数维护索引。Indexer 使用线程安全的数据存储来存储对象及其键。 在 Store 中定义了一个名为 MetaNamespaceKeyFunc 的默认函数，该函数生成对象的键作为该对象的 namespace/name 组合。 2.3.2 自定义 controller 组件 # Informer reference：指的是 Informer 实例的引用，定义如何使用自定义资源对象。自定义控制器代码需要创建对应的 Informer。 Indexer reference：自定义控制器对 Indexer 实例的引用。自定义控制器需要创建对应的 Indexer。 Resource Event Handlers：资源事件回调函数，当它想要将对象传递给控制器时，它将被调用。编写这些函数的典型模式是获取调度对象的 key，并将该 key 排入工作队列以进行进一步处理。 Work queue：任务队列。编写资源事件处理程序函数以提取传递的对象的 key 并将其添加到任务队列。 Process Item：处理任务队列中对象的函数，这些函数通常使用 Indexer 引用或 Listing 包装器来重试与该 key 对应的对象。 简单的说，整个处理流程大概为：Reflector 通过检测 Kubernetes API 来跟踪该扩展资源类型的变化，一旦发现有变化，就将该 Object 存储队列中，Informer 循环取出该 Object 并将其存入 Indexer 进行检索，同时触发 Callback 回调函数，并将变更的 Object Key 信息放入到工作队列中，此时自定义 Controller 里面的 Process Item 就会获取工作队列里面的 Key，并从 Indexer 中获取 Key 对应的 Object，从而进行相关的业务处理。\n3. sample-controller 使用 # 3.1 资源准备 # 3.1.1 foo.crd.yaml # 使用 2.2 节的示例。\n3.1.2 example.foo.yaml # 创建 foo 资源类型的对象：\napiVersion: samplecontroller.io/v1alpha1 kind: Foo metadata: name: example-foo spec: deploymentName: example-foo replicas: 1 3.2 sample-controller 部署 # 3.2.1 代码 # 获取示例代码：\ngit clone https://github.com/kubernetes/sample-controller.git cd sample-controller 3.2.2 编译 # 在编译二进制之前，要修改 pkg/apis/samplecontroller/register.go:21 GroupName 的定义，k8s.io 和 kubernetes.io 后缀都是官方保留字段，不建议 CRD 使用。所以将其修改为 samplecontroller.io。\ngo build -o sample-controller . 3.2.3 启动 # 使用节点上的 kubeconfig 文件启动 controller：\n./sample-controller -kubeconfig=$HOME/.kube/config 启动时，会提示 watch 不到资源，因为还没创建定义：\nI1026 23:40:56.529233 57978 controller.go:115] Setting up event handlers I1026 23:40:56.529444 57978 controller.go:156] Starting Foo controller I1026 23:40:56.529450 57978 controller.go:159] Waiting for informer caches to sync E1026 23:40:56.554564 57978 reflector.go:138] k8s.io/sample-controller/pkg/generated/informers/externalversions/factory.go:117: Failed to watch *v1alpha1.Foo: failed to list *v1alpha1.Foo: the server could not find the requested resource (get foos.samplecontroller.io) E1026 23:40:57.841194 57978 reflector.go:138] k8s.io/sample-controller/pkg/generated/informers/externalversions/factory.go:117: Failed to watch *v1alpha1.Foo: failed to list *v1alpha1.Foo: the server could not find the requested resource (get foos.samplecontroller.io) 3.2.4 创建资源 # $ kubectl create -f foos.crd.yaml customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.io created $ kubectl create -f example.foos.yaml foo.samplecontroller.io/example-foo created 创建完成，可以使用 kubectl get crd 查看资源定义和对象：\n$ kubectl get crd NAME CREATED AT clusterversions.cfe.io 2020-10-26T14:43:27Z foos.samplecontroller.io 2020-10-26T15:41:52Z $ kubectl get foos NAME AGE example-foo 19m sample-controller 会 watch 到新对象 default/example-foo：\nI1026 23:42:03.232577 57978 controller.go:164] Starting workers I1026 23:42:03.232607 57978 controller.go:170] Started workers I1026 23:42:03.515970 57978 controller.go:228] Successfully synced \u0026#39;default/example-foo\u0026#39; I1026 23:42:03.516099 57978 event.go:291] \u0026#34;Event occurred\u0026#34; object=\u0026#34;default/example-foo\u0026#34; kind=\u0026#34;Foo\u0026#34; apiVersion=\u0026#34;samplecontroller.io/v1alpha1\u0026#34; type=\u0026#34;Normal\u0026#34; reason=\u0026#34;Synced\u0026#34; message=\u0026#34;Foo synced successfully\u0026#34; 3.2.5 校验 # 检查 controller 创建 Deployment 对象：\n$ kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE example-foo 1/1 1 1 3m1s $ kubectl get pod NAME READY STATUS RESTARTS AGE example-foo-54dc4db9fc-4r8pp 1/1 Running 0 3m8s shell-demo 1/1 Running 9 43d web-0 1/1 Running 16 78d web-1 1/1 Running 16 78d 4. 参考资料 # 定制资源 controller-client-go sample-controller "},{"id":23,"href":"/docs/golang/data-structure/map/","title":"map","section":"数据结构","content":" 1. 引言 # 粗略的讲，Go 语言中 map 采用的是哈希查找表， 由一个 key 通过哈希函数得到哈希值， 64 位系统中就生成一个 64 bit 的哈希值， 由这个哈希值将 key 对应到不同的桶（bucket）中， 当有多个哈希映射到相同的的桶中时，使用链表解决哈希冲突。\n1.1 hash 函数 # 首先要知道的就是 map 中哈希函数的作用，go 中 map 使用 hash 作查找， 就是将 key 作哈希运算，得到一个哈希值，根据哈希值确定 key-value 落在哪个 bucket 的哪个 cell。 golang 使用的 hash 算法和 CPU 有关，如果 CPU 支持 aes，那么使用 aes hash，否则使用 memhash。\n1.2 数据结构 # hmap 可以理解为 header of map 的缩写，即 map 数据结构的入口。\ntype hmap struct { // map 中的元素个数，必须放在 struct 的第一个位置，因为 内置的 len 函数会从这里读取 count int // map 状态标识，比如是否在被写或者迁移等，因为 map 不是线程安全的所以操作时需要判断 flags flags uint8 // log_2 of buckets （最多可以放 loadFactor * 2^B 个元素即 6.5*2^B，再多就要 hashGrow 了） B uint8 // overflow 的 bucket 的近似数 noverflow uint16 // hash seed，随机哈希种子可以防止哈希碰撞攻击 hash0 uint32 // 存储数据的 buckets 数组的指针， 大小 2^B，如果 count == 0 的话，可能是 nil buckets unsafe.Pointer // 一半大小的之前的 bucket 数组，只有在 growing 过程中是非 nil oldbuckets unsafe.Pointer // 扩容进度标志，小于此地址的 buckets 已迁移完成。 nevacuate uintptr // 可以减少 GC 扫描，当 key 和 value 都可以 inline 的时候，就会用这个字段 extra *mapextra // optional fields } 用 mapextra 来存储 key 和 value 都不是指针类型的 map，并且大小都小于 128 字节，这样可以避免 GC 扫描整个 map。\ntype mapextra struct { // 如果 key 和 value 都不包含指针，并且可以被 inline(\u0026lt;=128 字节） // 使用 extra 来存储 overflow bucket，这样可以避免 GC 扫描整个 map // 然而 bmap.overflow 也是个指针。这时候我们只能把这些 overflow 的指针 // 都放在 hmap.extra.overflow 和 hmap.extra.oldoverflow 中了 // overflow 包含的是 hmap.buckets 的 overflow 的 bucket // oldoverflow 包含扩容时的 hmap.oldbuckets 的 overflow 的 bucket overflow *[]*bmap oldoverflow *[]*bmap // 指向空闲的 overflow bucket 的指针 nextOverflow *bmap } bmap 可以理解为 buckets of map 的缩写，它就是 map 中 bucket 的本体，即存 key 和 value 数据的“桶”。\ntype bmap struct { // tophash 是 hash 值的高 8 位 tophash [bucketCnt]uint8 // 以下字段没有显示定义在 bmap，但是编译时编译器会自动添加 // keys // 每个桶最多可以装 8 个 key // values // 8 个 key 分别有 8 个 value 一一对应 // overflow pointer // 发生哈希碰撞之后创建的 overflow bucket } 根据哈希函数将 key 生成一个哈希值，其中低位哈希用来判断桶位置，高位哈希用来确定在桶中哪个 cell。 低位哈希就是哈希值的低 B 位，hmap 结构体中的 B，比如 B 为 5，2^5=32， 即该 map 有 32 个桶，只需要取哈希值的低 5 位就可以确定当前 key-value 落在哪个桶 (bucket) 中； 高位哈希即 tophash，是指哈希值的高 8 bits，根据 tophash 来确定 key 在桶中的位置。 每个桶可以存储 8 对 key-value，存储结构不是 key/value/key/value\u0026hellip;，而是 key/key..value/value， 这样可以避免字节对齐时的 padding，节省内存空间。\n当不同的 key 根据哈希得到的 tophash 和低位 hash 都一样，发生哈希碰撞，这个时候就体现 overflow pointer 字段的作用了。 桶溢出时，就需要把 key-value 对存储在 overflow bucket（溢出桶），overflow pointer 就是指向 overflow bucket 的指针。 如果 overflow bucket 也溢出了呢？那就再给 overflow bucket 新建一个 overflow bucket，用指针串起来就形成了链式结构， map 本身有 2^B 个 bucket，只有当发生哈希碰撞后才会在 bucket 后链式增加 overflow bucket。\n2. map 内存布局 # 2.1 扩容 # 装填因子是否大于 6.5\n装填因子 = 元素个数/桶个数，大于 6.5 时，说明桶快要装满，需要扩容\noverflow bucket 是否太多\n​当 bucket 的数量 \u0026lt; 2^15，但 overflow bucket 的数量大于桶数量 ​当 bucket 的数量 \u0026gt;= 2^15，但 overflow bucket 的数量大于 2^15\n双倍扩容：装载因子多大，直接翻倍，B+1；扩容也不是申请一块内存，立马开始拷贝，每一次访问旧的 buckets 时，就迁移一部分，直到完成，旧 bucket 被 GC 回收。\n等量扩容：重新排列，极端情况下，重新排列也解决不了，map 成了链表，性能大大降低，此时哈希种子 hash0 的设置，可以降低此类极端场景的发生。\n2.2 查找 # 根据 key 计算出哈希值 根据哈希值低位确定所在 bucket 根据哈希值高 8 位确定在 bucket 中的存储位置 当前 bucket 未找到则查找对应的 overflow bucket。 对应位置有数据则对比完整的哈希值，确定是否是要查找的数据 如果当前处于 map 进行了扩容，处于数据搬移状态，则优先从 oldbuckets 查找。 2.3 插入 # 根据 key 计算出哈希值 根据哈希值低位确定所在 bucket 根据哈希值高 8 位确定在 bucket 中的存储位置 查找该 key 是否存在，已存在则更新，不存在则插入 2.4 map 无序 # map 的本质是散列表，而 map 的增长扩容会导致重新进行散列，这就可能使 map 的遍历结果在扩容前后变得不可靠， Go 设计者为了让大家不依赖遍历的顺序，故意在实现 map 遍历时加入了随机数， 让每次遍历的起点\u0026ndash;即起始 bucket 的位置不一样，即不让遍历都从 bucket0 开始， 所以即使未扩容时我们遍历出来的 map 也总是无序的。\n3. 参考资料 # Golang map 底层实现 Go 语言之 map：map 的用法到 map 底层实现分析 Go 语言 map 底层浅析 哈希表 "},{"id":24,"href":"/docs/golang/language-basics/reflect/","title":"reflect","section":"语言基础","content":" 1. 引言 # 计算机中提到的反射一般是指，程序借助某种手段检查自己结构的一种能力，通常就是借助编程语言中定义的类型（types）。因此，反射是建立在类型系统上的。\ngo 是静态类型化，每个变量都有一个静态类型，也就是说，在编译时，变量的类型就已经确定。不显示地去做强制类型转换，不同类型之间是无法相互赋值的。\n有一种特殊的类型叫做接口（interface），一个接口表示的是一组方法集合。一个接口变量能存储任何具体的值，只要这个值实现了这个接口的方法集合。比如 io 包中的 Reader 和 Writer，io.Reader 接口变量能够保存任意实现了 Read() 方法的类型所定义的值。\n一个特殊接口就是空接口 interface{}，任何值都可以说实现了空接口，因为空接口中没有定义方法，所以空接口可以保存任何值。\n一个接口类型变量存储了一对值：赋值给这个接口变量的具体值 + 这个值的类型描述符。更进一步的讲，这个“值”是实现了这个接口的底层具体数据项（underlying concrete data item)，而这个“类型”是描述了具体数据项（item）的全类型（full type）。\n所以反射是干嘛的呢？反射是一种检查存储在接口变量中的（类型/值）对的机制。reflect 包中提供的 2 个类型 Type 和 Value，提供了访问接口值的 reflect.Type 和 reflect.Value 部分。\n2. 三大法则 # 2.1. Reflection goes from interface value to reflecton object # 从 interface{} 变量可以反射出反射对象\ntype MyInt int32 func main() { var x MyInt = 7 v := reflect.ValueOf(x) t := reflect.TypeOf(x) fmt.Println(\u0026#34;type:\u0026#34;, t) // type: main.MyInt fmt.Println(\u0026#34;value:\u0026#34;, v) // value: 7 fmt.Println(\u0026#34;kind:\u0026#34;, v.Kind()) // kind: int32 fmt.Println(\u0026#34;type:\u0026#34;, v.Type()) // type: main.MyInt x = MyInt(int32(v.Int)) // v.Int returns a int64 } reflect.Value 的 Type 返回的是静态类型 MyInt，而 kind() 方法返回的是底层类型 int32；为了保持 API 简单，value 的 Setter 和 Getter 类型方法操作，是包含某个值的最大类型，v.Int() 返回的是 int64，必要时转化成实际类型。\n2.2 Reflection goes from reflection object to interface value # 从反射对象可以获取 interface{} 变量；\ntype MyInt int32 func main() { var x MyInt = 7 v := reflect.ValueOf(x) y := v.Interface().(int32) fmt.Println(y) // 7 } 对于一个 reflect.Value，可以用 Interface() 方法恢复成一个接口值，效果就是包类型和值打包成接口，并返回结果。\n2.3 To modify a reflection object, the value must be settable # 要修改反射对象，其值必须可设置；\nfunc main() { var x float64 = 3.4 v := reflect.ValueOf(x) // panic: reflect: reflect.flag.mustBeAssignable using addressable value v.SetFloat(7.1) p := reflect.ValueOf(\u0026amp;X) // panic: reflect: reflect.flag.mustBeAssignable using addressable value p.SetFloat(7.1) e := reflect.ValueOf(\u0026amp;X).Elem() // OK e.SetFloat(7.1) } 如果我们想通过反射来修改变量 x，我们必须把我们想要修改的值的指针传给一个反射库。Go 语言的函数调用都是值传递的，所以我们只能先获取指针对应的 reflect.Value，再通过 reflect.Value.Elem 方法迂回的方式得到可以被设置的变量。我们通过如下所示的代码理解这个过程：\nfunc main() { i := 1 v := \u0026amp;i *v = 10 } 如果不能直接操作 i 变量修改其持有的值，我们就只能获取 i 变量所在地址并使用 *v 修改所在地址中存储的整数。\n3. 相关资料 # Go 语言中反射包的实现原理 4.3 反射 "},{"id":25,"href":"/docs/golang/data-structure/slice/","title":"slice","section":"数据结构","content":" 1. 数据结构 # type slice struct { array unsafe.Pointer len int cap int } slice 的底层数据结构中的 array 是一个指针，指向的是一个 Array len 代表这个 slice 的元素个数 cap 表示 slice 指向的底层数组容量 对 slice 的赋值，以值作为函数参数时，只拷贝 1 个指针和 2 个 int 值。\n2. 操作 # 2.1 创建 # var []T 或 []T{} func make([]T,len,cap) []T 2.2 nil 切片和空切片 # nil 切片被用在很多标准库和内置函数中，描述一个不存在的切片的时候，就需要用到 nil 切片。比如函数在发生异常的时候，返回的切片就是 nil 切片。nil 切片的指针指向 nil. 空切片一般会用来表示一个空集合。比如数据库查询，一条结果也没有查到，那么就可以返回一个空切片。 2.3 扩容 # 2.3.1 计算策略 # 若 Slice cap 大于 doublecap，则扩容后容量大小为 新 Slice 的容量（超了基准值，我就只给你需要的容量大小） 若 Slice len 小于 1024 个，在扩容时，增长因子为 1（也就是 3 个变 6 个） 若 Slice len 大于 1024 个，在扩容时，增长因子为 0.25（原本容量的四分之一） 2.3.2 内存策略 # 翻新扩展：当前元素为 kindNoPointers，也就是非指针类型，将在老 Slice cap 的地址后继续申请空间用于扩容 举家搬迁：重新申请一块内存地址，整体迁移并扩容 2.4 拷贝 # slicecopy() 方法会把源切片值（即 from Slice ) 中的元素复制到目标切片（即 to Slice ) 中，并返回被复制的元素个数，copy 的两个类型必须一致。slicecopy() 方法最终的复制结果取决于较短的那个切片，当较短的切片复制完成，整个复制过程就全部完成了。\n3. 特性 # slice 的 array 存储在连续内存上，因此具有以下特点：\n随机访问很快，适合下标访问，缓存命中率很高； 动态扩容会涉及内存拷贝和开辟新内存，会带来 gc 压力，内存碎片化； 如果可预估使用空间，提前分配 cap 的大小是极好的； 新、老 slice 共用底层数组，对底层数组的更改都会影响到彼此； append 可以掰断新老 slice 共用底层数组的关系； 4. 参考资料 # 深入解析 Go 中 Slice 底层实现 深入解析 Go Slice "},{"id":26,"href":"/docs/golang/memory-manage/memory-model/","title":"内存模型","section":"内存管理","content":" 1. 引言 # Go 语言的内存模型规定了一个 goroutine 可以看到另外一个 goroutine 修改同一个变量的值的条件，这类似 java 内存模型中内存可见性问题。当多个 goroutine 并发同时存取同一个数据时候必须把并发的存取的操作顺序化，在 go 中可以实现操作顺序化的工具有高级的通道（channel）通信和同步原语比如 sync 包中的互斥锁（Mutex）、读写锁（RWMutex）或者和 sync/atomic 中的原子操作。\n2. 设计原则 # 2.1 happens-before # 假设 A 和 B 表示一个多线程的程序执行的两个操作。如果 A happens-before B，那么 A 操作对内存的影响将对执行 B 的线程（且执行 B 之前）可见。\n单一 goroutine 中当满足下面条件时候，对一个变量的写操作 w1 对读操作 r1 可见：\n读操作 r1 不是发生在写操作 w1 前 在读操作 r1 之前，写操作 w1 之后没有其他的写操作 w2 对变量进行了修改 多 goroutine 下则需要满足下面条件才能保证写操作 w1 对读操作 r1 可见：\n写操作 w1 先于读操作 r1 任何对变量的写操作 w2 要先于写操作 w1 或者晚于读操作 r1 关于 channel 的 happens-before 在 Go 的内存模型中提到了三种情况：\n带缓冲的 channel 的发送操作 happens-before 相应 channel 的接收操作完成 不带缓冲的 channel 的接收操作 happens-before 相应 channel 的发送操作完成 关闭一个 channel happens-before 从该 channel 接收到最后的返回值 0 2.2 Synchronization # 2.2.1 初始化 # 如果在一个 goroutine 所在的源码包 p 里面通过 import 命令导入了包 q，那么 q 包里面 go 文件的初始化方法的执行会 happens before 于包 p 里面的初始化方法执行。\n2.2.2 创建 goroutine # go 语句启动一个新的 goroutine 的动作 happen before 该新 goroutine 的运行。\n2.2.3 销毁 goroutine # 一个 goroutine 的销毁操作并不能确保 happen before 程序中的任何事件。\n2.2.4 通道通信 # 在 go 中通道是用来解决多个 goroutines 之间进行同步的主要措施，在多个 goroutines 中，每个对通道进行写操作的 goroutine 都对应着一个从通道读操作的 goroutine。\n在有缓冲的通道时候向通道写入一个数据总是 happen before 这个数据被从通道中读取完成。 对应无缓冲的通道来说从通道接受（获取叫做读取）元素 happen before 向通道发送（写入）数据完成。 从容量为 C 的通道接受第 K 个元素 happen before 向通道第 k+C 次写入完成，比如从容量为 1 的通道接受第 3 个元素 happen before 向通道第 3+1 次写入完成。 2.3 Locks # 对应任何 sync.Mutex 或 sync.RWMutex 类型的变量 I 来说，调用 n 次 l.Unlock() 操作 happen before 调用 m 次 l.Lock() 操作返回，其中 n\u0026lt;m。 对任何一个 sync.RWMutex 类型的变量 l 来说，存在一个次数 n，调用 l.RLock()（读锁）操作 happens after 调用 n 次 l.Unlock()（释放写锁）并且相应的 l.RUnlock()（释放读锁） happen before 调用 n+1 次 l.Lock()（写锁）。 2.4 Once # 多 goroutine 下同时调用 once.Do(f) 时，真正执行 f() 函数的 goroutine， happen before 任何其他由于调用 once.Do(f) 而被阻塞的 goroutine 返回。\n3. 参考资料 # Golang 内存模型 The Go Memory Model "}]